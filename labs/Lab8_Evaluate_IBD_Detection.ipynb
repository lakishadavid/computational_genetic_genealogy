{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from collections import Counter\n",
    "import logging\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import subprocess\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from IPython.display import display, HTML\n",
    "import IPython\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "from dotenv import load_dotenv\n",
    "import numpy as np\n",
    "from sklearn.metrics import precision_recall_curve, average_precision_score, roc_curve, auc\n",
    "from intervaltree import IntervalTree\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_comp_gen_dir():\n",
    "    \"\"\"Find the computational_genetic_genealogy directory by searching up from current directory.\"\"\"\n",
    "    current = Path.cwd()\n",
    "    \n",
    "    # Search up through parent directories\n",
    "    while current != current.parent:\n",
    "        # Check if target directory exists in current path\n",
    "        target = current / 'computational_genetic_genealogy'\n",
    "        if target.is_dir():\n",
    "            return target\n",
    "        # Move up one directory\n",
    "        current = current.parent\n",
    "    \n",
    "    raise FileNotFoundError(\"Could not find computational_genetic_genealogy directory\")\n",
    "\n",
    "def load_env_file():\n",
    "    \"\"\"Find and load the .env file from the computational_genetic_genealogy directory.\"\"\"\n",
    "    try:\n",
    "        # Find the computational_genetic_genealogy directory\n",
    "        comp_gen_dir = find_comp_gen_dir()\n",
    "        \n",
    "        # Look for .env file\n",
    "        env_path = comp_gen_dir / '.env'\n",
    "        if not env_path.exists():\n",
    "            print(f\"Warning: No .env file found in {comp_gen_dir}\")\n",
    "            return None\n",
    "        \n",
    "        # Load the .env file\n",
    "        load_dotenv(env_path, override=True)\n",
    "        print(f\"Loaded environment variables from: {env_path}\")\n",
    "        return env_path\n",
    "        \n",
    "    except FileNotFoundError as e:\n",
    "        print(f\"Error: {e}\")\n",
    "        return None\n",
    "\n",
    "# Use the function\n",
    "env_path = load_env_file()\n",
    "\n",
    "working_directory = os.getenv('PROJECT_WORKING_DIR', default=None)\n",
    "data_directory = os.getenv('PROJECT_DATA_DIR', default=None)\n",
    "references_directory = os.getenv('PROJECT_REFERENCES_DIR', default=None)\n",
    "results_directory = os.getenv('PROJECT_RESULTS_DIR', default=None)\n",
    "utils_directory = os.getenv('PROJECT_UTILS_DIR', default=None)\n",
    "\n",
    "os.environ[\"WORKING_DIRECTORY\"] = working_directory\n",
    "os.environ[\"DATA_DIRECTORY\"] = data_directory\n",
    "os.environ[\"REFERENCES_DIRECTORY\"] = references_directory\n",
    "os.environ[\"RESULTS_DIRECTORY\"] = results_directory\n",
    "os.environ[\"UTILS_DIRECTORY\"] = utils_directory\n",
    "\n",
    "print(f\"Working Directory: {working_directory}\")\n",
    "print(f\"Data Directory: {data_directory}\")\n",
    "print(f\"References Directory: {references_directory}\")\n",
    "print(f\"Results Directory: {results_directory}\")\n",
    "print(f\"Utils Directory: {utils_directory}\")\n",
    "\n",
    "os.chdir(working_directory)\n",
    "print(f\"The current directory is {os.getcwd()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def configure_logging(log_filename, log_file_debug_level=\"INFO\", console_debug_level=\"INFO\"):\n",
    "    \"\"\"\n",
    "    Configure logging for both file and console handlers.\n",
    "\n",
    "    Args:\n",
    "        log_filename (str): Path to the log file where logs will be written.\n",
    "        log_file_debug_level (str): Logging level for the file handler.\n",
    "        console_debug_level (str): Logging level for the console handler.\n",
    "    \"\"\"\n",
    "    # Create a root logger\n",
    "    logger = logging.getLogger()\n",
    "    logger.setLevel(logging.DEBUG)  # Capture all messages at the root level\n",
    "\n",
    "    # Convert level names to numeric levels\n",
    "    file_level = getattr(logging, log_file_debug_level.upper(), logging.INFO)\n",
    "    console_level = getattr(logging, console_debug_level.upper(), logging.INFO)\n",
    "\n",
    "    # File handler: Logs messages at file_level and above to the file\n",
    "    file_handler = logging.FileHandler(log_filename)\n",
    "    file_handler.setLevel(file_level)\n",
    "    file_formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')\n",
    "    file_handler.setFormatter(file_formatter)\n",
    "\n",
    "    # Console handler: Logs messages at console_level and above to the console\n",
    "    console_handler = logging.StreamHandler(sys.stdout)\n",
    "    console_handler.setLevel(console_level)\n",
    "    console_formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')\n",
    "    console_handler.setFormatter(console_formatter)\n",
    "\n",
    "    # Add handlers to the root logger\n",
    "    logger.addHandler(file_handler)\n",
    "    logger.addHandler(console_handler)\n",
    "    \n",
    "def clear_logger():\n",
    "    \"\"\"Remove all handlers from the root logger.\"\"\"\n",
    "    logger = logging.getLogger()\n",
    "    for handler in logger.handlers[:]:\n",
    "        logger.removeHandler(handler)\n",
    "        \n",
    "log_filename = os.path.join(results_directory, \"lab8_log.txt\")\n",
    "print(f\"The Lab 8 log file is located at {log_filename}.\")\n",
    "\n",
    "# Ensure the results_directory exists\n",
    "if not os.path.exists(results_directory):\n",
    "    os.makedirs(results_directory)\n",
    "\n",
    "# Check if the file exists; if not, create it\n",
    "if not os.path.exists(log_filename):\n",
    "    with open(log_filename, 'w') as file:\n",
    "        pass  # The file is now created.\n",
    "    \n",
    "clear_logger() # Clear the logger before reconfiguring it\n",
    "configure_logging(log_filename, log_file_debug_level=\"INFO\", console_debug_level=\"INFO\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prune the input vcf (real data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "# Define the input and output VCF files\n",
    "input_vcf=${DATA_DIRECTORY}/class_data/merged_opensnps_data_autosomes.vcf.gz\n",
    "output_vcf=${RESULTS_DIRECTORY}/merged_opensnps_data_autosomes_pruned\n",
    "\n",
    "# Create directory in results directory - lab8_output\n",
    "mkdir -p ${RESULTS_DIRECTORY}/lab8_output\n",
    "\n",
    "echo \"Input sample size:\" $(bcftools query -l ${input_vcf} | wc -l)\n",
    "\n",
    "# Step 1: Convert the VCF file to PLINK binary format (BED/BIM/FAM)\n",
    "plink2 --vcf ${input_vcf} --make-bed --out dataset\n",
    "\n",
    "# Step 2: Remove close relatives using a KING cutoff of 0.125.\n",
    "# The threshold 0.125 corresponds roughly to the expected kinship coefficient for first cousins.\n",
    "plink2 --bfile dataset --king-cutoff 0.125 --make-bed --out ${RESULTS_DIRECTORY}/lab8_output/dataset_unrelated\n",
    "\n",
    "# Step 3: Convert the filtered, unrelated dataset back to VCF format\n",
    "plink2 --bfile ${RESULTS_DIRECTORY}/lab8_output/dataset_unrelated --export vcf --out ${output_vcf}\n",
    "\n",
    "# Step 4: Compress the VCF file using bgzip and index it using tabix\n",
    "bgzip -c ${output_vcf}.vcf > ${output_vcf}.vcf.gz\n",
    "tabix -p vcf ${output_vcf}.vcf.gz\n",
    "rm ${output_vcf}.vcf\n",
    "\n",
    "# Step 5: Use bcftools to report the sample size in both the output VCF files.\n",
    "echo \"######################################################################\"\n",
    "echo \"Output sample size:\" $(bcftools query -l ${output_vcf}.vcf.gz | wc -l)\n",
    "echo \"######################################################################\"\n",
    "\n",
    "# Remove all the intermediate files\n",
    "rm ${RESULTS_DIRECTORY}/lab8_output/dataset_unrelated.bed\n",
    "rm ${RESULTS_DIRECTORY}/lab8_output/dataset_unrelated.bim\n",
    "rm ${RESULTS_DIRECTORY}/lab8_output/dataset_unrelated.fam\n",
    "rm ${RESULTS_DIRECTORY}/lab8_output/dataset_unrelated.log\n",
    "rm ${RESULTS_DIRECTORY}/lab8_output/dataset_unrelated.king.cutoff.in.id\n",
    "rm ${RESULTS_DIRECTORY}/lab8_output/dataset_unrelated.king.cutoff.out.id\n",
    "\n",
    "${RESULTS_DIRECTORY}/lab8_output/dataset.log\n",
    "\n",
    "# make directory for the results\n",
    "mkdir -p ${RESULTS_DIRECTORY}/ped_sim_run2_autosomes/unphased_samples\n",
    "\n",
    "for chr in {1..22}; do\n",
    "    \n",
    "    plink2 --vcf ${output_vcf}.vcf.gz \\\n",
    "           --chr ${chr} \\\n",
    "           --export vcf bgz \\\n",
    "           --out ${RESULTS_DIRECTORY}/ped_sim_run2_autosomes/unphased_samples/merged_opensnps_data_pruned_chr${chr}\n",
    "done"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Phasing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash -s \"$results_directory\"\n",
    "\n",
    "results_directory=$1\n",
    "\n",
    "sample_file=\"merged_opensnps\"\n",
    "beagle=\"${UTILS_DIRECTORY}/beagle.17Dec24.224.jar\"\n",
    "\n",
    "# Create directories\n",
    "mkdir -p \"${results_directory}/ped_sim_run2_autosomes/phased_samples\"\n",
    "\n",
    "# Phase chromosomes using Beagle\n",
    "for chr in {1..22}; do\n",
    "    echo \"Processing chromosome $chr\"\n",
    "\n",
    "    INPUT_VCF=\"${RESULTS_DIRECTORY}/ped_sim_run2_autosomes/unphased_samples/merged_opensnps_data_pruned_chr${chr}.vcf.gz\"\n",
    "    REF_VCF=\"${REFERENCES_DIRECTORY}/onethousandgenomes_genotype_no_chr_prefix/subset_chr${chr}.vcf.gz\"\n",
    "    MAP_FILE=\"${REFERENCES_DIRECTORY}/genetic_maps/beagle_genetic_maps/plink.chr${chr}.GRCh38.map\"\n",
    "    OUTPUT_PREFIX=\"${results_directory}/ped_sim_run2_autosomes/phased_samples/merged_opensnps_data_pruned_chr${chr}\"\n",
    "    PHASED_VCF=\"${OUTPUT_PREFIX}.vcf.gz\"\n",
    "    TEMP_VCF=\"${results_directory}/ped_sim_run2_autosomes/phased_samples/temp_pruned_chr${chr}.vcf.gz\"\n",
    "    SORTED_VCF=\"${results_directory}/ped_sim_run2_autosomes/phased_samples/merged_opensnps_data_pruned_phased_chr${chr}_sorted.vcf.gz\"\n",
    "\n",
    "    # Check if input VCF exists\n",
    "    if [ ! -f \"$INPUT_VCF\" ]; then\n",
    "        echo \"Input VCF file not found for chromosome $chr. Skipping.\"\n",
    "        echo \"$INPUT_VCF\"\n",
    "        continue\n",
    "    fi\n",
    "\n",
    "    # Run Beagle phasing\n",
    "    if [ -f \"$REF_VCF\" ]; then\n",
    "        echo \"Running Beagle with reference panel for chromosome $chr\"\n",
    "        echo \"$INPUT_VCF\"\n",
    "        java -jar ${beagle} \\\n",
    "            gt=\"$INPUT_VCF\" \\\n",
    "            ref=\"$REF_VCF\" \\\n",
    "            map=\"$MAP_FILE\" \\\n",
    "            out=\"$OUTPUT_PREFIX\" || {\n",
    "                echo \"Beagle failed for chromosome $chr. Skipping.\"\n",
    "                continue\n",
    "            }\n",
    "    else\n",
    "        echo \"Running Beagle without reference panel for chromosome $chr\"\n",
    "        java -jar ${beagle} \\\n",
    "            gt=\"$INPUT_VCF\" \\\n",
    "            map=\"$MAP_FILE\" \\\n",
    "            out=\"$OUTPUT_PREFIX\" || {\n",
    "                echo \"Beagle failed for chromosome $chr. Skipping.\"\n",
    "                continue\n",
    "            }\n",
    "    fi\n",
    "\n",
    "    if [ ! -f \"$PHASED_VCF\" ]; then\n",
    "        echo \"Phasing failed for chromosome $chr. Output file not found. Skipping.\"\n",
    "        continue\n",
    "    fi\n",
    "\n",
    "    # Index the file\n",
    "    tabix -f -p vcf \"$PHASED_VCF\"\n",
    "    \n",
    "    # Add INFO field definition and sort\n",
    "    echo \"Sorting VCF for chromosome $chr\"\n",
    "    bcftools annotate --header-lines <(echo '##INFO=<ID=END,Number=1,Type=Integer,Description=\"End position of the variant\">') \"$PHASED_VCF\" | \\\n",
    "    bcftools sort -Oz -o \"$SORTED_VCF\" || {\n",
    "        echo \"Sorting failed for chromosome $chr\"\n",
    "        continue\n",
    "    }\n",
    "\n",
    "    # Index the sorted file\n",
    "    tabix -f -p vcf \"$SORTED_VCF\"\n",
    "    \n",
    "    # If the sorted vcf and index exists, remove phased vcf and index\n",
    "    if [ -f \"$SORTED_VCF\" ] && [ -f \"$SORTED_VCF.tbi\" ]; then\n",
    "        rm -f \"$PHASED_VCF\"\n",
    "        rm -f \"$PHASED_VCF.tbi\"\n",
    "    fi\n",
    "done\n",
    "\n",
    "bcftools concat -Oz -o \"${results_directory}/ped_sim_run2_autosomes/phased_samples/merged_opensnps_data_pruned_phased_all_sorted.vcf.gz\" \"${results_directory}/ped_sim_run2_autosomes/phased_samples/merged_opensnps_data_pruned_phased_chr\"{1..22}\"_sorted.vcf.gz\"\n",
    "tabix -f -p vcf \"${results_directory}/ped_sim_run2_autosomes/phased_samples/merged_opensnps_data_pruned_phased_all_sorted.vcf.gz\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "# Set up the references directory\n",
    "references_directory=\"${REFERENCES_DIRECTORY}\"\n",
    "mkdir -p \"$references_directory\"\n",
    "\n",
    "# Download the genetic map\n",
    "wget https://github.com/cbherer/Bherer_etal_SexualDimorphismRecombination/raw/master/Refined_genetic_map_b37.tar.gz -P $references_directory\n",
    "tar xvzf $references_directory/Refined_genetic_map_b37.tar.gz -C $references_directory\n",
    "\n",
    "# Create the combined map file\n",
    "printf \"#chr\\tpos\\tmale_cM\\tfemale_cM\\n\" > $references_directory/refined_mf_b37.simmap\n",
    "\n",
    "# Process each chromosome\n",
    "for chr in {1..22}; do\n",
    "  paste $references_directory/Refined_genetic_map_b37/male_chr$chr.txt $references_directory/Refined_genetic_map_b37/female_chr$chr.txt \\\n",
    "    | awk -v OFS=\"\\t\" 'NR > 1 && $2 == $6 {print $1,$2,$4,$8}' \\\n",
    "    | sed 's/^chr//' >> $references_directory/refined_mf_b37.simmap\n",
    "done\n",
    "\n",
    "# Clean up the downloaded files\n",
    "rm $references_directory/Refined_genetic_map_b37.tar.gz\n",
    "rm -r $references_directory/Refined_genetic_map_b37\n",
    "\n",
    "# Download the chain file for liftOver\n",
    "wget -O \"${references_directory}/hg19ToHg38.over.chain.gz\" \\\n",
    "  \"https://hgdownload.soe.ucsc.edu/goldenPath/hg19/liftOver/hg19ToHg38.over.chain.gz\"\n",
    "\n",
    "# Create a BED file from the build 37 simmap\n",
    "awk 'NR>1 {print \"chr\"$1, $2-1, $2, $3, $4}' OFS=\"\\t\" $references_directory/refined_mf_b37.simmap > $references_directory/refined_mf_b37.bed\n",
    "\n",
    "# Run liftOver to convert coordinates\n",
    "liftOver $references_directory/refined_mf_b37.bed \\\n",
    "         $references_directory/hg19ToHg38.over.chain.gz \\\n",
    "         $references_directory/refined_mf_b38.bed \\\n",
    "         $references_directory/refined_mf_b38.unmapped\n",
    "\n",
    "# Clean up temporary files\n",
    "rm $references_directory/refined_mf_b37.bed\n",
    "\n",
    "echo \"✅ Genetic map successfully created at: $references_directory/refined_mf_b38.bed\"\n",
    "echo \"This file contains the sex-specific genetic map in build 38 coordinates, ready for ped-sim.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simulate the seg and vcf data for the given genetic family tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulate_ground_truth():\n",
    "    \"\"\"Run ped-sim to simulate IBD segments based on pedigree definition\"\"\"\n",
    "    vcf_file = f\"{results_directory}/ped_sim_run2_autosomes/phased_samples/merged_opensnps_data_pruned_phased_all_sorted.vcf.gz\"\n",
    "    pedigree_def_file = f\"{data_directory}/class_data/pedigree.def\"\n",
    "    refined_map = f\"{references_directory}/refined_mf_b38.simmap\"\n",
    "    ped_sim_exec = f\"{utils_directory}/ped-sim/ped-sim\"\n",
    "    interfere_file = f\"{utils_directory}/ped-sim/interfere/nu_p_campbell.tsv\"\n",
    "    ped_sim_basename = \"merged_opensnps_autosomes_ped_sim\"\n",
    "    output_prefix = f\"{results_directory}/{ped_sim_basename}\"\n",
    "    \n",
    "    # Run ped-sim\n",
    "    print(f\"Running ped-sim with input VCF {vcf_file}\")\n",
    "    cmd = [\n",
    "        ped_sim_exec,\n",
    "        \"-d\", pedigree_def_file,\n",
    "        \"-m\", refined_map,\n",
    "        \"-o\", output_prefix,\n",
    "        \"-i\", vcf_file,\n",
    "        \"--intf\", interfere_file,\n",
    "        \"--seed\", \"1234\",\n",
    "        \"--fam\",\n",
    "        \"--mrca\"\n",
    "    ]\n",
    "    \n",
    "    subprocess.run(cmd, check=True)\n",
    "    \n",
    "    # Check results\n",
    "    if os.path.exists(f\"{output_prefix}.seg\") and os.path.exists(f\"{output_prefix}.vcf.gz\"):\n",
    "        print(f\"Truth segments file: {output_prefix}.seg\")\n",
    "        print(f\"Simulated VCF file: {output_prefix}.vcf.gz\")\n",
    "        return f\"{output_prefix}.vcf.gz\", f\"{output_prefix}.seg\"\n",
    "\n",
    "    else:\n",
    "        print(\"ERROR: Ped-sim failed to generate output files\")\n",
    "        return None\n",
    "    \n",
    "simulated_vcf, simulated_seg = simulate_ground_truth()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparing Data for IBD Detection Algorithms\n",
    "\n",
    "## Overview\n",
    "To accurately evaluate the performance of different IBD detection methods, we need to prepare the data in formats compatible with each algorithm. Each tool has specific input requirements that must be met.\n",
    "\n",
    "## Algorithm Requirements\n",
    "- **RefinedIBD**: Requires phased data, processes by chromosome\n",
    "- **HapIBD**: Requires phased data, processes by chromosome\n",
    "- **IBIS**: Can work with unphased data, can process all chromosomes together\n",
    "\n",
    "## Preparation Steps\n",
    "\n",
    "1. **Split by Chromosome**: The simulated VCF file contains data for all autosomes. We'll extract each chromosome separately to allow chromosome-by-chromosome processing.\n",
    "\n",
    "2. **Quality Control**: For each chromosome, we apply standard QC filters:\n",
    "   - Keep only biallelic SNPs\n",
    "   - Remove exact duplicate variants\n",
    "   - Filter variants on minor allele frequency (MAF > 0.05)\n",
    "   - Remove variants with high missing data rates (< 5%)\n",
    "\n",
    "3. **Phasing**: For RefinedIBD and HapIBD, we'll phase each chromosome separately using Beagle:\n",
    "   - Use reference panels when available for improved phasing accuracy\n",
    "   - Incorporate genetic maps for accurate recombination estimation\n",
    "   - Add necessary INFO fields required by downstream tools\n",
    "\n",
    "4. **Merging**: For IBIS, we'll merge all phased chromosomes back together into a single file\n",
    "\n",
    "The output of this process includes:\n",
    "- Chromosome-specific phased VCF files for RefinedIBD and HapIBD\n",
    "- A merged phased VCF for IBIS\n",
    "- File paths organized in a dictionary for subsequent analysis steps\n",
    "\n",
    "To run the data preparation function:\n",
    "\n",
    "```python\n",
    "data_paths = prepare_data_for_ibd_detection(simulated_prefix)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check Genetic Maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_genetic_maps():\n",
    "    \"\"\"Prepare genetic maps for all IBD detection algorithms\"\"\"\n",
    "    print(\"Preparing genetic maps for IBD detection algorithms...\")\n",
    "    \n",
    "    # Create genetic maps directories\n",
    "    beagle_map_dir = os.path.join(references_directory, \"genetic_maps/beagle_genetic_maps\")\n",
    "    ibis_map_dir = os.path.join(references_directory, \"genetic_maps/ibis_genetic_maps\")\n",
    "    os.makedirs(beagle_map_dir, exist_ok=True)\n",
    "    os.makedirs(ibis_map_dir, exist_ok=True)\n",
    "    \n",
    "    # Check if we have Beagle maps already\n",
    "    beagle_maps_exist = any(f.endswith(\".map\") for f in os.listdir(beagle_map_dir)) if os.path.exists(beagle_map_dir) else False\n",
    "    \n",
    "    # Download Beagle maps if needed\n",
    "    if not beagle_maps_exist:\n",
    "        print(\"Downloading Beagle genetic maps...\")\n",
    "        subprocess.run(\n",
    "            f\"poetry run python -m scripts_support.genetic_maps_download --data-source BEAGLE --assembly GRCh38\",\n",
    "            shell=True, check=True\n",
    "        )\n",
    "    \n",
    "    # Convert Beagle maps to IBIS format\n",
    "    print(\"Converting Beagle maps to IBIS format...\")\n",
    "    for map_file in os.listdir(beagle_map_dir):\n",
    "        if map_file.endswith(\".map\"):\n",
    "            beagle_map_filename = os.path.join(beagle_map_dir, map_file)\n",
    "            ibis_map_filename = os.path.join(ibis_map_dir, map_file)\n",
    "            \n",
    "            # For IBIS maps, we need: CHR POSITION GENETIC_POSITION [RATE]\n",
    "            # From Beagle maps which are: CHR . GENETIC_POSITION PHYSICAL_POSITION\n",
    "            subprocess.run(\n",
    "                f\"awk '{{print $1, $4, $3, 0}}' {beagle_map_filename} > {ibis_map_filename}\",\n",
    "                shell=True, check=True\n",
    "            )\n",
    "    \n",
    "    print(\"Genetic maps preparation complete.\")\n",
    "    return beagle_map_dir, ibis_map_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_directory_unphased=f\"{results_directory}/ped_sim_run2_autosomes/unphased_samples\"\n",
    "output_directory_phased=f\"{results_directory}/ped_sim_run2_autosomes/phased_samples\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # debugging\n",
    "# simulated_seg = \"/home/lakishadavid/computational_genetic_genealogy/results/merged_opensnps_autosomes_ped_sim.seg\"\n",
    "# simulated_vcf = \"/home/lakishadavid/computational_genetic_genealogy/results/merged_opensnps_autosomes_ped_sim.vcf.gz\"\n",
    "# simulated_vcf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quality Control"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash -s \"$simulated_vcf\" \"$output_directory_unphased\" \"$output_directory_phased\"\n",
    "\n",
    "simulated_vcf=\"$1\"\n",
    "echo \"simulated_vcf: ${simulated_vcf}\"\n",
    "output_directory_unphased=\"$2\"\n",
    "output_directory_phased=\"$3\"\n",
    "\n",
    "# Create a properly BGZF-compressed file from scratch\n",
    "gunzip -c \"$simulated_vcf\" > \"${simulated_vcf%.gz}.temp\"\n",
    "bgzip -c \"${simulated_vcf%.gz}.temp\" > \"${simulated_vcf%.gz}.proper.gz\"\n",
    "rm \"${simulated_vcf%.gz}.temp\"  # Clean up the temp file\n",
    "rm \"${simulated_vcf}\"\n",
    "mv \"${simulated_vcf%.gz}.proper.gz\" \"${simulated_vcf}\"\n",
    "# Now index the properly compressed file\n",
    "tabix -p vcf \"${simulated_vcf}\"\n",
    "\n",
    "\n",
    "# Get the base name of the VCF file\n",
    "filename_wo_ext=\"${simulated_vcf%.vcf.gz}\"\n",
    "file_basename=$(basename \"$filename_wo_ext\")\n",
    "\n",
    "# Run Quality Control (QC) on the VCF file\n",
    "for chromosome in {1..22}; do\n",
    "    echo \"Processing chromosome $chromosome...\"\n",
    "    \n",
    "    output_vcf=\"$output_directory_unphased/${file_basename}_qcfinished_chr${chromosome}.vcf.gz\"\n",
    "    \n",
    "    # Extended QC pipeline:\n",
    "    # 1. Select autosomal chromosome\n",
    "    # 2. Keep only biallelic SNPs\n",
    "    # -m2 keeps only variants with at least 2 alleles\n",
    "    # -M2 keeps only variants with at most 2 alleles\n",
    "    # could add: -i 'strlen(REF)=1 && strlen(ALT)=1' | \\\n",
    "    # 3. Remove exact duplicate variants\n",
    "    # 4. Filter on MAF and missing data\n",
    "    # 5. Sort variants\n",
    "    bcftools view \"$simulated_vcf\" \\\n",
    "        --regions \"${chromosome}\" \\\n",
    "        --types snps \\\n",
    "        -m2 -M2 \\\n",
    "        -i 'strlen(REF)=1 && strlen(ALT)=1' | \\\n",
    "    bcftools norm --rm-dup exact | \\\n",
    "    bcftools view \\\n",
    "        -q 0.05:minor \\\n",
    "        -i 'F_MISSING < 0.05' | \\\n",
    "    bcftools sort -Oz -o \"$output_vcf\"\n",
    "    \n",
    "    # Index the final VCF with force flag\n",
    "    bcftools index -f \"$output_vcf\"\n",
    "    \n",
    "    # Report number of variants\n",
    "    # echo \"Number of variants in chromosome $chromosome after QC:\"\n",
    "    bcftools index -n \"$output_vcf\"\n",
    "    echo\n",
    "done\n",
    "\n",
    "beagle=\"${UTILS_DIRECTORY}/beagle.17Dec24.224.jar\"\n",
    "\n",
    "# Phase chromosomes using Beagle\n",
    "for chr in {1..22}; do\n",
    "    echo \"Processing chromosome ${chr}\"\n",
    "\n",
    "    INPUT_VCF=\"${output_directory_unphased}/${file_basename}_qcfinished_chr${chr}.vcf.gz\"\n",
    "    REF_VCF=\"${REFERENCES_DIRECTORY}/onethousandgenomes_genotype/onethousandgenomes_genotyped_phased.chr${chr}.vcf.gz\"\n",
    "    MAP_FILE=\"${REFERENCES_DIRECTORY}/genetic_maps/beagle_genetic_maps/plink.chr${chr}.GRCh38.map\"\n",
    "    OUTPUT_PREFIX=\"${output_directory_phased}/${file_basename}_phased_chr${chr}_temp\"\n",
    "    PHASED_VCF=\"${OUTPUT_PREFIX}.vcf.gz\"\n",
    "    TEMP_VCF=\"${output_directory_phased}/temp_chr${chr}.vcf.gz\"\n",
    "    SORTED_VCF=\"${output_directory_phased}/${file_basename}_phased_chr${chr}.vcf.gz\"\n",
    "\n",
    "    # Check if input VCF exists\n",
    "    if [ ! -f \"${INPUT_VCF}\" ]; then\n",
    "        echo \"Input VCF file not found for chromosome ${chr}. Skipping.\"\n",
    "        echo \"${INPUT_VCF}\"\n",
    "        continue\n",
    "    fi\n",
    "\n",
    "    # Run Beagle phasing\n",
    "    if [ -f \"${REF_VCF}\" ]; then\n",
    "        echo \"Running Beagle with reference panel for chromosome ${chr}\"\n",
    "        java -jar ${beagle} \\\n",
    "            gt=\"${INPUT_VCF}\" \\\n",
    "            ref=\"${REF_VCF}\" \\\n",
    "            map=\"${MAP_FILE}\" \\\n",
    "            out=\"${OUTPUT_PREFIX}\" || {\n",
    "                echo \"Beagle failed for chromosome ${chr}. Skipping.\"\n",
    "                continue\n",
    "            }\n",
    "    else\n",
    "        echo \"Running Beagle without reference panel for chromosome ${chr}\"\n",
    "        java -jar ${beagle} \\\n",
    "            gt=\"${INPUT_VCF}\" \\\n",
    "            map=\"${MAP_FILE}\" \\\n",
    "            out=\"${OUTPUT_PREFIX}\" || {\n",
    "                echo \"Beagle failed for chromosome ${chr}. Skipping.\"\n",
    "                continue\n",
    "            }\n",
    "    fi\n",
    "\n",
    "    if [ ! -f \"${PHASED_VCF}\" ]; then\n",
    "        echo \"Phasing failed for chromosome ${chr}. Output file not found. Skipping.\"\n",
    "        continue\n",
    "    fi\n",
    "\n",
    "    # Index the file\n",
    "    tabix -f -p vcf \"${PHASED_VCF}\"\n",
    "    \n",
    "    # Add INFO field definition and sort\n",
    "    echo \"Sorting VCF for chromosome $CHR\"\n",
    "    bcftools annotate --header-lines <(echo '##INFO=<ID=END,Number=1,Type=Integer,Description=\"End position of the variant\">') \"${PHASED_VCF}\" | \\\n",
    "    bcftools sort -Oz -o \"${SORTED_VCF}\" || {\n",
    "        echo \"Sorting failed for chromosome $CHR\"\n",
    "        continue\n",
    "    }\n",
    "\n",
    "    # Index the sorted file\n",
    "    tabix -f -p vcf \"${SORTED_VCF}\"\n",
    "    \n",
    "    # If the sorted vcf and index exists, remove phased vcf and index\n",
    "    if [ -f \"${SORTED_VCF}\" ] && [ -f \"${SORTED_VCF}.tbi\" ]; then\n",
    "        rm -f \"${PHASED_VCF}\"\n",
    "        rm -f \"${PHASED_VCF}.tbi\"\n",
    "        rm -f \"${PHASED_VCF}.log\"\n",
    "    fi\n",
    "done"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Refined IBD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash -s \"$simulated_vcf\" \"$output_directory_unphased\" \"$output_directory_phased\"\n",
    "\n",
    "simulated_vcf=\"$1\"\n",
    "output_directory_unphased=\"$2\"\n",
    "output_directory_phased=\"$3\"\n",
    "echo \"output_directory_phased: ${output_directory_phased}\"\n",
    "\n",
    "# Get the base name of the VCF file\n",
    "filename_wo_ext=\"${simulated_vcf%.vcf.gz}\"\n",
    "file_basename=$(basename \"$filename_wo_ext\")\n",
    "\n",
    "# Define the Refined-IBD executable paths\n",
    "refined_ibd=\"${UTILS_DIRECTORY}/refined-ibd.17Jan20.102.jar\"\n",
    "merge_ibd=\"${UTILS_DIRECTORY}/merge-ibd-segments.17Jan20.102.jar\"\n",
    "\n",
    "# Create directory for segments\n",
    "mkdir -p \"${RESULTS_DIRECTORY}/segments\"\n",
    "\n",
    "# Loop for multiple runs to enhance sensitivity\n",
    "for run in {1..3}; do\n",
    "    for chr in {1..22}; do\n",
    "        phased_file=\"${output_directory_phased}/${file_basename}_phased_chr${chr}.vcf.gz\"\n",
    "        \n",
    "        # Check if file exists\n",
    "        if [[ ! -f \"${phased_file}\" ]]; then\n",
    "            echo \"Phased file not found for chr${chr}: ${phased_file}\"\n",
    "            continue\n",
    "        fi\n",
    "        \n",
    "        echo \"Processing chromosome ${chr}, run ${run} with RefinedIBD\"\n",
    "        \n",
    "        # Run RefinedIBD\n",
    "        java -jar \"${refined_ibd}\" \\\n",
    "            gt=\"${phased_file}\" \\\n",
    "            map=\"${REFERENCES_DIRECTORY}/genetic_maps/beagle_genetic_maps/plink.chr${chr}.GRCh38.map\" \\\n",
    "            lod=4 \\\n",
    "            length=3 \\\n",
    "            out=\"${RESULTS_DIRECTORY}/segments/temp_${file_basename}_refinedibd_chr${chr}_run${run}.seg\" \\\n",
    "            nthreads=4\n",
    "    done\n",
    "done\n",
    "\n",
    "gap_threshold=0.6\n",
    "discord_threshold=1\n",
    "\n",
    "for chr in {1..22}; do\n",
    "    echo \"🔀 Merging IBD segments for chr${chr}\"\n",
    "    phased_file=\"${output_directory_phased}/${file_basename}_phased_chr${chr}.vcf.gz\"\n",
    "    map_file=\"${REFERENCES_DIRECTORY}/genetic_maps/beagle_genetic_maps/plink.chr${chr}.GRCh38.map\"\n",
    "    merged_out=\"${RESULTS_DIRECTORY}/segments/${file_basename}_refinedibd_chr${chr}_merged.seg\"\n",
    "\n",
    "    # collect the .ibd.gz outputs from each run\n",
    "    ibd_files=()\n",
    "    for run in {1..3}; do\n",
    "        prefix=\"${RESULTS_DIRECTORY}/segments/temp_${file_basename}_refinedibd_chr${chr}_run${run}.seg\"\n",
    "        f=\"${prefix}.ibd.gz\"\n",
    "        if [[ -f \"$f\" ]]; then\n",
    "        ibd_files+=(\"$f\")\n",
    "        else\n",
    "        echo \"   ⚠️  Missing IBD file: $f\"\n",
    "        fi\n",
    "    done\n",
    "\n",
    "    if (( ${#ibd_files[@]} == 0 )); then\n",
    "        echo \"   ❌  No runs to merge for chr${chr}, skipping.\"\n",
    "        continue\n",
    "    fi\n",
    "\n",
    "    # decompress & merge\n",
    "    echo \"   📦  Found ${#ibd_files[@]} runs; merging...\"\n",
    "    # process substitution avoids combining into one long string\n",
    "    {\n",
    "        for f in \"${ibd_files[@]}\"; do\n",
    "        zcat \"$f\"\n",
    "        done\n",
    "    } | java -jar \"${merge_ibd}\" \\\n",
    "            \"${phased_file}\" \\\n",
    "            \"${map_file}\" \\\n",
    "            \"${gap_threshold}\" \\\n",
    "            \"${discord_threshold}\" \\\n",
    "        > \"${merged_out}\"\n",
    "\n",
    "    echo \"   ✅  Merged output written to ${merged_out}\"\n",
    "done\n",
    "\n",
    "# Combine all per‐chromosome merge files into one final output\n",
    "final_output=\"${RESULTS_DIRECTORY}/${file_basename}_refinedibd.seg\"\n",
    "> \"${final_output}\"   # truncate or create\n",
    "\n",
    "for chr in {1..22}; do\n",
    "    chr_merged=\"${RESULTS_DIRECTORY}/segments/${file_basename}_refinedibd_chr${chr}_merged.seg\"\n",
    "    if [[ -f \"${chr_merged}\" ]]; then\n",
    "        cat \"${chr_merged}\" >> \"${final_output}\"\n",
    "    else\n",
    "        echo \"⚠️  Missing merged file for chr${chr}: ${chr_merged}\"\n",
    "    fi\n",
    "done\n",
    "\n",
    "echo \"✅  All chromosomes merged into: ${final_output}\"\n",
    "\n",
    "# 4. Clean up intermediate files\n",
    "for chr in {1..22}; do\n",
    "    for run in {1..3}; do\n",
    "        rm -f \"${RESULTS_DIRECTORY}/segments/temp_${file_basename}_refinedibd_chr${chr}_run${run}.seg.ibd.gz\"\n",
    "        rm -f \"${RESULTS_DIRECTORY}/segments/temp_${file_basename}_refinedibd_chr${chr}_run${run}.seg.hbd.gz\"\n",
    "        rm -f \"${RESULTS_DIRECTORY}/segments/temp_${file_basename}_refinedibd_chr${chr}_run${run}.seg.log\"\n",
    "    done\n",
    "    rm -f \"${RESULTS_DIRECTORY}/segments/${file_basename}_refinedibd_chr${chr}_merged.seg\"\n",
    "done\n",
    "\n",
    "echo \"🧹  Cleaned up all temporary segment files.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check RefinedIBD output\n",
    "refined_ibd_output = os.path.join(results_directory, \"merged_opensnps_autosomes_ped_sim_refinedibd.seg\")\n",
    "if os.path.exists(refined_ibd_output):\n",
    "    print(f\"RefinedIBD output found: {refined_ibd_output}\")\n",
    "    print(f\"File size: {os.path.getsize(refined_ibd_output)} bytes\")\n",
    "else:\n",
    "    print(f\"RefinedIBD output not found: {refined_ibd_output}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HAP-IBD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash -s \"$simulated_vcf\" \"$output_directory_unphased\" \"$output_directory_phased\"\n",
    "\n",
    "simulated_vcf=\"$1\"\n",
    "output_directory_unphased=\"$2\"\n",
    "output_directory_phased=\"$3\"\n",
    "echo \"output_directory_phased: ${output_directory_phased}\"\n",
    "\n",
    "# Get the base name of the VCF file\n",
    "filename_wo_ext=\"${simulated_vcf%.vcf.gz}\"\n",
    "file_basename=$(basename \"$filename_wo_ext\")\n",
    "\n",
    "# Define the Hap-IBD executable paths\n",
    "hap_ibd=\"${UTILS_DIRECTORY}/hap-ibd.jar\"\n",
    "\n",
    "# Create directory for segments\n",
    "mkdir -p \"${RESULTS_DIRECTORY}/segments\"\n",
    "\n",
    "for chr in {1..22}; do\n",
    "    phased_file=\"${output_directory_phased}/${file_basename}_phased_chr${chr}.vcf.gz\"\n",
    "    \n",
    "    echo \"Processing chromosome ${chr} with Hap IBD\"\n",
    "    \n",
    "    # Run Hap IBD\n",
    "    java -jar \"${hap_ibd}\" \\\n",
    "        gt=\"${phased_file}\" \\\n",
    "        map=\"${REFERENCES_DIRECTORY}/genetic_maps/beagle_genetic_maps/plink.chr${chr}.GRCh38.map\" \\\n",
    "        out=\"${RESULTS_DIRECTORY}/segments/${file_basename}_hapibd_chr${chr}.seg\" \\\n",
    "        nthreads=4\n",
    "done\n",
    "\n",
    "\n",
    "\n",
    "# Combine all per‐chromosome merge files into one final output\n",
    "final_output=\"${RESULTS_DIRECTORY}/${file_basename}_hapibd.seg\"\n",
    "> \"${final_output}\"   # truncate or create\n",
    "\n",
    "for chr in {1..22}; do\n",
    "    chr_merged=\"${RESULTS_DIRECTORY}/segments/${file_basename}_hapibd_chr${chr}.seg.ibd.gz\"\n",
    "    if [[ -f \"${chr_merged}\" ]]; then\n",
    "        zcat \"${chr_merged}\" >> \"${final_output}\"\n",
    "    else\n",
    "        echo \"⚠️  Missing merged file for chr${chr}: ${chr_merged}\"\n",
    "    fi\n",
    "done\n",
    "\n",
    "echo \"✅  All chromosomes merged into: ${final_output}\"\n",
    "\n",
    "# 4. Clean up intermediate files\n",
    "for run in {1..3}; do\n",
    "    rm -f \"${RESULTS_DIRECTORY}/segments/${file_basename}_hapibd_chr${chr}.seg.ibd.gz\"\n",
    "    rm -f \"${RESULTS_DIRECTORY}/segments/${file_basename}_hapibd_chr${chr}.seg.hbd.gz\"\n",
    "    rm -f \"${RESULTS_DIRECTORY}/segments/${file_basename}_hapibd_chr${chr}.seg.log\"\n",
    "done\n",
    "\n",
    "echo \"🧹  Cleaned up all temporary segment files.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check HapIBD output\n",
    "hap_ibd_output = os.path.join(results_directory, \"merged_opensnps_autosomes_ped_sim_hapibd.seg\")\n",
    "if os.path.exists(hap_ibd_output):\n",
    "    print(f\"HapIBD output found: {hap_ibd_output}\")\n",
    "    print(f\"File size: {os.path.getsize(hap_ibd_output)} bytes\")\n",
    "else:\n",
    "    print(f\"HapIBD output not found: {hap_ibd_output}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prefix = \"merged_opensnps_autosomes_ped_sim\"\n",
    "\n",
    "segments_pedsim = Path(results_directory) / f\"{prefix}.seg\"\n",
    "segments_refinedibd = Path(results_directory) /f\"{prefix}_refinedibd.seg\"\n",
    "segments_hapibd = Path(results_directory) / f\"{prefix}_hapibd.seg\"\n",
    "segments_ibis = Path(results_directory) / f\"{prefix}_ibis.seg\"\n",
    "\n",
    "segment_files = [segments_pedsim, segments_refinedibd, segments_hapibd, segments_ibis]\n",
    "\n",
    "# Check if each file exists\n",
    "missing_files = []\n",
    "for file_path in segment_files:\n",
    "    if not file_path.exists():\n",
    "        print(f\"Warning: File not found - {file_path}\")\n",
    "        missing_files.append(file_path.name)\n",
    "    else:\n",
    "        print(f\"File exists: {file_path}\")\n",
    "\n",
    "# Provide instructions if files are missing\n",
    "if missing_files:\n",
    "    print(\"\\nMissing segment files. To fix this issue:\")\n",
    "    print(\"1. Make sure you've run the corresponding IBD detection labs (Lab4-Lab7) first\")\n",
    "    print(\"2. Check the output directories in your results folder\")\n",
    "    print(\"3. For missing PedSim ground truth, try running cell 4 to copy it from the expected location\")\n",
    "    print(\"\\nMissing files: \" + \", \".join(missing_files))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = [segments_pedsim, segments_refinedibd, segments_hapibd, segments_ibis]\n",
    "# See head of each file\n",
    "for file in files:\n",
    "    if file.exists():\n",
    "        print(f\"Head of {file}:\")\n",
    "        with open(file, 'r') as f:\n",
    "            for _ in range(5):\n",
    "                print(f.readline().strip())\n",
    "        print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_refined_ibd(filepath):\n",
    "    \"\"\"Load Refined IBD output file\"\"\"\n",
    "    try:\n",
    "        df = pd.read_csv(filepath, sep=\"\\t\", header=None)\n",
    "        df.columns = [\"sample1\", \"sample1_haplotype\", \"sample2\", \n",
    "                      \"sample2_haplotype\", \"chrom\", \"start\", \"end\", \"LOD\", \"cM\"]\n",
    "        # Create a unique segment ID for each segment\n",
    "        df['segment_id'] = range(len(df))\n",
    "        df['tool'] = 'RefinedIBD'\n",
    "        df['length'] = df['end'] - df['start']\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading Refined IBD file: {e}\")\n",
    "        cols = [\"sample1\", \"sample1_haplotype\", \"sample2\", \"sample2_haplotype\", \n",
    "                \"chrom\", \"start\", \"end\", \"LOD\", \"cM\"]\n",
    "        return pd.DataFrame(columns=cols + ['segment_id', 'tool', 'length'])\n",
    "\n",
    "def load_hap_ibd(filepath):\n",
    "   \"\"\"Load Hap IBD output file\"\"\"\n",
    "   try:\n",
    "       df = pd.read_csv(filepath, sep=\"\\t\", header=None)\n",
    "       df.columns = [\"sample1\", \"sample1_haplotype\", \"sample2\", \n",
    "                     \"sample2_haplotype\", \"chrom\", \"start\", \"end\", \"cM\"]\n",
    "       df['segment_id'] = range(len(df))\n",
    "       df['tool'] = 'HapIBD'\n",
    "       df['length'] = df['end'] - df['start']\n",
    "       # Since LOD isn't in the HapIBD output, create a placeholder or use cM as proxy\n",
    "       df['LOD'] = df['cM']  # Using cM as a proxy for confidence\n",
    "       return df\n",
    "   except Exception as e:\n",
    "       print(f\"Error loading Hap IBD file: {e}\")\n",
    "       cols = [\"sample1\", \"sample1_haplotype\", \"sample2\", \"sample2_haplotype\", \n",
    "               \"chrom\", \"start\", \"end\", \"cM\"]\n",
    "       return pd.DataFrame(columns=cols + ['segment_id', 'tool', 'length', 'LOD'])\n",
    "\n",
    "def load_ibis(filepath):\n",
    "    \"\"\"Load IBIS output file\"\"\"\n",
    "    try:\n",
    "        df = pd.read_csv(filepath, sep=\"\\t\", header=None)\n",
    "        df.columns = [\"sample1\", \"sample2\", \"chrom\", \n",
    "                    \"phys_start_pos\", \"phys_end_pos\", \n",
    "                    \"IBD_type\", \"genetic_start_pos\", \n",
    "                    \"genetic_end_pos\", \"genetic_seg_length\", \n",
    "                    \"marker_count\", \"error_count\", \"error_density\"]\n",
    "        \n",
    "        # Map IBIS columns to standardized column names for evaluation\n",
    "        df['start'] = df['phys_start_pos']\n",
    "        df['end'] = df['phys_end_pos']\n",
    "        df['cM'] = df['genetic_seg_length']\n",
    "        df['segment_id'] = range(len(df))\n",
    "        df['tool'] = 'IBIS'\n",
    "        df['length'] = df['end'] - df['start']\n",
    "        \n",
    "        # Create LOD-like score based on error density (lower error = higher score)\n",
    "        # Invert error_density to make higher values better\n",
    "        df['LOD'] = 1.0 / (df['error_density'] + 0.001)  # Add small constant to avoid division by zero\n",
    "        \n",
    "        # Add haplotype columns if needed for consistent evaluation\n",
    "        df['sample1_haplotype'] = 0  # Placeholder if IBIS doesn't specify haplotypes\n",
    "        df['sample2_haplotype'] = 0\n",
    "        \n",
    "        return df\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading IBIS file: {e}\")\n",
    "        cols = [\"sample1\", \"sample2\", \"chrom\", \"phys_start_pos\", \"phys_end_pos\", \n",
    "                \"IBD_type\", \"genetic_start_pos\", \"genetic_end_pos\", \"genetic_seg_length\", \n",
    "                \"marker_count\", \"error_count\", \"error_density\"]\n",
    "        return pd.DataFrame(columns=cols + ['start', 'end', 'segment_id', 'tool', 'length', 'LOD', \n",
    "                                           'sample1_haplotype', 'sample2_haplotype'])\n",
    "\n",
    "def load_pedsim_truth(filepath):\n",
    "   \"\"\"Load ground truth IBD segments from ped-sim\"\"\"\n",
    "   try:\n",
    "       df = pd.read_csv(filepath, sep=\"\\t\", header=None)\n",
    "       df.columns = [\"id1\", \"id2\", \"chromosome\", \"physical_position_start\", \n",
    "                    \"physical_position_end\", \"IBD_type\", \"genetic_position_start\", \n",
    "                    \"genetic_position_end\", \"genetic_length\"]\n",
    "       \n",
    "       # Map to standardized column names used in evaluation\n",
    "       df['sample1'] = df['id1']\n",
    "       df['sample2'] = df['id2']\n",
    "       df['chrom'] = df['chromosome']\n",
    "       df['start'] = df['physical_position_start']\n",
    "       df['end'] = df['physical_position_end']\n",
    "       df['cM'] = df['genetic_length']\n",
    "       df['sample1_haplotype'] = 0  # Add placeholder haplotypes if needed\n",
    "       df['sample2_haplotype'] = 0  # These can be updated if actual haplotype info exists\n",
    "       \n",
    "       df['segment_id'] = range(len(df))\n",
    "       df['tool'] = 'Truth'\n",
    "       df['length'] = df['end'] - df['start']\n",
    "       return df\n",
    "   except Exception as e:\n",
    "       print(f\"Error loading PedSim truth file: {e}\")\n",
    "       cols = [\"id1\", \"id2\", \"chromosome\", \"physical_position_start\", \n",
    "              \"physical_position_end\", \"IBD_type\", \"genetic_position_start\", \n",
    "              \"genetic_position_end\", \"genetic_length\"]\n",
    "       return pd.DataFrame(columns=cols + ['sample1', 'sample2', 'chrom', 'start', 'end', \n",
    "                                         'cM', 'segment_id', 'tool', 'length',\n",
    "                                         'sample1_haplotype', 'sample2_haplotype'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "refined_df = load_refined_ibd(segments_refinedibd)\n",
    "hap_df = load_hap_ibd(segments_hapibd)\n",
    "ibis_df = load_ibis(segments_ibis)\n",
    "truth_df = load_pedsim_truth(segments_pedsim)\n",
    "\n",
    "# Print data summaries\n",
    "print(f\"Loaded {len(refined_df)} Refined IBD segments\")\n",
    "print(f\"Loaded {len(hap_df)} Hap IBD segments\")\n",
    "print(f\"Loaded {len(ibis_df)} IBIS segments\")\n",
    "print(f\"Loaded {len(truth_df)} truth segments from ped-sim\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_interval_tree(truth_df):\n",
    "    \"\"\"Create an interval tree from truth segments for efficient overlap checking\"\"\"\n",
    "    trees = {}\n",
    "    \n",
    "    print(\"Building interval trees from truth data...\")\n",
    "    \n",
    "    # First try with standard column names\n",
    "    if all(col in truth_df.columns for col in ['sample1', 'sample2', 'chrom', 'start', 'end']):\n",
    "        sample1_col, sample2_col = 'sample1', 'sample2'\n",
    "    # Then try with ped-sim column names\n",
    "    elif all(col in truth_df.columns for col in ['id1', 'id2', 'chromosome', 'physical_position_start', 'physical_position_end']):\n",
    "        sample1_col, sample2_col = 'id1', 'id2'\n",
    "        # Map columns for consistency\n",
    "        truth_df['sample1'] = truth_df['id1']\n",
    "        truth_df['sample2'] = truth_df['id2']\n",
    "        truth_df['chrom'] = truth_df['chromosome']\n",
    "        truth_df['start'] = truth_df['physical_position_start']\n",
    "        truth_df['end'] = truth_df['physical_position_end']\n",
    "    else:\n",
    "        print(\"Error: Could not identify required columns in truth data\")\n",
    "        return trees\n",
    "    \n",
    "    # Add haplotype columns if they don't exist\n",
    "    if 'sample1_haplotype' not in truth_df.columns:\n",
    "        truth_df['sample1_haplotype'] = 0\n",
    "    if 'sample2_haplotype' not in truth_df.columns:\n",
    "        truth_df['sample2_haplotype'] = 0\n",
    "    \n",
    "    # Create trees with and without haplotype information\n",
    "    for _, row in truth_df.iterrows():\n",
    "        # Create keys in both orders to handle either order in tool data\n",
    "        pairs = [\n",
    "            # With haplotypes\n",
    "            ((row['sample1'], row['sample1_haplotype']), \n",
    "             (row['sample2'], row['sample2_haplotype']), \n",
    "             row['chrom']),\n",
    "            ((row['sample2'], row['sample2_haplotype']), \n",
    "             (row['sample1'], row['sample1_haplotype']), \n",
    "             row['chrom']),\n",
    "            # Without haplotypes\n",
    "            ((row['sample1'], None), (row['sample2'], None), row['chrom']),\n",
    "            ((row['sample2'], None), (row['sample1'], None), row['chrom'])\n",
    "        ]\n",
    "        \n",
    "        for sample1, sample2, chrom in pairs:\n",
    "            pair_key = (sample1, sample2)\n",
    "            if (pair_key, chrom) not in trees:\n",
    "                trees[(pair_key, chrom)] = IntervalTree()\n",
    "            \n",
    "            trees[(pair_key, chrom)].addi(row['start'], row['end'], row['segment_id'])\n",
    "    \n",
    "    print(f\"Created {len(trees)} interval trees\")\n",
    "    return trees\n",
    "\n",
    "def calculate_overlap(segment, tree):\n",
    "    \"\"\"Calculate overlap between a segment and truth segments in the tree\"\"\"\n",
    "    overlaps = tree.overlap(segment['start'], segment['end'])\n",
    "    if not overlaps:\n",
    "        return 0, None\n",
    "    \n",
    "    # Find the best overlapping segment\n",
    "    best_overlap = 0\n",
    "    best_truth_id = None\n",
    "    \n",
    "    for interval in overlaps:\n",
    "        overlap_start = max(segment['start'], interval.begin)\n",
    "        overlap_end = min(segment['end'], interval.end)\n",
    "        overlap_length = overlap_end - overlap_start\n",
    "        \n",
    "        if overlap_length > best_overlap:\n",
    "            best_overlap = overlap_length\n",
    "            best_truth_id = interval.data\n",
    "    \n",
    "    return best_overlap / (segment['end'] - segment['start']), best_truth_id\n",
    "\n",
    "def evaluate_tool(tool_df, truth_trees):\n",
    "    \"\"\"Evaluate IBD detection performance for a specific tool\"\"\"\n",
    "    # If the DataFrame is empty, return it without processing\n",
    "    if len(tool_df) == 0:\n",
    "        print(f\"Tool: {tool_df.name if hasattr(tool_df, 'name') else 'Unknown'} - No segments to evaluate\")\n",
    "        return tool_df\n",
    "        \n",
    "    # Add columns for evaluation metrics\n",
    "    tool_df['detected_truth'] = False\n",
    "    tool_df['overlap_pct'] = 0.0\n",
    "    tool_df['truth_id'] = None\n",
    "    \n",
    "    # Debug counters\n",
    "    total_segments = len(tool_df)\n",
    "    matched_segments = 0\n",
    "    \n",
    "    for idx, row in tool_df.iterrows():\n",
    "        # Check if we have both sample pair in regular and reversed order\n",
    "        sample_pairs = [\n",
    "            # Regular order\n",
    "            ((row['sample1'], row.get('sample1_haplotype', 0)), \n",
    "             (row['sample2'], row.get('sample2_haplotype', 0)),\n",
    "             row['chrom']),\n",
    "            # Reversed order\n",
    "            ((row['sample2'], row.get('sample2_haplotype', 0)), \n",
    "             (row['sample1'], row.get('sample1_haplotype', 0)),\n",
    "             row['chrom'])\n",
    "        ]\n",
    "        \n",
    "        found_match = False\n",
    "        for sample1, sample2, chrom in sample_pairs:\n",
    "            pair_key = (sample1, sample2)\n",
    "            if (pair_key, chrom) in truth_trees:\n",
    "                overlap_pct, truth_id = calculate_overlap(row, truth_trees[(pair_key, chrom)])\n",
    "                if overlap_pct > 0:\n",
    "                    tool_df.at[idx, 'overlap_pct'] = overlap_pct\n",
    "                    tool_df.at[idx, 'truth_id'] = truth_id\n",
    "                    tool_df.at[idx, 'detected_truth'] = (overlap_pct >= 0.5)  # Consider >=50% overlap a true positive\n",
    "                    matched_segments += 1 if overlap_pct >= 0.5 else 0\n",
    "                    found_match = True\n",
    "                    break\n",
    "        \n",
    "        # Try without haplotypes if no match found and haplotypes present\n",
    "        if not found_match and 'sample1_haplotype' in row:\n",
    "            # Create keys without haplotype information\n",
    "            sample_pairs_no_hap = [\n",
    "                ((row['sample1'], None), (row['sample2'], None), row['chrom']),\n",
    "                ((row['sample2'], None), (row['sample1'], None), row['chrom'])\n",
    "            ]\n",
    "            \n",
    "            for sample1, sample2, chrom in sample_pairs_no_hap:\n",
    "                pair_key = (sample1, sample2)\n",
    "                if (pair_key, chrom) in truth_trees:\n",
    "                    overlap_pct, truth_id = calculate_overlap(row, truth_trees[(pair_key, chrom)])\n",
    "                    if overlap_pct > 0:\n",
    "                        tool_df.at[idx, 'overlap_pct'] = overlap_pct\n",
    "                        tool_df.at[idx, 'truth_id'] = truth_id\n",
    "                        tool_df.at[idx, 'detected_truth'] = (overlap_pct >= 0.5)\n",
    "                        matched_segments += 1 if overlap_pct >= 0.5 else 0\n",
    "                        break\n",
    "    \n",
    "    # Get the tool name safely\n",
    "    tool_name = tool_df['tool'].iloc[0] if len(tool_df) > 0 else \"Unknown\"\n",
    "    \n",
    "    # Calculate percentage safely\n",
    "    percentage = (matched_segments/total_segments*100) if total_segments > 0 else 0\n",
    "    \n",
    "    print(f\"Tool: {tool_name} - Matched {matched_segments} of {total_segments} segments ({percentage:.2f}%)\")\n",
    "    return tool_df\n",
    "\n",
    "def evaluate_all_tools(refined_df, hap_df, ibis_df, truth_df):\n",
    "    \"\"\"Evaluate all IBD detection tools\"\"\"\n",
    "    # Create interval trees for truth segments\n",
    "    truth_trees = create_interval_tree(truth_df)\n",
    "    \n",
    "    # Evaluate each tool\n",
    "    refined_eval = evaluate_tool(refined_df, truth_trees)\n",
    "    \n",
    "    # Only evaluate tools with data\n",
    "    if len(hap_df) > 0:\n",
    "        hap_eval = evaluate_tool(hap_df, truth_trees)\n",
    "    else:\n",
    "        print(\"Tool: HapIBD - No segments to evaluate\")\n",
    "        hap_eval = hap_df.copy()\n",
    "        hap_eval['tool'] = 'HapIBD'  # Ensure tool column exists\n",
    "    \n",
    "    ibis_eval = evaluate_tool(ibis_df, truth_trees)\n",
    "    \n",
    "    # Combine results\n",
    "    all_results = pd.concat([refined_eval, hap_eval, ibis_eval], ignore_index=True)\n",
    "    \n",
    "    return all_results, truth_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate tools\n",
    "print(\"Evaluating IBD detection tools...\")\n",
    "all_results, truth_df = evaluate_all_tools(refined_df, hap_df, ibis_df, truth_df)\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_length_distribution(all_results, truth_df):\n",
    "    \"\"\"Plot the distribution of segment lengths for each tool and truth\"\"\"\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    \n",
    "    # Combine all data\n",
    "    all_data = pd.concat([\n",
    "        all_results[all_results['tool'] == 'RefinedIBD'][['length', 'tool']],\n",
    "        all_results[all_results['tool'] == 'HapIBD'][['length', 'tool']],\n",
    "        all_results[all_results['tool'] == 'IBIS'][['length', 'tool']],\n",
    "        truth_df[['length', 'tool']]\n",
    "    ])\n",
    "    \n",
    "    # Convert length from bp to Mbp\n",
    "    all_data['length'] = all_data['length'] / 1_000_000\n",
    "    \n",
    "    # Plot density\n",
    "    sns.kdeplot(data=all_data, x='length', hue='tool', fill=True, alpha=0.5)\n",
    "    \n",
    "    plt.title('Distribution of IBD Segment Lengths')\n",
    "    plt.xlabel('Segment Length (Mbp)')\n",
    "    plt.ylabel('Density')\n",
    "    plt.grid(True, linestyle='--', alpha=0.7)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(Path(results_directory) / 'ibd_length_distribution.png')\n",
    "    plt.show()\n",
    "\n",
    "def plot_precision_recall(all_results, truth_df):\n",
    "    \"\"\"Plot precision-recall curves for each tool\"\"\"\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    \n",
    "    has_data = False\n",
    "    # For each tool, calculate precision and recall using overlap percentage as score\n",
    "    for tool_name, color in zip(['RefinedIBD', 'HapIBD', 'IBIS'], ['blue', 'green', 'red']):\n",
    "        tool_df = all_results[all_results['tool'] == tool_name]\n",
    "        \n",
    "        # Skip if no data for this tool\n",
    "        if len(tool_df) == 0:\n",
    "            continue\n",
    "            \n",
    "        # Check if we have true positives\n",
    "        if tool_df['detected_truth'].sum() == 0:\n",
    "            print(f\"Warning: No true positives found for {tool_name}, skipping PR curve\")\n",
    "            continue\n",
    "            \n",
    "        has_data = True\n",
    "        \n",
    "        if 'LOD' in tool_df.columns and not tool_df['LOD'].isna().all():\n",
    "            score = tool_df['LOD']  # Use LOD score if available\n",
    "        else:\n",
    "            score = tool_df['length']  # Otherwise use length as a proxy for confidence\n",
    "            \n",
    "        y_true = tool_df['detected_truth'].astype(int)\n",
    "        \n",
    "        try:\n",
    "            # Calculate precision and recall\n",
    "            precision, recall, _ = precision_recall_curve(y_true, score)\n",
    "            avg_precision = average_precision_score(y_true, score)\n",
    "            \n",
    "            # Plot PR curve\n",
    "            plt.plot(recall, precision, lw=2, color=color,\n",
    "                     label=f'{tool_name} (AP={avg_precision:.2f})')\n",
    "        except Exception as e:\n",
    "            print(f\"Error plotting PR curve for {tool_name}: {e}\")\n",
    "    \n",
    "    if has_data:\n",
    "        plt.title('Precision-Recall Curves for IBD Detection Tools')\n",
    "        plt.xlabel('Recall')\n",
    "        plt.ylabel('Precision')\n",
    "        plt.xlim([0.0, 1.0])\n",
    "        plt.ylim([0.0, 1.05])\n",
    "        plt.grid(True, linestyle='--', alpha=0.7)\n",
    "        plt.legend(loc=\"best\")\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(Path(results_directory) / 'ibd_precision_recall.png')\n",
    "        plt.show()\n",
    "    else:\n",
    "        print(\"Skipping PR curve plot: No valid data\")\n",
    "\n",
    "def plot_roc_curves(all_results, truth_df):\n",
    "    \"\"\"Plot ROC curves for each tool\"\"\"\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    \n",
    "    has_data = False\n",
    "    # For each tool, calculate ROC using overlap percentage as score\n",
    "    for tool_name, color in zip(['RefinedIBD', 'HapIBD', 'IBIS'], ['blue', 'green', 'red']):\n",
    "        tool_df = all_results[all_results['tool'] == tool_name]\n",
    "        \n",
    "        # Skip if no data for this tool\n",
    "        if len(tool_df) == 0:\n",
    "            continue\n",
    "            \n",
    "        # Check if we have true positives\n",
    "        if tool_df['detected_truth'].sum() == 0:\n",
    "            print(f\"Warning: No true positives found for {tool_name}, skipping ROC curve\")\n",
    "            continue\n",
    "            \n",
    "        has_data = True\n",
    "        \n",
    "        if 'LOD' in tool_df.columns and not tool_df['LOD'].isna().all():\n",
    "            score = tool_df['LOD']  # Use LOD score if available\n",
    "        else:\n",
    "            score = tool_df['length']  # Otherwise use length as a proxy for confidence\n",
    "            \n",
    "        y_true = tool_df['detected_truth'].astype(int)\n",
    "        \n",
    "        try:\n",
    "            # Calculate ROC\n",
    "            fpr, tpr, _ = roc_curve(y_true, score)\n",
    "            roc_auc = auc(fpr, tpr)\n",
    "            \n",
    "            # Plot ROC curve\n",
    "            plt.plot(fpr, tpr, lw=2, color=color,\n",
    "                     label=f'{tool_name} (AUC={roc_auc:.2f})')\n",
    "        except Exception as e:\n",
    "            print(f\"Error plotting ROC curve for {tool_name}: {e}\")\n",
    "    \n",
    "    if has_data:\n",
    "        plt.plot([0, 1], [0, 1], linestyle='--', lw=2, color='gray', alpha=0.8)\n",
    "        plt.title('ROC Curves for IBD Detection Tools')\n",
    "        plt.xlabel('False Positive Rate')\n",
    "        plt.ylabel('True Positive Rate')\n",
    "        plt.xlim([0.0, 1.0])\n",
    "        plt.ylim([0.0, 1.05])\n",
    "        plt.grid(True, linestyle='--', alpha=0.7)\n",
    "        plt.legend(loc=\"lower right\")\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(Path(results_directory) / 'ibd_roc_curves.png')\n",
    "        plt.show()\n",
    "    else:\n",
    "        print(\"Skipping ROC curve plot: No valid data\")\n",
    "\n",
    "def plot_overlap_histogram(all_results):\n",
    "    \"\"\"Plot histogram of overlap percentages for each tool\"\"\"\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    \n",
    "    for tool_name, color in zip(['RefinedIBD', 'HapIBD', 'IBIS'], ['blue', 'green', 'red']):\n",
    "        tool_df = all_results[all_results['tool'] == tool_name]\n",
    "        plt.hist(tool_df['overlap_pct'], bins=20, alpha=0.5, color=color, label=tool_name)\n",
    "    \n",
    "    plt.title('Distribution of Overlap with Truth Segments')\n",
    "    plt.xlabel('Overlap Percentage')\n",
    "    plt.ylabel('Count')\n",
    "    plt.grid(True, linestyle='--', alpha=0.7)\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(Path(results_directory) / 'ibd_overlap_histogram.png')\n",
    "    plt.show()\n",
    "    \n",
    "# Generate visualizations\n",
    "print(\"Generating visualizations...\")\n",
    "plot_length_distribution(all_results, truth_df)\n",
    "plot_precision_recall(all_results, truth_df)\n",
    "plot_roc_curves(all_results, truth_df)\n",
    "plot_overlap_histogram(all_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_summary_metrics(all_results, truth_df):\n",
    "    \"\"\"Calculate summary statistics for each tool\"\"\"\n",
    "    metrics = []\n",
    "    \n",
    "    # Count total truth segments\n",
    "    total_truth = len(truth_df)\n",
    "    print(f\"Total truth segments: {total_truth}\")\n",
    "    \n",
    "    for tool_name in ['RefinedIBD', 'HapIBD', 'IBIS']:\n",
    "        tool_df = all_results[all_results['tool'] == tool_name]\n",
    "        \n",
    "        if len(tool_df) == 0:\n",
    "            print(f\"No segments found for {tool_name}, skipping metrics\")\n",
    "            continue\n",
    "        \n",
    "        # Count true positives (segments with >50% overlap)\n",
    "        true_positives = tool_df['detected_truth'].sum()\n",
    "        \n",
    "        # Count false positives (segments with ≤50% overlap)\n",
    "        false_positives = len(tool_df) - true_positives\n",
    "        \n",
    "        # Count truth segments detected by this tool\n",
    "        detected_truth_ids = set([x for x in tool_df['truth_id'] if x is not None])\n",
    "        detected_truths = len(detected_truth_ids)\n",
    "        \n",
    "        # Calculate recall (proportion of truth segments detected)\n",
    "        recall = detected_truths / total_truth if total_truth > 0 else 0\n",
    "        \n",
    "        # Calculate precision (proportion of detected segments that are true)\n",
    "        precision = true_positives / len(tool_df) if len(tool_df) > 0 else 0\n",
    "        \n",
    "        # Calculate F1 score\n",
    "        f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "        \n",
    "        # Print detailed debug info\n",
    "        print(f\"\\n{tool_name} Summary:\")\n",
    "        print(f\"  Total segments: {len(tool_df)}\")\n",
    "        print(f\"  True positives: {true_positives} ({true_positives/len(tool_df)*100:.2f}% of segments)\")\n",
    "        print(f\"  False positives: {false_positives} ({false_positives/len(tool_df)*100:.2f}% of segments)\")\n",
    "        print(f\"  Unique truth segments detected: {detected_truths} ({detected_truths/total_truth*100:.2f}% of truth)\")\n",
    "        print(f\"  Precision: {precision:.4f}\")\n",
    "        print(f\"  Recall: {recall:.4f}\")\n",
    "        print(f\"  F1 Score: {f1:.4f}\")\n",
    "        \n",
    "        metrics.append({\n",
    "            'Tool': tool_name,\n",
    "            'Total Segments': len(tool_df),\n",
    "            'True Positives': true_positives,\n",
    "            'False Positives': false_positives,\n",
    "            'Detected Truth Segments': detected_truths,\n",
    "            'Precision': precision,\n",
    "            'Recall': recall,\n",
    "            'F1 Score': f1\n",
    "        })\n",
    "    \n",
    "    metrics_df = pd.DataFrame(metrics)\n",
    "    \n",
    "    # Check if we have meaningful metrics\n",
    "    if len(metrics_df) == 0 or metrics_df['True Positives'].sum() == 0:\n",
    "        print(\"\\n⚠️ WARNING: No true positives detected across any tool!\")\n",
    "        print(\"This suggests an issue with matching segments to ground truth.\")\n",
    "        print(\"Possible causes:\")\n",
    "        print(\"1. Column name mismatches between tool output and truth data\")\n",
    "        print(\"2. Sample ID format differences\")\n",
    "        print(\"3. Chromosome notation differences (e.g., 'chr1' vs '1')\")\n",
    "        print(\"4. Position coordinate system differences\")\n",
    "    \n",
    "    return metrics_df\n",
    "\n",
    "# Calculate and display summary metrics\n",
    "print(\"Calculating summary metrics...\")\n",
    "metrics_df = calculate_summary_metrics(all_results, truth_df)\n",
    "display(metrics_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_summary_barplot(metrics_df):\n",
    "    \"\"\"Plot summary metrics as a bar chart\"\"\"\n",
    "    plt.figure(figsize=(14, 10))\n",
    "    \n",
    "    # Melt the dataframe to make it suitable for grouped bar chart\n",
    "    plot_metrics = ['Precision', 'Recall', 'F1 Score']\n",
    "    plot_df = pd.melt(metrics_df, id_vars=['Tool'], value_vars=plot_metrics, \n",
    "                      var_name='Metric', value_name='Value')\n",
    "    \n",
    "    # Create grouped bar chart\n",
    "    sns.barplot(x='Tool', y='Value', hue='Metric', data=plot_df)\n",
    "    \n",
    "    plt.title('Performance Metrics by IBD Detection Tool')\n",
    "    plt.ylabel('Score')\n",
    "    plt.ylim(0, 1)\n",
    "    plt.grid(True, linestyle='--', alpha=0.7, axis='y')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(Path(results_directory) / 'ibd_performance_metrics.png')\n",
    "    plt.show()\n",
    "    \n",
    "plot_summary_barplot(metrics_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_accuracy_by_length(all_results, truth_df, sample_size=None):\n",
    "    \"\"\"\n",
    "    Plot detection accuracy as a function of segment length.\n",
    "    \n",
    "    Args:\n",
    "        all_results: DataFrame with IBD detection results\n",
    "        truth_df: DataFrame with ground truth segments\n",
    "        sample_size: If provided, limit analysis to this many truth segments for faster execution\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    \n",
    "    # Bin truth segments by length\n",
    "    bins = [\n",
    "        0,\n",
    "        2e6,   # 2 Mb\n",
    "        3e6,\n",
    "        4e6,\n",
    "        5e6,\n",
    "        6e6,\n",
    "        7e6,\n",
    "        8e6,\n",
    "        9e6,\n",
    "        10e6,  # 10 Mb\n",
    "        15e6,  # 15 Mb\n",
    "        20e6,  # 20 Mb\n",
    "        float('inf')\n",
    "    ]\n",
    "    bin_labels = [\n",
    "        '<2Mb',\n",
    "        '2–3Mb',\n",
    "        '3–4Mb',\n",
    "        '4–5Mb',\n",
    "        '5–6Mb',\n",
    "        '6–7Mb',\n",
    "        '7–8Mb',\n",
    "        '8–9Mb',\n",
    "        '9–10Mb',\n",
    "        '10–15Mb',\n",
    "        '15–20Mb',\n",
    "        '>20Mb'\n",
    "    ]\n",
    "    \n",
    "    # Add length bins to truth dataframe\n",
    "    truth_df['length_bin'] = pd.cut(truth_df['length'], bins=bins, labels=bin_labels)\n",
    "    \n",
    "    # Optionally sample the truth data for faster execution\n",
    "    if sample_size is not None and len(truth_df) > sample_size:\n",
    "        # Stratified sampling to maintain distribution across bins\n",
    "        truth_df = truth_df.groupby('length_bin', observed=True).apply(\n",
    "            lambda x: x.sample(min(len(x), max(1, int(sample_size * len(x) / len(truth_df)))))\n",
    "        ).reset_index(drop=True)\n",
    "        print(f\"Using a stratified sample of {len(truth_df)} truth segments for faster execution\")\n",
    "    \n",
    "    # Count total truth segments per bin\n",
    "    truth_counts = truth_df.groupby('length_bin', observed=True).size()\n",
    "    \n",
    "    # For each tool, calculate detection rate by length bin using vectorized operations\n",
    "    for tool_name, color in zip(['RefinedIBD', 'HapIBD', 'IBIS'], ['blue', 'green', 'red']):\n",
    "        detection_rates = []\n",
    "        \n",
    "        # Get the tool's dataframe\n",
    "        tool_df = all_results[all_results['tool'] == tool_name]\n",
    "        if len(tool_df) == 0:\n",
    "            continue\n",
    "            \n",
    "        # Create a set of truth_ids detected by this tool for faster lookup\n",
    "        detected_truth_ids = set(tool_df['truth_id'].dropna())\n",
    "        \n",
    "        for bin_label in bin_labels:\n",
    "            # Get truth segments in this bin\n",
    "            bin_truth = truth_df[truth_df['length_bin'] == bin_label]\n",
    "            total = len(bin_truth)\n",
    "            \n",
    "            if total == 0:\n",
    "                detection_rates.append(0)\n",
    "                continue\n",
    "            \n",
    "            # Count how many were detected by this tool - vectorized approach\n",
    "            detected = sum(id in detected_truth_ids for id in bin_truth['segment_id'])\n",
    "            detection_rates.append(detected / total if total > 0 else 0)\n",
    "        \n",
    "        plt.plot(bin_labels, detection_rates, marker='o', label=tool_name, color=color, linewidth=2)\n",
    "    \n",
    "    plt.title('IBD Detection Rate by Segment Length')\n",
    "    plt.xlabel('Segment Length')\n",
    "    plt.ylabel('Detection Rate')\n",
    "    plt.ylim(0, 1.05)\n",
    "    plt.grid(True, linestyle='--', alpha=0.7)\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(Path(results_directory) / 'ibd_detection_by_length.png')\n",
    "    plt.show()\n",
    "    \n",
    "    # Also create a bar chart showing segment counts by length\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    truth_counts.plot(kind='bar', color='purple')\n",
    "    plt.title('Number of Truth Segments by Length')\n",
    "    plt.xlabel('Segment Length')\n",
    "    plt.ylabel('Count')\n",
    "    plt.grid(True, linestyle='--', alpha=0.7, axis='y')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(Path(results_directory) / 'ibd_truth_segments_by_length.png')\n",
    "    plt.show()\n",
    "\n",
    "# Choose one of these calls:\n",
    "\n",
    "# 1. Full analysis (takes ~76 minutes)\n",
    "plot_accuracy_by_length(all_results, truth_df)\n",
    "\n",
    "# 2. Faster version with sampling (takes ~5-10 minutes)\n",
    "# plot_accuracy_by_length(all_results, truth_df, sample_size=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Attempt to convert this notebook to PDF using the Poetry environment\n",
    "# Note: The notebook's CWD should be the project's base directory now.\n",
    "print(f\"Current directory for conversion: {os.getcwd()}\")\n",
    "notebook_path = \"labs/Lab8_Evaluate_IBD_Detection.ipynb\"\n",
    "\n",
    "if Path(notebook_path).exists():\n",
    "    print(f\"Converting {notebook_path} to PDF...\")\n",
    "    !poetry run jupyter nbconvert --to pdf \"{notebook_path}\"\n",
    "else:\n",
    "    print(f\"Error: Notebook not found at relative path {notebook_path} from CWD {os.getcwd()}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
