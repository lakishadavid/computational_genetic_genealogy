{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Lab 14: Data Structures and Algorithmic Design in Bonsai (Updated for v3)\n\nBuilding upon our exploration of the mathematical foundations of Bonsai in Lab 13, we now delve into the data structures and algorithmic design that power Bonsai's pedigree reconstruction capabilities. This lab focuses on how Bonsai v3 organizes and processes genetic data for efficient relationship inference and pedigree construction.\n\n> **Why This Matters:** The efficacy of pedigree reconstruction algorithms depends heavily on the design of their underlying data structures. Understanding how Bonsai v3 organizes and processes genetic data allows researchers to optimize algorithm performance, implement custom extensions, and interpret complex outputs accurately.\n\n**Learning Objectives**:\n- Analyze the core data structures that power the Bonsai v3 algorithm\n- Understand how both phased and unphased IBD segment data is processed\n- Explore the up-node dictionary structure and its role in representing pedigree relationships\n- Examine the graph-theoretical foundations of pedigree representation\n- Learn how Bonsai's modular architecture enables efficient search and optimization\n- Implement key v3 data structures and operations used in pedigree reconstruction",
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!poetry install --no-root"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": "import os\nimport sys\nimport math\nimport random\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport networkx as nx\nfrom scipy.stats import poisson, expon, norm, multivariate_normal\nfrom collections import defaultdict, deque\nfrom pathlib import Path\nfrom IPython.display import display, HTML\nfrom dotenv import load_dotenv\nimport heapq\nfrom typing import Any, Dict, List, Set, Tuple, Optional, FrozenSet\n\n# Constants matching v3 implementation\nINF = float('inf')\nMIN_SEG_LEN = 0\nMAX_CON_PTS = INF\nRESTRICT_CON_PTS = True\nCONNECT_UP_ONLY = False\nMAX_PEDS = 3\nMAX_START_PEDS = 3\nMEAN_BGD_NUM = 0.01  # Background IBD parameters\nMEAN_BGD_LEN = 5"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_comp_gen_dir():\n",
    "    \"\"\"Find the computational_genetic_genealogy directory by searching up from current directory.\"\"\"\n",
    "    current = Path.cwd()\n",
    "    \n",
    "    # Search up through parent directories\n",
    "    while current != current.parent:\n",
    "        # Check if target directory exists in current path\n",
    "        target = current / 'computational_genetic_genealogy'\n",
    "        if target.is_dir():\n",
    "            return target\n",
    "        # Move up one directory\n",
    "        current = current.parent\n",
    "    \n",
    "    raise FileNotFoundError(\"Could not find computational_genetic_genealogy directory\")\n",
    "\n",
    "def load_env_file():\n",
    "    \"\"\"Find and load the .env file from the computational_genetic_genealogy directory.\"\"\"\n",
    "    try:\n",
    "        # Find the computational_genetic_genealogy directory\n",
    "        comp_gen_dir = find_comp_gen_dir()\n",
    "        \n",
    "        # Look for .env file\n",
    "        env_path = comp_gen_dir / '.env'\n",
    "        if not env_path.exists():\n",
    "            print(f\"Warning: No .env file found in {comp_gen_dir}\")\n",
    "            return None\n",
    "        \n",
    "        # Load the .env file\n",
    "        load_dotenv(env_path, override=True)\n",
    "        print(f\"Loaded environment variables from: {env_path}\")\n",
    "        return env_path\n",
    "        \n",
    "    except FileNotFoundError as e:\n",
    "        print(f\"Error: {e}\")\n",
    "        return None\n",
    "\n",
    "# Use the function\n",
    "env_path = load_env_file()\n",
    "\n",
    "working_directory = os.getenv('PROJECT_WORKING_DIR', default=None)\n",
    "data_directory = os.getenv('PROJECT_DATA_DIR', default=None)\n",
    "references_directory = os.getenv('PROJECT_REFERENCES_DIR', default=None)\n",
    "results_directory = os.getenv('PROJECT_RESULTS_DIR', default=None)\n",
    "utils_directory = os.getenv('PROJECT_UTILS_DIR', default=None)\n",
    "\n",
    "os.environ[\"WORKING_DIRECTORY\"] = working_directory\n",
    "os.environ[\"DATA_DIRECTORY\"] = data_directory\n",
    "os.environ[\"REFERENCES_DIRECTORY\"] = references_directory\n",
    "os.environ[\"RESULTS_DIRECTORY\"] = results_directory\n",
    "os.environ[\"UTILS_DIRECTORY\"] = utils_directory\n",
    "\n",
    "print(f\"Working Directory: {working_directory}\")\n",
    "print(f\"Data Directory: {data_directory}\")\n",
    "print(f\"References Directory: {references_directory}\")\n",
    "print(f\"Results Directory: {results_directory}\")\n",
    "print(f\"Utils Directory: {utils_directory}\")\n",
    "\n",
    "os.chdir(working_directory)\n",
    "print(f\"The current directory is {os.getcwd()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 1. Core Data Structures in Bonsai v3\n\nBonsai v3 introduces several improvements to its core data structures, making them more modular, efficient, and adaptable to different types of genetic data. Let's examine the key structures that power the algorithm.",
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### IBD Segment Representation\n\nThe fundamental input to Bonsai is Identity-By-Descent (IBD) segment data. In v3, Bonsai supports both phased and unphased IBD segments with well-defined format conventions:",
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": "class IBDSegment:\n    def __init__(self, ind1, ind2, chrom, start_pos, end_pos, is_ibd2, length_cm):\n        self.ind1 = ind1          # First individual ID\n        self.ind2 = ind2          # Second individual ID\n        self.chrom = chrom        # Chromosome number\n        self.start_pos = start_pos  # Start position (base pairs)\n        self.end_pos = end_pos    # End position (base pairs)\n        self.is_ibd2 = is_ibd2    # Whether this is an IBD2 segment\n        self.length_cm = length_cm  # Genetic length in centiMorgans\n    \n    def __repr__(self):\n        return f\"IBDSegment({self.ind1}, {self.ind2}, chr{self.chrom}, {self.length_cm:.2f}cM, {'IBD2' if self.is_ibd2 else 'IBD1'})\"\n\n# In v3 Bonsai, segments are typically stored in array format rather than as objects:\n# Unphased format: [id1, id2, chromosome, start_bp, end_bp, is_full_ibd, seg_cm]\n# Phased format: [id1, id2, hap1, hap2, chromosome, start_cm, end_cm, seg_cm]\n\ndef get_unphased_to_phased(unphased_ibd_seg_list):\n    \"\"\"\n    Convert unphased IBD segments to phased IBD segments.\n    \n    Args:\n        unphased_ibd_seg_list: List of the form \n            [[id1, id2, chromosome, start_bp, end_bp, is_full_ibd, seg_len_cm],...]\n            \n    Returns:\n        phased_ibd_seg_list: List of the form \n            [[id1, id2, hap1, hap2, chromosome, start_cm, end_cm, cm],...]\n    \"\"\"\n    phased_ibd_seg_list = []\n    for seg in unphased_ibd_seg_list:\n        # In a real implementation, we would convert physical to genetic positions\n        # For this example, we'll make a simplified conversion\n        id1, id2, chrom, start_bp, end_bp, is_full, seg_len_cm = seg\n        \n        # Simplified conversion (in practice would use genetic map)\n        start_cm = start_bp / 1_000_000  # Simple conversion factor\n        end_cm = end_bp / 1_000_000\n        \n        # For phased data, we assign haplotype 0,0 for simplicity\n        phased_ibd_seg_list.append([id1, id2, 0, 0, chrom, start_cm, end_cm, seg_len_cm])\n    return phased_ibd_seg_list\n\ndef get_phased_to_unphased(phased_ibd_seg_list):\n    \"\"\"\n    Convert phased IBD segments to unphased IBD segments.\n    \n    Args:\n        phased_ibd_seg_list: List of the form \n            [[id1, id2, hap1, hap2, chromosome, start_cm, end_cm, seg_len_cm],...]\n            \n    Returns:\n        unphased_ibd_seg_list: List of the form \n            [[id1, id2, chromosome, start_bp, end_bp, is_full_ibd, seg_len_cm],...]\n    \"\"\"\n    unphased_ibd_seg_list = []\n    for seg in phased_ibd_seg_list:\n        id1, id2, hap1, hap2, chrom, start_cm, end_cm, seg_len_cm = seg\n        \n        # Simplified conversion (in practice would use genetic map)\n        start_bp = int(start_cm * 1_000_000)  # Simple conversion factor\n        end_bp = int(end_cm * 1_000_000)\n        \n        # Determine if segment is full (simplified logic)\n        is_full = False  # By default, we assume IBD1\n        \n        unphased_ibd_seg_list.append([id1, id2, chrom, start_bp, end_bp, is_full, seg_len_cm])\n    return unphased_ibd_seg_list\n\n# Example of how segments are organized in Bonsai v3\ndef organize_segments_by_pair(ibd_segments):\n    \"\"\"\n    Organize IBD segments by pair for efficient access.\n    \n    This is similar to get_pair_to_seg_dict in v3's ibd.py\n    \"\"\"\n    segments_by_pair = {}\n    for seg in ibd_segments:\n        # For v3 compatibility, we support both object-based and array-based segments\n        if isinstance(seg, IBDSegment):\n            id1, id2 = seg.ind1, seg.ind2\n        else:\n            id1, id2 = seg[0], seg[1]\n            \n        pair = frozenset({id1, id2})  # v3 uses frozenset for pair keys\n        if pair not in segments_by_pair:\n            segments_by_pair[pair] = []\n        segments_by_pair[pair].append(seg)\n    return segments_by_pair"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create some example IBD segments to work with throughout this lab:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": "# Create example IBD segments (class-based format for visualization)\nexample_segments = [\n    # Parent-child segments (extensive IBD1 sharing)\n    IBDSegment(1000, 1001, 1, 10000000, 50000000, False, 45.5),  # IBD1 segment on chr1\n    IBDSegment(1000, 1001, 1, 60000000, 125000000, False, 65.2),  # IBD1 segment on chr1\n    IBDSegment(1000, 1001, 2, 5000000, 80000000, False, 75.0),   # IBD1 segment on chr2\n    \n    # Sibling segments (mix of IBD1 and IBD2)\n    IBDSegment(1001, 1002, 1, 15000000, 45000000, True, 30.1),   # IBD2 segment on chr1\n    IBDSegment(1001, 1002, 1, 50000000, 90000000, False, 40.3),  # IBD1 segment on chr1\n    IBDSegment(1001, 1002, 2, 10000000, 45000000, False, 35.5),  # IBD1 segment on chr2\n    IBDSegment(1001, 1002, 2, 50000000, 70000000, True, 20.8),   # IBD2 segment on chr2\n    \n    # First cousin segments (modest IBD1 sharing)\n    IBDSegment(1000, 1003, 1, 25000000, 50000000, False, 25.0),  # IBD1 segment on chr1\n    IBDSegment(1000, 1003, 3, 15000000, 35000000, False, 20.5),  # IBD1 segment on chr3\n    \n    # Distant relative (single small segment)\n    IBDSegment(1002, 1004, 5, 30000000, 45000000, False, 15.2),  # IBD1 segment on chr5\n]\n\n# Convert to array format used by v3\nexample_segments_array = []\nfor seg in example_segments:\n    example_segments_array.append([\n        seg.ind1, seg.ind2, seg.chrom, \n        seg.start_pos, seg.end_pos, \n        seg.is_ibd2, seg.length_cm\n    ])\n\n# Organize segments by pair\nsegments_by_pair = organize_segments_by_pair(example_segments)\n\n# Display the organized segments\nfor pair, segs in segments_by_pair.items():\n    print(f\"Pair {pair}:\")\n    for seg in segs:\n        print(f\"  {seg}\")\n    print()\n\n# Also process the IBD statistics in v3 format\ndef get_ibd_stats_unphased(unphased_ibd_segs):\n    \"\"\"Get IBD statistics from unphased segment data (v3 format)\"\"\"\n    ibd_stats = defaultdict(lambda: {\n        'total_half': 0,\n        'total_full': 0,\n        'num_half': 0,\n        'num_full': 0,\n        'max_seg_cm': 0})\n\n    for s in unphased_ibd_segs:\n        id1, id2, chromosome, start, end, is_full_ibd, seg_cm = s\n        key = frozenset({id1, id2})\n        ibd_stats[key]['total_half'] += (seg_cm if not is_full_ibd else 0)\n        ibd_stats[key]['total_full'] += (seg_cm if is_full_ibd else 0)\n        ibd_stats[key]['num_half'] += int(not is_full_ibd)\n        ibd_stats[key]['num_full'] += int(is_full_ibd)\n        ibd_stats[key]['max_seg_cm'] = max(ibd_stats[key]['max_seg_cm'], seg_cm)\n\n    return ibd_stats\n\n# Get IBD statistics\nibd_stats = get_ibd_stats_unphased(example_segments_array)\n\n# Display some IBD statistics\nprint(\"\\nIBD Statistics (v3 format):\")\nfor pair, stats in ibd_stats.items():\n    ids = list(pair)\n    print(f\"Pair {ids[0]}-{ids[1]}:\")\n    print(f\"  Total IBD1: {stats['total_half']:.2f} cM\")\n    print(f\"  Total IBD2: {stats['total_full']:.2f} cM\")\n    print(f\"  IBD1 segments: {stats['num_half']}\")\n    print(f\"  IBD2 segments: {stats['num_full']}\")\n    print(f\"  Max segment: {stats['max_seg_cm']:.2f} cM\")\n    print()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This organization allows Bonsai to efficiently access all IBD segments between a specific pair of individuals, which is crucial for calculating relationship likelihoods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### The Up-Node Dictionary in Bonsai v3\n\nThe core data structure for representing pedigrees in Bonsai is the \"up-node dictionary.\" In v3, this structure is used consistently throughout the codebase:",
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example up-node dictionary structure\n",
    "up_node_dict = {\n",
    "    1000: {1001: 1, 1002: 1},  # Individual 1000 has parents 1001 and 1002\n",
    "    1003: {1001: 1, 1002: 1},  # Individual 1003 has the same parents\n",
    "    1004: {-1: 1, -2: 1},      # Individual 1004 has inferred parents -1 and -2\n",
    "    -1: {1005: 1, 1006: 1},    # Inferred individual -1 has parents 1005 and 1006\n",
    "    1005: {},                  # Individual 1005 has no recorded parents\n",
    "    1006: {},                  # Individual 1006 has no recorded parents\n",
    "    1001: {},                  # Individual 1001 has no recorded parents\n",
    "    1002: {}                   # Individual 1002 has no recorded parents\n",
    "}\n",
    "\n",
    "# Function to visualize pedigree from up-node dictionary\n",
    "def visualize_pedigree(up_node_dict):\n",
    "    \"\"\"Create a visualization of the pedigree from an up-node dictionary.\"\"\"\n",
    "    # Create a directed graph\n",
    "    G = nx.DiGraph()\n",
    "    \n",
    "    # Add nodes and edges\n",
    "    for child, parents in up_node_dict.items():\n",
    "        G.add_node(child)\n",
    "        for parent in parents:\n",
    "            G.add_node(parent)\n",
    "            G.add_edge(parent, child)  # Direction from parent to child\n",
    "    \n",
    "    # Set node colors: green for real individuals (positive IDs), gray for latent (negative IDs)\n",
    "    node_colors = ['lightgreen' if isinstance(node, int) and node > 0 else 'lightgray' \n",
    "                   for node in G.nodes()]\n",
    "    \n",
    "    # Calculate layout\n",
    "    try:\n",
    "        pos = nx.nx_agraph.graphviz_layout(G, prog='dot')\n",
    "    except:\n",
    "        pos = nx.spring_layout(G)  # Fallback if graphviz is not available\n",
    "    \n",
    "    # Draw the graph\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    nx.draw(G, pos, with_labels=True, node_color=node_colors, \n",
    "            node_size=700, font_size=10, arrows=True)\n",
    "    plt.title('Pedigree Structure')\n",
    "    plt.show()\n",
    "\n",
    "# Visualize the example pedigree\n",
    "visualize_pedigree(up_node_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Key features of the up-node dictionary:\n",
    "- Each key represents an individual ID (positive for observed individuals, negative for inferred ancestors)\n",
    "- Each value is a dictionary mapping parent IDs to 1 (the value 1 is a placeholder; the structure could be extended to include additional information)\n",
    "- An empty dictionary indicates an individual with no recorded parents (either a founder or an individual with unknown parentage)\n",
    "- The structure supports efficient traversal of ancestors and descendants\n",
    "- It can represent complex multi-generational pedigrees with inferred latent ancestors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BioInfo Structure\n",
    "\n",
    "Bonsai incorporates biological metadata about individuals through the BioInfo structure:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example BioInfo structure\n",
    "bio_info = [\n",
    "    {'genotype_id': 1000, 'age': 75, 'sex': 'F'},\n",
    "    {'genotype_id': 1001, 'age': 80, 'sex': 'M'},\n",
    "    {'genotype_id': 1002, 'age': 78, 'sex': 'F'},\n",
    "    {'genotype_id': 1003, 'age': 45, 'sex': 'M'},\n",
    "    {'genotype_id': 1004, 'age': 43, 'sex': 'F'},\n",
    "    {'genotype_id': 1005, 'age': 20, 'sex': 'M'}\n",
    "]\n",
    "\n",
    "# Display the information\n",
    "bio_df = pd.DataFrame(bio_info)\n",
    "bio_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This structure is used to enforce biological constraints (e.g., age-appropriate relationships, sex-specific reproduction) and to enhance the accuracy of pedigree reconstruction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Preprocessing IBD Data for Efficient Access\n",
    "\n",
    "Before pedigree reconstruction begins, Bonsai preprocesses the IBD data to optimize subsequent operations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Indexing and Filtering\n",
    "\n",
    "The preprocessing pipeline includes several key steps:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": "from IPython.display import display\n\n# Add a new cell with the v3 PwLogLike class implementation\nclass PwLogLike:\n    \"\"\"Class for computing pairwise likelihoods between individuals.\"\"\"\n    \n    def __init__(self, bio_info, unphased_ibd_seg_list, condition_pair_set=None,\n                 mean_bgd_num=MEAN_BGD_NUM, mean_bgd_len=MEAN_BGD_LEN):\n        \"\"\"Initialize with biological info and IBD segments.\"\"\"\n        self.bio_info = bio_info\n        self.id_to_info = {info['genotype_id']: info for info in bio_info}\n        self.ibd_stat_dict = get_ibd_stats_unphased(unphased_ibd_seg_list)\n        self.condition_pair_set = condition_pair_set or set()\n        self.mean_bgd_num = mean_bgd_num\n        self.mean_bgd_len = mean_bgd_len\n        \n    def get_log_like(self, id1, id2, relationship_tuple, condition=False):\n        \"\"\"\n        Calculate log likelihood of a specific relationship between two individuals.\n        \n        Args:\n            id1, id2: IDs of the two individuals\n            relationship_tuple: (up, down, num_ancs) tuple representing the relationship\n            condition: Whether to condition on observing IBD\n            \n        Returns:\n            Log likelihood of the relationship\n        \"\"\"\n        # This is a simplified version of the actual likelihood calculation\n        # In a real implementation, this would use moments-based models\n        \n        pair_key = frozenset({id1, id2})\n        ibd_stats = self.ibd_stat_dict.get(pair_key, {\n            'total_half': 0, 'total_full': 0, \n            'num_half': 0, 'num_full': 0, 'max_seg_cm': 0\n        })\n        \n        up, down, num_ancs = relationship_tuple\n        \n        # Simple likelihood model (for demonstration)\n        # In reality, this would be a complex statistical model\n        total_meioses = up + down\n        \n        # Expected values based on relationship\n        if total_meioses == 0 and num_ancs == 1:  # Same person\n            expected_half = 0\n            expected_full = 3600  # Full genome\n        elif total_meioses == 1 and num_ancs == 1:  # Parent-child\n            expected_half = 3600\n            expected_full = 0\n        elif total_meioses == 2 and num_ancs == 2:  # Full siblings\n            expected_half = 2700\n            expected_full = 900\n        elif total_meioses == 2 and num_ancs == 1:  # Half siblings\n            expected_half = 1800\n            expected_full = 0\n        elif total_meioses == 3 and num_ancs == 1:  # First cousins\n            expected_half = 900\n            expected_full = 0\n        else:\n            # Exponential decay model for distant relationships\n            expected_half = 3600 * (0.5 ** (total_meioses - 1)) * num_ancs\n            expected_full = 0\n        \n        # Calculate log likelihood using normal distribution (simplified)\n        observed_half = ibd_stats['total_half']\n        observed_full = ibd_stats['total_full']\n        \n        # Standard deviations (simplified model)\n        sd_half = max(50, expected_half * 0.1)\n        sd_full = max(50, expected_full * 0.15)\n        \n        # Calculate log likelihood components\n        if expected_half > 0:\n            ll_half = -0.5 * ((observed_half - expected_half) / sd_half) ** 2 - math.log(sd_half)\n        else:\n            ll_half = 0\n            \n        if expected_full > 0:\n            ll_full = -0.5 * ((observed_full - expected_full) / sd_full) ** 2 - math.log(sd_full)\n        else:\n            ll_full = 0\n        \n        # Add background IBD model if condition=True\n        if condition and total_meioses > 4:\n            # Add likelihood of background IBD\n            bgd_ll = math.log(self.mean_bgd_num) + math.log(self.mean_bgd_len)\n            return ll_half + ll_full + bgd_ll\n        \n        return ll_half + ll_full\n\n# Create an instance of the v3 likelihood calculator\npw_ll_cls = PwLogLike(bio_info, example_segments_array)\n\n# Test with some example relationship calculations\nrelationships = [\n    (1000, 1001, (1, 0, 1), \"Parent-Child\"),\n    (1001, 1002, (0, 0, 2), \"Spouses\"),\n    (1000, 1003, (1, 1, 2), \"Siblings\"),\n    (1000, 1004, (2, 2, 2), \"First Cousins\")\n]\n\nprint(\"V3 Likelihood Calculations:\")\nfor id1, id2, rel_tuple, rel_name in relationships:\n    ll = pw_ll_cls.get_log_like(id1, id2, rel_tuple)\n    print(f\"{rel_name} log-likelihood ({id1}-{id2}): {ll:.2f}\")\n\n# Display new v3 function for initializing pedigree structures\ndef initialize_input_dicts(bio_info):\n    \"\"\"\n    For a list of individual IDs, set up the necessary data structures to\n    combine them into a pedigree.\n    \n    Args:\n        bio_info: List of dicts with genotype_id, sex, age information\n        \n    Returns:\n        idx_to_up_dict_ll_list: Dict mapping pedigree indices to list of pedigrees and log likelihoods\n        id_to_idx: Dict mapping IDs to pedigree indices\n        idx_to_id_set: Dict mapping pedigree indices to sets of IDs\n    \"\"\"\n    idx = 0\n    id_to_idx = {}\n    idx_to_id_set = {}\n    idx_to_up_dict_ll_list = {}\n    \n    for info in bio_info:\n        idx += 1\n        gid = info['genotype_id']\n        id_to_idx[gid] = idx\n        idx_to_id_set[idx] = {gid}\n        idx_to_up_dict_ll_list[idx] = [({gid: {}}, 0)]  # Empty dict means no parents\n        \n    return idx_to_up_dict_ll_list, id_to_idx, idx_to_id_set\n\n# Initialize for our example data\nidx_to_up_dict_ll_list, id_to_idx, idx_to_id_set = initialize_input_dicts(bio_info)\n\nprint(\"\\nInitialized pedigree tracking structures:\")\nprint(f\"\\nid_to_idx: {id_to_idx}\")\nprint(f\"\\nidx_to_id_set: {idx_to_id_set}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Connecting Pedigrees in Bonsai v3\n\nA key feature of Bonsai v3 is its improved approach to connecting pedigrees. This is implemented through functions that find optimal connection points:",
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "def get_connecting_points_degs_and_log_likes(\n    up_dct1, \n    up_dct2, \n    pw_ll_cls,\n    ibd_seg_list,\n    condition=False, \n    min_seg_len=MIN_SEG_LEN,\n    max_con_pts=MAX_CON_PTS, \n    restrict_connection_points=RESTRICT_CON_PTS,\n    connect_up_only=CONNECT_UP_ONLY,\n):\n    \"\"\"\n    Find potential connection points between two pedigrees.\n    \n    Args:\n        up_dct1: First pedigree structure\n        up_dct2: Second pedigree structure\n        pw_ll_cls: Instance of PwLogLike class\n        ibd_seg_list: List of IBD segments\n        condition: Whether to condition on observing IBD\n        min_seg_len: Minimum segment length to consider\n        max_con_pts: Maximum number of connection points to examine\n        restrict_connection_points: Whether to restrict to relevant parts of pedigree\n        connect_up_only: Whether to only connect through ancestors\n        \n    Returns:\n        List of [point1, point2, relationship_tuple, log_likelihood] items\n    \"\"\"\n    # Get genotyped IDs from each pedigree\n    gtid_set1 = {i for i in up_dct1 if i > 0}\n    gtid_set2 = {i for i in up_dct2 if i > 0}\n    \n    # Find potential connection points\n    connection_points = []\n    \n    # For demonstration, we'll consider connecting each pair of IDs\n    for id1 in gtid_set1:\n        for id2 in gtid_set2:\n            # For each pair, consider different relationship types\n            relationships = [\n                (1, 1, 1),  # Half siblings\n                (2, 2, 1),  # First cousins\n                (1, 2, 1),  # Half uncle-nephew\n                (2, 1, 1),  # Half uncle-nephew (reverse)\n                (2, 2, 2),  # Full first cousins\n            ]\n            \n            # Calculate likelihood for each relationship\n            for rel_tuple in relationships:\n                log_like = pw_ll_cls.get_log_like(id1, id2, rel_tuple, condition)\n                \n                # Create connection points\n                # point1 is (id, partner, direction)\n                # Here direction=1 means connect upward\n                point1 = (id1, None, 1)\n                point2 = (id2, None, 1)\n                \n                connection_points.append([point1, point2, rel_tuple, log_like])\n    \n    # Sort by likelihood (descending)\n    connection_points.sort(key=lambda x: x[3], reverse=True)\n    \n    # Limit to max_con_pts\n    return connection_points[:max_con_pts]\n\ndef connect_new_node(\n    node, \n    up_node_dict, \n    con_pt, \n    rel_tuple,\n):\n    \"\"\"\n    Connect a new node to an existing pedigree through a connection point.\n    \n    Args:\n        node: New node ID to connect\n        up_node_dict: Existing pedigree structure\n        con_pt: Connection point (id, partner, direction)\n        rel_tuple: Relationship tuple (up, down, num_ancs)\n        \n    Returns:\n        Updated pedigree with the new node connected\n    \"\"\"\n    # Extract connection information\n    id1, pid1, direction = con_pt\n    id2 = node\n    \n    # Get relationship\n    deg1, deg2, num_ancs = rel_tuple\n    \n    # Create a copy of the pedigree to modify\n    new_pedigree = up_node_dict.copy()\n    \n    # Make sure node is in the pedigree\n    if node not in new_pedigree:\n        new_pedigree[node] = {}\n    \n    # Simple implementation for common relationships\n    if deg1 == 1 and deg2 == 1 and num_ancs == 1:\n        # Half siblings - create a common parent\n        common_parent = -1  # Use negative ID for inferred ancestors\n        while common_parent in new_pedigree:\n            common_parent -= 1\n            \n        # Add the common parent\n        new_pedigree[common_parent] = {}\n        \n        # Connect parent to both individuals\n        if id1 in new_pedigree:\n            new_pedigree[id1][common_parent] = 1\n        if id2 in new_pedigree:\n            new_pedigree[id2][common_parent] = 1\n    \n    elif deg1 == 2 and deg2 == 2 and num_ancs == 1:\n        # First cousins - create grandparent and parents\n        grandparent = -1\n        parent1 = -2\n        parent2 = -3\n        \n        # Make sure IDs are unique\n        while any(id in new_pedigree for id in [grandparent, parent1, parent2]):\n            grandparent -= 3\n            parent1 = grandparent - 1\n            parent2 = grandparent - 2\n        \n        # Create the structure\n        new_pedigree[grandparent] = {}\n        new_pedigree[parent1] = {grandparent: 1}\n        new_pedigree[parent2] = {grandparent: 1}\n        \n        # Connect to individuals\n        if id1 in new_pedigree:\n            new_pedigree[id1][parent1] = 1\n        if id2 in new_pedigree:\n            new_pedigree[id2][parent2] = 1\n    \n    return new_pedigree\n\n# Test finding connection points between two small pedigrees\nped1 = {1000: {}}\nped2 = {1001: {}}\ncon_pts = get_connecting_points_degs_and_log_likes(\n    ped1, ped2, pw_ll_cls, \n    get_unphased_to_phased(example_segments_array)\n)\n\n# Display the top connection points\nprint(\"Top connection points:\")\nfor i, (point1, point2, rel_tuple, ll) in enumerate(con_pts[:3]):\n    up, down, n_ancs = rel_tuple\n    rel_type = \"Unknown\"\n    if up == 1 and down == 0 and n_ancs == 1:\n        rel_type = \"Parent-Child\"\n    elif up == 1 and down == 1 and n_ancs == 1:\n        rel_type = \"Half-Siblings\"\n    elif up == 2 and down == 2 and n_ancs == 1:\n        rel_type = \"First Cousins\"\n    elif up == 2 and down == 2 and n_ancs == 2:\n        rel_type = \"Full First Cousins\"\n    print(f\"{i+1}. {point1[0]}-{point2[0]} as {rel_type} (up={up}, down={down}, n_ancs={n_ancs}), LL={ll:.2f}\")\n\n# Test connecting a new node\nbase_ped = {1000: {}, 1001: {}}\nconn_pt = (1000, None, 1)  # Connection point from previous step\nrel_tuple = (1, 1, 1)  # Half siblings\n\nnew_ped = connect_new_node(1004, base_ped, conn_pt, rel_tuple)\n\n# Visualize the result\nvisualize_pedigree(new_ped)",
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": "## Finding the Next Node in Bonsai v3\n\nAnother key feature of Bonsai v3 is its ability to determine which unplaced individual should be added next:"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "def get_next_node(\n    placed_id_set,\n    unphased_ibd_seg_list=None,\n    phased_ibd_seg_list=None,\n):\n    \"\"\"\n    Find the next node to add to the pedigree.\n    \n    Args:\n        placed_id_set: Set of genotyped IDs already placed in a pedigree\n        unphased_ibd_seg_list: List of unphased IBD segments\n        phased_ibd_seg_list: List of phased IBD segments\n        \n    Returns:\n        (node, pid, max_ibd): Next node to add, best placed relative, and IBD amount\n    \"\"\"\n    # Handle phased/unphased conversions\n    if unphased_ibd_seg_list is None and phased_ibd_seg_list is not None:\n        unphased_ibd_seg_list = get_phased_to_unphased(phased_ibd_seg_list)\n    \n    # Get IBD statistics\n    ibd_stats = get_ibd_stats_unphased(unphased_ibd_seg_list)\n    \n    # Find all IDs\n    all_id_set = set()\n    for seg in unphased_ibd_seg_list:\n        all_id_set.add(seg[0])\n        all_id_set.add(seg[1])\n    \n    # Get unplaced IDs\n    unplaced_id_set = all_id_set - placed_id_set\n    \n    # Find the unplaced ID that shares the most IBD with any placed ID\n    id_max_ibd = []\n    for uid in unplaced_id_set:\n        max_ibd = 0\n        best_pid = None\n        for pid in placed_id_set:\n            key = frozenset({uid, pid})\n            if key in ibd_stats:\n                total_half = ibd_stats[key]['total_half']\n                total_full = ibd_stats[key]['total_full']\n                total_ibd = total_half + total_full\n                if total_ibd > max_ibd:\n                    max_ibd = total_ibd\n                    best_pid = pid\n        \n        if best_pid is not None:\n            id_max_ibd.append((uid, best_pid, max_ibd))\n    \n    # Sort by IBD amount (descending)\n    id_max_ibd.sort(key=lambda x: x[2], reverse=True)\n    \n    if len(id_max_ibd) == 0:\n        return None, None, None\n    else:\n        return id_max_ibd[0]\n\n# Test finding the next node to add\nplaced_ids = {1000, 1001}\nnext_node, connected_to, ibd_amount = get_next_node(placed_ids, example_segments_array)\n\nprint(f\"Next node to add: {next_node}\")\nprint(f\"Connected to: {connected_to}\")\nprint(f\"Shared IBD: {ibd_amount:.1f} cM\")",
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## The Complete Bonsai v3 Pedigree Building Process\n\nLet's simulate the entire pedigree building process using v3's step-by-step approach:",
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "def build_pedigree_step_by_step(unphased_ibd_segs, bio_info):\n    \"\"\"Build a pedigree step by step using the v3 approach.\"\"\"\n    # Convert to phased format for compatibility\n    phased_ibd_segs = get_unphased_to_phased(unphased_ibd_segs)\n    \n    # Create likelihood calculator\n    pw_ll_cls = PwLogLike(bio_info, unphased_ibd_segs)\n    \n    # Start with an empty pedigree\n    pedigree = {}\n    placed_ids = set()\n    \n    # Find highest sharing pair to start with\n    ibd_stats = get_ibd_stats_unphased(unphased_ibd_segs)\n    highest_pair = None\n    highest_ibd = 0\n    \n    for pair, stats in ibd_stats.items():\n        total_ibd = stats['total_half'] + stats['total_full']\n        if total_ibd > highest_ibd:\n            highest_ibd = total_ibd\n            highest_pair = pair\n    \n    if highest_pair is None:\n        return pedigree\n    \n    # Add the first pair\n    id1, id2 = highest_pair\n    pedigree[id1] = {}\n    pedigree[id2] = {}\n    placed_ids.add(id1)\n    placed_ids.add(id2)\n    \n    # Get best relationship between them\n    best_rel = None\n    best_ll = float(\"-inf\")\n    \n    for rel in [(1, 0, 1), (0, 1, 1), (1, 1, 2), (1, 1, 1)]:\n        ll = pw_ll_cls.get_log_like(id1, id2, rel)\n        if ll > best_ll:\n            best_ll = ll\n            best_rel = rel\n    \n    # Connect them according to best relationship\n    if best_rel == (1, 0, 1):  # id2 is parent of id1\n        pedigree[id1][id2] = 1\n    elif best_rel == (0, 1, 1):  # id1 is parent of id2\n        pedigree[id2][id1] = 1\n    elif best_rel == (1, 1, 2):  # Full siblings\n        parent1 = -1\n        parent2 = -2\n        pedigree[parent1] = {}\n        pedigree[parent2] = {}\n        pedigree[id1][parent1] = 1\n        pedigree[id1][parent2] = 1\n        pedigree[id2][parent1] = 1\n        pedigree[id2][parent2] = 1\n    elif best_rel == (1, 1, 1):  # Half siblings\n        parent = -1\n        pedigree[parent] = {}\n        pedigree[id1][parent] = 1\n        pedigree[id2][parent] = 1\n    \n    # Add remaining individuals one by one\n    while True:\n        next_id, connected_to, _ = get_next_node(placed_ids, unphased_ibd_segs)\n        \n        if next_id is None:\n            break\n            \n        # Find best connection point\n        con_pts = get_connecting_points_degs_and_log_likes(\n            pedigree, {next_id: {}}, pw_ll_cls, phased_ibd_segs\n        )\n        \n        if not con_pts:\n            # No good connection found\n            pedigree[next_id] = {}\n        else:\n            # Use best connection\n            best_pt1, best_pt2, best_rel, _ = con_pts[0]\n            pedigree = connect_new_node(next_id, pedigree, best_pt1, best_rel)\n        \n        placed_ids.add(next_id)\n    \n    return pedigree\n\n# Build a complete pedigree using v3 approach\nfull_pedigree = build_pedigree_step_by_step(example_segments_array, bio_info)\n\n# Visualize the result\nvisualize_pedigree(full_pedigree)",
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def would_create_cycle(up_node_dict, child_id, proposed_parent_id):\n",
    "    \"\"\"Check if adding a parent-child relationship would create a cycle.\"\"\"\n",
    "    # If the proposed parent is already a descendant of the child,\n",
    "    # adding this relationship would create a cycle\n",
    "    \n",
    "    # Start with the proposed parent\n",
    "    current_ids = [proposed_parent_id]\n",
    "    visited = set(current_ids)\n",
    "    \n",
    "    # Traverse up the pedigree\n",
    "    while current_ids:\n",
    "        next_ids = []\n",
    "        for current_id in current_ids:\n",
    "            # If we've reached the child, a cycle would be created\n",
    "            if current_id == child_id:\n",
    "                return True\n",
    "                \n",
    "            # Add this individual's parents to the search\n",
    "            if current_id in up_node_dict:\n",
    "                parents = up_node_dict[current_id].keys()\n",
    "                for parent_id in parents:\n",
    "                    if parent_id not in visited:\n",
    "                        next_ids.append(parent_id)\n",
    "                        visited.add(parent_id)\n",
    "        \n",
    "        current_ids = next_ids\n",
    "    \n",
    "    # No cycle found\n",
    "    return False\n",
    "\n",
    "# Test the cycle detection function\n",
    "test_pedigree = {\n",
    "    1000: {1001: 1, 1002: 1},  # 1000 has parents 1001 and 1002\n",
    "    1001: {1003: 1, 1004: 1},  # 1001 has parents 1003 and 1004\n",
    "    1002: {},\n",
    "    1003: {},\n",
    "    1004: {}\n",
    "}\n",
    "\n",
    "# Would adding 1000 as a parent of 1003 create a cycle?\n",
    "# (1000 -> 1001 -> 1003 -> 1000 would form a cycle)\n",
    "cycle_detected = would_create_cycle(test_pedigree, 1003, 1000)\n",
    "print(f\"Would adding 1000 as a parent of 1003 create a cycle? {cycle_detected}\")\n",
    "\n",
    "# Would adding 1002 as a parent of 1003 create a cycle?\n",
    "cycle_detected = would_create_cycle(test_pedigree, 1003, 1002)\n",
    "print(f\"Would adding 1002 as a parent of 1003 create a cycle? {cycle_detected}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Community Detection and Graph Partitioning\n",
    "\n",
    "For large datasets, Bonsai uses graph-based community detection algorithms (e.g., Louvain method) to partition the data into more manageable subproblems:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def partition_with_community_detection(segments):\n",
    "    \"\"\"Partition individuals into communities based on IBD sharing.\"\"\"\n",
    "    # Create a graph where nodes are individuals and edges represent IBD sharing\n",
    "    G = nx.Graph()\n",
    "    \n",
    "    # Add edges with weights based on IBD sharing\n",
    "    for seg in segments:\n",
    "        ind1, ind2 = seg.ind1, seg.ind2\n",
    "        weight = seg.length_cm\n",
    "        \n",
    "        if G.has_edge(ind1, ind2):\n",
    "            G[ind1][ind2]['weight'] += weight\n",
    "        else:\n",
    "            G.add_edge(ind1, ind2, weight=weight)\n",
    "    \n",
    "    # Apply Louvain community detection if available\n",
    "    try:\n",
    "        communities = nx.community.louvain_communities(G, weight='weight')\n",
    "    except AttributeError:\n",
    "        # Fallback to connected components if Louvain isn't available\n",
    "        communities = [c for c in nx.connected_components(G)]\n",
    "    \n",
    "    # Return communities as lists of individual IDs\n",
    "    return [list(community) for community in communities]\n",
    "\n",
    "# Apply community detection to our example segments\n",
    "communities = partition_with_community_detection(example_segments)\n",
    "\n",
    "# Display the communities\n",
    "print(f\"Detected {len(communities)} communities:\")\n",
    "for i, community in enumerate(communities):\n",
    "    print(f\"Community {i+1}: {community}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This partitioning strategy offers several advantages:\n",
    "- Reduces computational complexity by breaking a large problem into smaller subproblems\n",
    "- Focuses reconstruction on groups of individuals that are likely related\n",
    "- Enables parallel processing of different communities\n",
    "- Improves scalability to handle large datasets with thousands of individuals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's visualize the communities based on IBD sharing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_ibd_communities(segments, communities):\n",
    "    \"\"\"Visualize communities based on IBD sharing.\"\"\"\n",
    "    # Create a graph where nodes are individuals and edges represent IBD sharing\n",
    "    G = nx.Graph()\n",
    "    \n",
    "    # Add edges with weights based on IBD sharing\n",
    "    for seg in segments:\n",
    "        ind1, ind2 = seg.ind1, seg.ind2\n",
    "        weight = seg.length_cm\n",
    "        \n",
    "        if G.has_edge(ind1, ind2):\n",
    "            G[ind1][ind2]['weight'] += weight\n",
    "        else:\n",
    "            G.add_edge(ind1, ind2, weight=weight)\n",
    "    \n",
    "    # Create a mapping from individual to community\n",
    "    community_map = {}\n",
    "    for i, community in enumerate(communities):\n",
    "        for ind in community:\n",
    "            community_map[ind] = i\n",
    "    \n",
    "    # Create node colors based on community\n",
    "    cmap = plt.cm.get_cmap('tab10', len(communities))\n",
    "    node_colors = [cmap(community_map[node]) for node in G.nodes()]\n",
    "    \n",
    "    # Create edge weights based on IBD sharing\n",
    "    edge_weights = [G[u][v]['weight'] / 10 for u, v in G.edges()]\n",
    "    \n",
    "    # Set up the plot\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    pos = nx.spring_layout(G, seed=42)  # Consistent layout\n",
    "    \n",
    "    # Draw the graph\n",
    "    nx.draw(G, pos, with_labels=True, node_color=node_colors, \n",
    "            edge_color='gray', width=edge_weights, alpha=0.7,\n",
    "            node_size=500, font_size=10)\n",
    "    \n",
    "    # Add edge labels (IBD sharing in cM)\n",
    "    edge_labels = {(u, v): f\"{d['weight']:.1f} cM\" for u, v, d in G.edges(data=True)}\n",
    "    nx.draw_networkx_edge_labels(G, pos, edge_labels=edge_labels, font_size=8)\n",
    "    \n",
    "    plt.title('IBD Sharing Communities')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Visualize the communities\n",
    "visualize_ibd_communities(example_segments, communities)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Building and Manipulating Pedigrees\n",
    "\n",
    "Bonsai includes operations for building and modifying pedigree structures during the reconstruction process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pedigree Modification Operations\n",
    "\n",
    "These operations form the basis of Bonsai's optimization algorithm:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_parent(up_node_dict, child_id, parent_id):\n",
    "    \"\"\"Add a parent-child relationship to the pedigree.\"\"\"\n",
    "    # Ensure child exists in dictionary\n",
    "    if child_id not in up_node_dict:\n",
    "        up_node_dict[child_id] = {}\n",
    "    \n",
    "    # Check if adding this parent would create a cycle\n",
    "    if would_create_cycle(up_node_dict, child_id, parent_id):\n",
    "        return False  # Cannot add this relationship\n",
    "    \n",
    "    # Check if child already has two parents\n",
    "    if len(up_node_dict[child_id]) >= 2:\n",
    "        return False  # Child already has maximum number of parents\n",
    "    \n",
    "    # Add the parent\n",
    "    up_node_dict[child_id][parent_id] = 1\n",
    "    \n",
    "    # Ensure parent exists in dictionary\n",
    "    if parent_id not in up_node_dict:\n",
    "        up_node_dict[parent_id] = {}\n",
    "    \n",
    "    return True\n",
    "\n",
    "def remove_parent(up_node_dict, child_id, parent_id):\n",
    "    \"\"\"Remove a parent-child relationship from the pedigree.\"\"\"\n",
    "    if child_id not in up_node_dict or parent_id not in up_node_dict[child_id]:\n",
    "        return False  # Relationship doesn't exist\n",
    "    \n",
    "    # Remove the relationship\n",
    "    del up_node_dict[child_id][parent_id]\n",
    "    return True\n",
    "\n",
    "def swap_parent(up_node_dict, child_id, old_parent_id, new_parent_id):\n",
    "    \"\"\"Replace one parent with another.\"\"\"\n",
    "    # Remove old parent\n",
    "    if not remove_parent(up_node_dict, child_id, old_parent_id):\n",
    "        return False\n",
    "    \n",
    "    # Add new parent\n",
    "    if not add_parent(up_node_dict, child_id, new_parent_id):\n",
    "        # If adding new parent fails, restore old parent\n",
    "        add_parent(up_node_dict, child_id, old_parent_id)\n",
    "        return False\n",
    "    \n",
    "    return True\n",
    "\n",
    "# Create a simple pedigree\n",
    "test_pedigree = {\n",
    "    1000: {},\n",
    "    1001: {},\n",
    "    1002: {}\n",
    "}\n",
    "\n",
    "# Add a parent-child relationship\n",
    "add_parent(test_pedigree, 1000, 1001)\n",
    "print(\"After adding 1001 as parent of 1000:\")\n",
    "print(test_pedigree)\n",
    "\n",
    "# Add another parent\n",
    "add_parent(test_pedigree, 1000, 1002)\n",
    "print(\"\\nAfter adding 1002 as parent of 1000:\")\n",
    "print(test_pedigree)\n",
    "\n",
    "# Visualize the pedigree\n",
    "visualize_pedigree(test_pedigree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding Inferred Ancestors\n",
    "\n",
    "A key feature of Bonsai is its ability to infer missing ancestors. This is implemented by creating \"latent nodes\" with negative IDs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_latent_ancestor(up_node_dict, next_latent_id=-1):\n",
    "    \"\"\"Create a new latent ancestor node in the pedigree.\"\"\"\n",
    "    # Find an unused negative ID\n",
    "    while next_latent_id in up_node_dict:\n",
    "        next_latent_id -= 1\n",
    "    \n",
    "    # Create the new latent node with no parents\n",
    "    up_node_dict[next_latent_id] = {}\n",
    "    \n",
    "    return next_latent_id\n",
    "\n",
    "def add_latent_parent_pair(up_node_dict, child_id):\n",
    "    \"\"\"Add a pair of latent parents to a child.\"\"\"\n",
    "    # Create two latent parents\n",
    "    latent_parent1 = create_latent_ancestor(up_node_dict)\n",
    "    latent_parent2 = create_latent_ancestor(up_node_dict, latent_parent1 - 1)\n",
    "    \n",
    "    # Add them as parents of the child\n",
    "    add_parent(up_node_dict, child_id, latent_parent1)\n",
    "    add_parent(up_node_dict, child_id, latent_parent2)\n",
    "    \n",
    "    return latent_parent1, latent_parent2\n",
    "\n",
    "# Extend our test pedigree with latent ancestors\n",
    "test_pedigree = {\n",
    "    1000: {},\n",
    "    1001: {},\n",
    "    1002: {},\n",
    "    1003: {}\n",
    "}\n",
    "\n",
    "# Add observed relationships\n",
    "add_parent(test_pedigree, 1000, 1001)\n",
    "add_parent(test_pedigree, 1000, 1002)\n",
    "\n",
    "# Add latent parents for 1003\n",
    "latent_parent1, latent_parent2 = add_latent_parent_pair(test_pedigree, 1003)\n",
    "print(f\"Added latent parents {latent_parent1} and {latent_parent2} to individual 1003\")\n",
    "\n",
    "# Connect latent parents to observed individuals to create a more complex pedigree\n",
    "add_parent(test_pedigree, latent_parent1, 1001)\n",
    "print(\"\\nFinal pedigree with latent ancestors:\")\n",
    "print(test_pedigree)\n",
    "\n",
    "# Visualize the extended pedigree\n",
    "visualize_pedigree(test_pedigree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This capability allows Bonsai to reconstruct more complete pedigrees even when data for some ancestors is unavailable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Efficient Search and Optimization\n",
    "\n",
    "Bonsai's data structures are designed to support efficient search and optimization of pedigree structures."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Caching and Memoization\n",
    "\n",
    "To avoid redundant calculations, Bonsai extensively uses caching and memoization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RelationshipCalculator:\n",
    "    def __init__(self, up_node_dict):\n",
    "        self.up_node_dict = up_node_dict\n",
    "        self.coefficient_cache = {}  # Cache for relationship coefficients\n",
    "    \n",
    "    def get_coefficient(self, id1, id2):\n",
    "        \"\"\"Get the relationship coefficient between two individuals.\"\"\"\n",
    "        # Check cache first\n",
    "        pair = tuple(sorted([id1, id2]))\n",
    "        if pair in self.coefficient_cache:\n",
    "            return self.coefficient_cache[pair]\n",
    "        \n",
    "        # Calculate coefficient\n",
    "        coefficient = self._calculate_coefficient(id1, id2)\n",
    "        \n",
    "        # Cache the result\n",
    "        self.coefficient_cache[pair] = coefficient\n",
    "        return coefficient\n",
    "    \n",
    "    def _calculate_coefficient(self, id1, id2):\n",
    "        \"\"\"Calculate the relationship coefficient (actual implementation).\"\"\"\n",
    "        # Self-relationship\n",
    "        if id1 == id2:\n",
    "            return 1.0\n",
    "        \n",
    "        # Direct parent-child relationship\n",
    "        if id1 in self.up_node_dict.get(id2, {}) or id2 in self.up_node_dict.get(id1, {}):\n",
    "            return 0.5\n",
    "        \n",
    "        # Calculate common ancestor contributions\n",
    "        paths1 = self._get_paths_to_ancestors(id1)\n",
    "        paths2 = self._get_paths_to_ancestors(id2)\n",
    "        \n",
    "        # Find common ancestors\n",
    "        common_ancestors = set(paths1.keys()).intersection(set(paths2.keys()))\n",
    "        \n",
    "        # Sum contributions from each common ancestor\n",
    "        coefficient = 0.0\n",
    "        for ancestor in common_ancestors:\n",
    "            for path1 in paths1[ancestor]:\n",
    "                for path2 in paths2[ancestor]:\n",
    "                    # Contribution is 0.5^(length of paths)\n",
    "                    contribution = (0.5 ** len(path1)) * (0.5 ** len(path2))\n",
    "                    coefficient += contribution\n",
    "        \n",
    "        return coefficient\n",
    "    \n",
    "    def _get_paths_to_ancestors(self, individual_id):\n",
    "        \"\"\"Find all paths from an individual to ancestors.\"\"\"\n",
    "        paths = {individual_id: [[]]}\n",
    "        \n",
    "        # Queue of (individual, current_path) tuples to process\n",
    "        queue = [(individual_id, [])]\n",
    "        \n",
    "        while queue:\n",
    "            current_id, current_path = queue.pop(0)\n",
    "            \n",
    "            # Get parents of current individual\n",
    "            parents = self.up_node_dict.get(current_id, {})\n",
    "            \n",
    "            for parent_id in parents:\n",
    "                # Create a new path that includes this parent\n",
    "                new_path = current_path + [parent_id]\n",
    "                \n",
    "                # Add path to parent's paths\n",
    "                if parent_id not in paths:\n",
    "                    paths[parent_id] = []\n",
    "                paths[parent_id].append(new_path)\n",
    "                \n",
    "                # Add parent to queue for further processing\n",
    "                queue.append((parent_id, new_path))\n",
    "        \n",
    "        return paths\n",
    "    \n",
    "    def invalidate_cache_for_individual(self, individual_id):\n",
    "        \"\"\"Invalidate cache entries involving a specific individual.\"\"\"\n",
    "        # When the pedigree changes, we need to invalidate affected cache entries\n",
    "        keys_to_remove = []\n",
    "        for key in self.coefficient_cache:\n",
    "            if individual_id in key:\n",
    "                keys_to_remove.append(key)\n",
    "        \n",
    "        for key in keys_to_remove:\n",
    "            del self.coefficient_cache[key]\n",
    "\n",
    "# Test the relationship calculator with our example pedigree\n",
    "calculator = RelationshipCalculator(test_pedigree)\n",
    "\n",
    "# Calculate some relationships\n",
    "relationships = [\n",
    "    (1000, 1001),  # Parent-child\n",
    "    (1000, 1002),  # Parent-child\n",
    "    (1003, 1001),  # Grandparent-grandchild (through latent parent)\n",
    "    (1000, 1003)   # Half-siblings or cousins (depending on exact structure)\n",
    "]\n",
    "\n",
    "for id1, id2 in relationships:\n",
    "    coef = calculator.get_coefficient(id1, id2)\n",
    "    print(f\"Relationship coefficient between {id1} and {id2}: {coef:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This caching strategy dramatically improves performance, especially during optimization when many similar pedigree configurations are evaluated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Priority-Based Processing\n",
    "\n",
    "Bonsai processes relationships in order of confidence, focusing computational resources on the most reliable inferences:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_relationships_by_priority(pair_stats):\n",
    "    \"\"\"Process relationships in order of priority (higher IBD sharing first).\"\"\"\n",
    "    # Create priority queue\n",
    "    priority_queue = []\n",
    "    \n",
    "    # Add all pairs to the queue with priority based on IBD sharing\n",
    "    for pair, stats in pair_stats.items():\n",
    "        # Priority is negative of total IBD sharing (so higher sharing = higher priority)\n",
    "        priority = -stats['total_cm']\n",
    "        heapq.heappush(priority_queue, (priority, pair))\n",
    "    \n",
    "    # Process queue\n",
    "    processed_pairs = []\n",
    "    while priority_queue:\n",
    "        _, pair = heapq.heappop(priority_queue)\n",
    "        processed_pairs.append(pair)\n",
    "    \n",
    "    return processed_pairs\n",
    "\n",
    "# Process our preprocessed data by priority\n",
    "prioritized_pairs = process_relationships_by_priority(preprocessed_data['pair_stats'])\n",
    "\n",
    "# Display the pairs in priority order\n",
    "print(\"Pairs in order of processing priority:\")\n",
    "for i, pair in enumerate(prioritized_pairs):\n",
    "    stats = preprocessed_data['pair_stats'][pair]\n",
    "    print(f\"{i+1}. Pair {pair}: {stats['total_cm']:.2f} cM total sharing\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This approach helps Bonsai establish the most obvious relationships first, which constrains the search space for more ambiguous relationships."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Data Structure Implementation Challenges\n",
    "\n",
    "Implementing Bonsai's data structures involves several challenges that must be addressed for optimal performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Memory Efficiency\n",
    "\n",
    "For large datasets with thousands of individuals and millions of IBD segments, memory usage becomes a critical concern. Bonsai employs several strategies to minimize memory footprint:\n",
    "\n",
    "* **Sparse representation:** The up-node dictionary only stores relationships that exist, making it memory-efficient for sparse pedigrees\n",
    "* **Data filtering:** IBD segments below a minimum threshold are filtered out early in the preprocessing pipeline\n",
    "* **Community partitioning:** By breaking the problem into communities, each subproblem can be processed with a smaller memory footprint\n",
    "* **Selective caching:** Caching strategies that prioritize frequently used data while allowing less frequent data to be recalculated"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Time Complexity Considerations\n",
    "\n",
    "The time complexity of key operations in Bonsai:\n",
    "\n",
    "| Operation | Time Complexity | Notes |\n",
    "|-----------|-----------------|-------|\n",
    "| Accessing IBD segments for a pair | O(1) | Using pair-based index |\n",
    "| Finding all pairs involving an individual | O(d) where d is degree | Using individual-based index |\n",
    "| Calculating relationship coefficient | O(a) where a is number of ancestors | With caching, subsequent lookups are O(1) |\n",
    "| Checking for cycles | O(n) where n is individuals in pedigree | Worst case, but typically much faster |\n",
    "| Community detection | O(m log n) where m is number of IBD segments | Using optimized Louvain algorithm |\n",
    "| Overall reconstruction | O(i * p^2) where i is iterations, p is pairs | With optimizations, scales to thousands of individuals |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Custom Extensions and Adaptations\n",
    "\n",
    "The modular design of Bonsai's data structures facilitates custom extensions for specialized applications."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Incorporating Additional Metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extended BioInfo structure with additional metadata\n",
    "extended_bio_info = [\n",
    "    {\n",
    "        'genotype_id': 1000,\n",
    "        'age': 75,\n",
    "        'sex': 'F',\n",
    "        'population': 'EUR',\n",
    "        'birth_year': 1947,\n",
    "        'is_genotyped': True,\n",
    "        'phenotypes': {'height': 165, 'weight': 68},\n",
    "        'haplogroups': {'mt': 'H1', 'y': None}\n",
    "    },\n",
    "    # ... additional individuals ...\n",
    "]\n",
    "\n",
    "# Display the extended metadata\n",
    "extended_bio_df = pd.DataFrame(extended_bio_info)\n",
    "extended_bio_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This extended metadata can be used to enhance pedigree reconstruction accuracy or to analyze the resulting pedigrees in more detail."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom Relationship Types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extended up-node dictionary with relationship types\n",
    "extended_up_node_dict = {\n",
    "    1000: {\n",
    "        1001: {'type': 'biological', 'confidence': 0.98},\n",
    "        1002: {'type': 'biological', 'confidence': 0.97}\n",
    "    },\n",
    "    1003: {\n",
    "        1001: {'type': 'adoptive', 'confidence': 0.99},\n",
    "        1004: {'type': 'biological', 'confidence': 0.95}\n",
    "    },\n",
    "    # ... additional relationships ...\n",
    "}\n",
    "\n",
    "# Function to visualize extended pedigree\n",
    "def visualize_extended_pedigree(extended_dict):\n",
    "    \"\"\"Visualize pedigree with extended relationship information.\"\"\"\n",
    "    # Create a directed graph\n",
    "    G = nx.DiGraph()\n",
    "    \n",
    "    # Add nodes\n",
    "    for indiv_id in extended_dict.keys():\n",
    "        G.add_node(indiv_id)\n",
    "    \n",
    "    # Add edges with attributes\n",
    "    for child, parents in extended_dict.items():\n",
    "        for parent, attrs in parents.items():\n",
    "            G.add_edge(parent, child, **attrs)\n",
    "    \n",
    "    # Set up the visualization\n",
    "    pos = nx.spring_layout(G, seed=42)\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    \n",
    "    # Draw nodes\n",
    "    nx.draw_networkx_nodes(G, pos, node_color='lightgreen', node_size=500)\n",
    "    nx.draw_networkx_labels(G, pos)\n",
    "    \n",
    "    # Draw edges with different styles based on relationship type\n",
    "    biological_edges = [(p, c) for c, parents in extended_dict.items() \n",
    "                        for p, attrs in parents.items() if attrs.get('type') == 'biological']\n",
    "    adoptive_edges = [(p, c) for c, parents in extended_dict.items() \n",
    "                      for p, attrs in parents.items() if attrs.get('type') == 'adoptive']\n",
    "    \n",
    "    nx.draw_networkx_edges(G, pos, edgelist=biological_edges, edge_color='blue')\n",
    "    nx.draw_networkx_edges(G, pos, edgelist=adoptive_edges, edge_color='red', style='dashed')\n",
    "    \n",
    "    # Add edge labels with confidence scores\n",
    "    edge_labels = {(p, c): f\"{attrs.get('confidence', 1.0):.2f}\" \n",
    "                  for c, parents in extended_dict.items() \n",
    "                  for p, attrs in parents.items()}\n",
    "    nx.draw_networkx_edge_labels(G, pos, edge_labels=edge_labels, font_size=8)\n",
    "    \n",
    "    plt.title('Extended Pedigree with Relationship Types')\n",
    "    plt.axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Visualize the extended pedigree\n",
    "visualize_extended_pedigree(extended_up_node_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This extension allows the representation of more complex family structures, such as adoptive relationships, step-relationships, or relationships with uncertain confidence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 8. Exercises\n\nComplete the following exercises to deepen your understanding of the data structures and algorithmic design in Bonsai v3.",
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Exercise 1: Implement an efficient IBD segment indexing structure for v3 and benchmark its performance.",
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": "# Solution for Exercise 1\nimport time\nfrom numba import jit  # For performance optimization\n\nclass IBDIndex:\n    \"\"\"\n    An efficient indexing structure for IBD segments that supports both\n    phased and unphased data with optimized access patterns aligned with v3.\n    \"\"\"\n    def __init__(self, min_cm=7.0):\n        self.min_cm = min_cm\n        self.pair_index = {}           # {frozenset({id1, id2}): [segment_list]}\n        self.id_index = {}             # {id: [segment_list]}\n        self.pair_stats = {}           # {frozenset({id1, id2}): stats_dict}\n        self.chrom_index = {}          # {chrom: [segment_list]}\n        self.id_to_chrom_index = {}    # {id: {chrom: [segment_list]}}\n        self.haplotype_index = {}      # {(id, hap): [segment_list]} (for phased data)\n        \n    def add_segment(self, segment, segment_type=\"unphased\"):\n        \"\"\"Add a segment to all relevant indexes.\"\"\"\n        if segment_type == \"unphased\":\n            # [id1, id2, chromosome, start_bp, end_bp, is_full_ibd, seg_cm]\n            id1, id2, chrom, start, end, is_full, seg_cm = segment\n            \n            # Filter by minimum length\n            if seg_cm < self.min_cm:\n                return\n                \n            # Create keys\n            pair_key = frozenset({id1, id2})\n            \n            # Update pair index\n            if pair_key not in self.pair_index:\n                self.pair_index[pair_key] = []\n            self.pair_index[pair_key].append(segment)\n            \n            # Update individual index\n            for id_val in [id1, id2]:\n                if id_val not in self.id_index:\n                    self.id_index[id_val] = []\n                self.id_index[id_val].append(segment)\n                \n                # Update id-to-chrom index\n                if id_val not in self.id_to_chrom_index:\n                    self.id_to_chrom_index[id_val] = {}\n                if chrom not in self.id_to_chrom_index[id_val]:\n                    self.id_to_chrom_index[id_val][chrom] = []\n                self.id_to_chrom_index[id_val][chrom].append(segment)\n            \n            # Update chromosome index\n            if chrom not in self.chrom_index:\n                self.chrom_index[chrom] = []\n            self.chrom_index[chrom].append(segment)\n            \n            # Update stats\n            if pair_key not in self.pair_stats:\n                self.pair_stats[pair_key] = {\n                    'total_half': 0,\n                    'total_full': 0,\n                    'num_half': 0,\n                    'num_full': 0,\n                    'max_seg_cm': 0\n                }\n            \n            if is_full:\n                self.pair_stats[pair_key]['total_full'] += seg_cm\n                self.pair_stats[pair_key]['num_full'] += 1\n            else:\n                self.pair_stats[pair_key]['total_half'] += seg_cm\n                self.pair_stats[pair_key]['num_half'] += 1\n            \n            self.pair_stats[pair_key]['max_seg_cm'] = max(\n                self.pair_stats[pair_key]['max_seg_cm'], seg_cm\n            )\n            \n        elif segment_type == \"phased\":\n            # [id1, id2, hap1, hap2, chromosome, start_cm, end_cm, seg_cm]\n            id1, id2, hap1, hap2, chrom, start, end, seg_cm = segment\n            \n            # Filter by minimum length\n            if seg_cm < self.min_cm:\n                return\n                \n            # Create keys\n            pair_key = frozenset({id1, id2})\n            hap1_key = (id1, hap1)\n            hap2_key = (id2, hap2)\n            \n            # Update pair index\n            if pair_key not in self.pair_index:\n                self.pair_index[pair_key] = []\n            self.pair_index[pair_key].append(segment)\n            \n            # Update individual index\n            for id_val in [id1, id2]:\n                if id_val not in self.id_index:\n                    self.id_index[id_val] = []\n                self.id_index[id_val].append(segment)\n            \n            # Update haplotype index\n            for hap_key in [hap1_key, hap2_key]:\n                if hap_key not in self.haplotype_index:\n                    self.haplotype_index[hap_key] = []\n                self.haplotype_index[hap_key].append(segment)\n            \n            # Update chromosome index\n            if chrom not in self.chrom_index:\n                self.chrom_index[chrom] = []\n            self.chrom_index[chrom].append(segment)\n    \n    def add_segments(self, segments, segment_type=\"unphased\"):\n        \"\"\"Add multiple segments at once.\"\"\"\n        for segment in segments:\n            self.add_segment(segment, segment_type)\n    \n    @jit  # Use Numba for performance\n    def get_segments_for_pair(self, id1, id2):\n        \"\"\"Get all segments shared between a pair of individuals.\"\"\"\n        pair_key = frozenset({id1, id2})\n        return self.pair_index.get(pair_key, [])\n    \n    def get_stats_for_pair(self, id1, id2):\n        \"\"\"Get IBD statistics for a pair of individuals.\"\"\"\n        pair_key = frozenset({id1, id2})\n        return self.pair_stats.get(pair_key, {\n            'total_half': 0,\n            'total_full': 0,\n            'num_half': 0,\n            'num_full': 0,\n            'max_seg_cm': 0\n        })\n    \n    def get_segments_by_chromosome(self, chrom):\n        \"\"\"Get all segments on a particular chromosome.\"\"\"\n        return self.chrom_index.get(chrom, [])\n    \n    def get_segments_for_individual(self, individual_id):\n        \"\"\"Get all segments involving a particular individual.\"\"\"\n        return self.id_index.get(individual_id, [])\n    \n    def get_segments_for_haplotype(self, individual_id, haplotype):\n        \"\"\"Get all segments on a specific haplotype (for phased data).\"\"\"\n        key = (individual_id, haplotype)\n        return self.haplotype_index.get(key, [])\n    \n    def get_total_ibd_between_id_sets(self, id_set1, id_set2):\n        \"\"\"Calculate total IBD shared between two sets of IDs.\"\"\"\n        total_ibd = 0\n        \n        for id1 in id_set1:\n            for id2 in id_set2:\n                if id1 == id2:\n                    continue\n                    \n                pair_key = frozenset({id1, id2})\n                if pair_key in self.pair_stats:\n                    stats = self.pair_stats[pair_key]\n                    total_ibd += stats['total_half'] + stats['total_full']\n        \n        return total_ibd\n\n# Create test data - generate synthetic IBD segments\nimport random\n\ndef generate_random_segments(num_individuals, num_segments, phased=False):\n    \"\"\"Generate random IBD segments for benchmarking.\"\"\"\n    segments = []\n    \n    for _ in range(num_segments):\n        # Choose random individuals\n        id1, id2 = random.sample(range(1, num_individuals + 1), 2)\n        \n        # Generate random segment attributes\n        chrom = random.randint(1, 22)\n        start_pos = random.randint(1, 200_000_000)\n        end_pos = start_pos + random.randint(1_000_000, 50_000_000)\n        is_ibd2 = random.random() < 0.2  # 20% chance of IBD2\n        length_cm = random.uniform(7.0, 100.0)  # Random length between 7 and 100 cM\n        \n        if not phased:\n            # Unphased format\n            segments.append([id1, id2, chrom, start_pos, end_pos, is_ibd2, length_cm])\n        else:\n            # Phased format (add haplotype info)\n            hap1 = random.randint(0, 1)\n            hap2 = random.randint(0, 1)\n            segments.append([id1, id2, hap1, hap2, chrom, start_pos/1_000_000, end_pos/1_000_000, length_cm])\n    \n    return segments\n\n# Benchmark the IBD index\ndef benchmark_ibd_index():\n    num_individuals = 500\n    num_segments = 10000\n    \n    print(f\"Generating {num_segments} random segments for {num_individuals} individuals...\")\n    unphased_segments = generate_random_segments(num_individuals, num_segments)\n    phased_segments = generate_random_segments(num_individuals, num_segments, phased=True)\n    \n    # Time index creation\n    print(\"\\nBenchmarking index creation...\")\n    start_time = time.time()\n    index = IBDIndex()\n    index.add_segments(unphased_segments, \"unphased\")\n    index.add_segments(phased_segments, \"phased\")\n    end_time = time.time()\n    print(f\"Time to build index: {end_time - start_time:.4f} seconds\")\n    \n    # Time pair lookup\n    print(\"\\nBenchmarking pair lookup (1000 random pairs)...\")\n    start_time = time.time()\n    for _ in range(1000):\n        id1, id2 = random.sample(range(1, num_individuals + 1), 2)\n        segments = index.get_segments_for_pair(id1, id2)\n    end_time = time.time()\n    print(f\"Time for 1000 pair lookups: {end_time - start_time:.4f} seconds\")\n    print(f\"Average time per lookup: {(end_time - start_time) / 1000:.6f} seconds\")\n    \n    # Time individual lookup\n    print(\"\\nBenchmarking individual lookup (1000 random individuals)...\")\n    start_time = time.time()\n    for _ in range(1000):\n        id1 = random.randint(1, num_individuals)\n        segments = index.get_segments_for_individual(id1)\n    end_time = time.time()\n    print(f\"Time for 1000 individual lookups: {end_time - start_time:.4f} seconds\")\n    print(f\"Average time per lookup: {(end_time - start_time) / 1000:.6f} seconds\")\n    \n    # Time ID set calculations\n    print(\"\\nBenchmarking ID set calculations (100 random set pairs)...\")\n    start_time = time.time()\n    for _ in range(100):\n        set1 = set(random.sample(range(1, num_individuals + 1), 10))\n        set2 = set(random.sample(range(1, num_individuals + 1), 10))\n        total_ibd = index.get_total_ibd_between_id_sets(set1, set2)\n    end_time = time.time()\n    print(f\"Time for 100 ID set calculations: {end_time - start_time:.4f} seconds\")\n    print(f\"Average time per calculation: {(end_time - start_time) / 100:.6f} seconds\")\n    \n    return index\n\n# Run the benchmark (un-comment to run)\n# index = benchmark_ibd_index()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Exercise 2: Extend the up-node dictionary implementation to handle additional relationship metadata for v3.",
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": "# Solution for Exercise 2\nclass ExtendedUpNodeDict:\n    \"\"\"\n    Enhanced up-node dictionary that supports rich relationship metadata,\n    including relationship types, confidence scores, and temporal information.\n    \"\"\"\n    def __init__(self):\n        self.up_dict = {}  # Main relationship storage\n        self.metadata = {}  # Metadata storage (separate for memory efficiency)\n        \n    def add_individual(self, individual_id, metadata=None):\n        \"\"\"Add an individual to the pedigree.\"\"\"\n        if individual_id not in self.up_dict:\n            self.up_dict[individual_id] = {}\n            \n        # Store metadata if provided\n        if metadata and individual_id not in self.metadata:\n            self.metadata[individual_id] = metadata\n    \n    def add_relationship(self, child_id, parent_id, relationship_type='biological', \n                         confidence=1.0, year=None, source=None):\n        \"\"\"\n        Add a relationship between parent and child with metadata.\n        \n        Args:\n            child_id: ID of the child\n            parent_id: ID of the parent\n            relationship_type: Type of relationship (biological, adoptive, step, etc.)\n            confidence: Confidence score (0.0-1.0) for the relationship\n            year: Year when the relationship began (useful for adoptions, etc.)\n            source: Source of information for this relationship\n        \"\"\"\n        # Ensure both individuals exist\n        self.add_individual(child_id)\n        self.add_individual(parent_id)\n        \n        # Check for cycle\n        if self._would_create_cycle(child_id, parent_id):\n            return False\n            \n        # Create relationship metadata\n        rel_metadata = {\n            'type': relationship_type,\n            'confidence': confidence\n        }\n        \n        # Add optional fields\n        if year is not None:\n            rel_metadata['year'] = year\n        if source is not None:\n            rel_metadata['source'] = source\n            \n        # Store the relationship\n        self.up_dict[child_id][parent_id] = rel_metadata\n        return True\n    \n    def _would_create_cycle(self, child_id, parent_id):\n        \"\"\"Check if adding parent would create a cycle.\"\"\"\n        # Start with the proposed parent\n        to_visit = [parent_id]\n        visited = set(to_visit)\n        \n        while to_visit:\n            current = to_visit.pop(0)\n            \n            # If we can reach the child, adding this edge would create a cycle\n            if current == child_id:\n                return True\n                \n            # Add this individual's parents to the search\n            if current in self.up_dict:\n                for next_id in self.up_dict[current]:\n                    if next_id not in visited:\n                        to_visit.append(next_id)\n                        visited.add(next_id)\n        \n        return False\n    \n    def get_parents(self, individual_id):\n        \"\"\"Get all parents for an individual.\"\"\"\n        if individual_id not in self.up_dict:\n            return {}\n        return self.up_dict[individual_id]\n    \n    def get_relationship_metadata(self, child_id, parent_id):\n        \"\"\"Get metadata for a specific relationship.\"\"\"\n        if (child_id in self.up_dict and \n            parent_id in self.up_dict[child_id]):\n            return self.up_dict[child_id][parent_id]\n        return None\n    \n    def get_biological_parents(self, individual_id):\n        \"\"\"Get only biological parents of an individual.\"\"\"\n        if individual_id not in self.up_dict:\n            return {}\n        \n        return {\n            parent_id: metadata \n            for parent_id, metadata in self.up_dict[individual_id].items()\n            if metadata.get('type') == 'biological'\n        }\n    \n    def get_children(self, parent_id):\n        \"\"\"Get all children of an individual.\"\"\"\n        return {\n            child_id: metadata\n            for child_id, parents in self.up_dict.items()\n            if parent_id in parents\n            for metadata in [parents[parent_id]]\n        }\n    \n    def get_biological_children(self, parent_id):\n        \"\"\"Get biological children of an individual.\"\"\"\n        return {\n            child_id: metadata\n            for child_id, metadata in self.get_children(parent_id).items()\n            if metadata.get('type') == 'biological'\n        }\n    \n    def get_all_relationships(self):\n        \"\"\"Get all relationships in the pedigree.\"\"\"\n        relationships = []\n        for child_id, parents in self.up_dict.items():\n            for parent_id, metadata in parents.items():\n                relationships.append({\n                    'child': child_id,\n                    'parent': parent_id,\n                    'metadata': metadata\n                })\n        return relationships\n    \n    def to_standard_format(self):\n        \"\"\"Convert to standard up-node dictionary format for compatibility.\"\"\"\n        standard_dict = {}\n        for child_id, parents in self.up_dict.items():\n            standard_dict[child_id] = {parent_id: 1 for parent_id in parents}\n        return standard_dict\n    \n    def from_standard_format(self, standard_dict, default_type='biological', default_confidence=0.9):\n        \"\"\"Initialize from a standard up-node dictionary.\"\"\"\n        self.up_dict = {}\n        for child_id, parents in standard_dict.items():\n            self.up_dict[child_id] = {}\n            for parent_id in parents:\n                self.add_relationship(child_id, parent_id, default_type, default_confidence)\n    \n    def filter_by_confidence(self, min_confidence=0.8):\n        \"\"\"Get a filtered view of relationships above a confidence threshold.\"\"\"\n        filtered_dict = {}\n        for child_id, parents in self.up_dict.items():\n            filtered_dict[child_id] = {}\n            for parent_id, metadata in parents.items():\n                if metadata.get('confidence', 0) >= min_confidence:\n                    filtered_dict[child_id][parent_id] = metadata\n        return filtered_dict\n    \n    def merge_pedigrees(self, other_pedigree, override=False):\n        \"\"\"\n        Merge another extended pedigree into this one.\n        \n        Args:\n            other_pedigree: Another ExtendedUpNodeDict\n            override: Whether to override existing relationships with ones from other_pedigree\n        \"\"\"\n        for child_id, parents in other_pedigree.up_dict.items():\n            if child_id not in self.up_dict:\n                self.up_dict[child_id] = {}\n                \n            for parent_id, metadata in parents.items():\n                if parent_id not in self.up_dict[child_id] or override:\n                    # Only add if not creating a cycle\n                    if not self._would_create_cycle(child_id, parent_id):\n                        self.up_dict[child_id][parent_id] = metadata.copy()\n        \n        # Merge individual metadata\n        for ind_id, metadata in other_pedigree.metadata.items():\n            if ind_id not in self.metadata or override:\n                self.metadata[ind_id] = metadata.copy()\n\n# Test the extended up-node dictionary\nextended_pedigree = ExtendedUpNodeDict()\n\n# Add individuals with metadata\nextended_pedigree.add_individual(1000, {'sex': 'M', 'birth_year': 1950})\nextended_pedigree.add_individual(1001, {'sex': 'F', 'birth_year': 1952})\nextended_pedigree.add_individual(1002, {'sex': 'M', 'birth_year': 1975})\nextended_pedigree.add_individual(1003, {'sex': 'F', 'birth_year': 1977})\nextended_pedigree.add_individual(1004, {'sex': 'M', 'birth_year': 2000})\nextended_pedigree.add_individual(1005, {'sex': 'F', 'birth_year': 2002})\n\n# Add relationships\nextended_pedigree.add_relationship(1002, 1000, 'biological', 0.99, source='DNA test')\nextended_pedigree.add_relationship(1002, 1001, 'biological', 0.99, source='DNA test')\nextended_pedigree.add_relationship(1003, 1000, 'biological', 0.95, source='DNA test')\nextended_pedigree.add_relationship(1003, 1001, 'biological', 0.95, source='DNA test')\nextended_pedigree.add_relationship(1004, 1002, 'biological', 0.99, source='Birth certificate')\nextended_pedigree.add_relationship(1004, 1003, 'adoptive', 0.99, year=2002, source='Adoption records')\nextended_pedigree.add_relationship(1005, 1002, 'biological', 0.99, source='Birth certificate')\nextended_pedigree.add_relationship(1005, 1003, 'biological', 0.95, source='DNA test')\n\n# Display all relationships\nprint(\"All relationships in extended pedigree:\")\nfor rel in extended_pedigree.get_all_relationships():\n    print(f\"Child {rel['child']} -> Parent {rel['parent']}: {rel['metadata']}\")\n\n# Convert to standard format for visualization\nstandard_dict = extended_pedigree.to_standard_format()\nvisualize_pedigree(standard_dict)\n\n# Filter by confidence\nhigh_confidence = extended_pedigree.filter_by_confidence(0.98)\nprint(\"\\nHigh confidence relationships (≥0.98):\")\nfor child_id, parents in high_confidence.items():\n    if parents:\n        print(f\"Child {child_id} has parents: {list(parents.keys())}\")\n\n# Get biological children\nbio_children = extended_pedigree.get_biological_children(1002)\nprint(f\"\\nBiological children of 1002: {list(bio_children.keys())}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3: Implement and test the cycle detection algorithm for pedigree validation."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": "# Solution for Exercise 3\nclass PedigreeValidator:\n    \"\"\"\n    A class for validating pedigree structures with strong cycle detection\n    and consistency check capabilities aligned with v3 requirements.\n    \"\"\"\n    def __init__(self, up_node_dict=None, bio_info=None):\n        self.up_node_dict = up_node_dict or {}\n        self.bio_info = bio_info or []\n        self.id_to_info = {info['genotype_id']: info for info in self.bio_info} if bio_info else {}\n        self.errors = []\n        self.warnings = []\n        \n    def detect_cycles(self):\n        \"\"\"\n        Detect cycles in the pedigree structure.\n        \n        Returns:\n            List of cycles found, where each cycle is a list of individual IDs\n        \"\"\"\n        cycles = []\n        \n        # For each individual\n        for start_id in self.up_node_dict.keys():\n            # Do DFS to find cycles\n            visited = {}  # Maps individual to path index\n            path = [start_id]\n            self._dfs_detect_cycles(start_id, visited, path, cycles)\n            \n        return cycles\n            \n    def _dfs_detect_cycles(self, current_id, visited, path, cycles):\n        \"\"\"DFS helper for cycle detection.\"\"\"\n        visited[current_id] = len(path) - 1\n        \n        # Check all parents of current individual\n        if current_id in self.up_node_dict:\n            for parent_id in self.up_node_dict[current_id]:\n                if parent_id not in visited:\n                    # Continue DFS with this parent\n                    path.append(parent_id)\n                    self._dfs_detect_cycles(parent_id, visited, path, cycles)\n                    path.pop()\n                elif parent_id in path:  # Found a cycle\n                    # Extract the cycle\n                    cycle_start = visited[parent_id]\n                    cycle = path[cycle_start:]\n                    cycle.append(parent_id)  # Close the cycle\n                    \n                    # Only add if this exact cycle is not already found\n                    if tuple(cycle) not in [tuple(c) for c in cycles]:\n                        cycles.append(cycle)\n                        \n        # Remove from visited when backtracking\n        del visited[current_id]\n    \n    def check_temporal_consistency(self):\n        \"\"\"\n        Check if the pedigree is temporally consistent (parents older than children).\n        \n        Returns:\n            List of inconsistencies\n        \"\"\"\n        inconsistencies = []\n        \n        for child_id, parents in self.up_node_dict.items():\n            child_birth = self._get_birth_year(child_id)\n            \n            for parent_id in parents:\n                parent_birth = self._get_birth_year(parent_id)\n                \n                # Skip if birth years unknown\n                if child_birth is None or parent_birth is None:\n                    continue\n                    \n                # Child should be born after parent\n                if child_birth <= parent_birth:\n                    inconsistencies.append({\n                        'type': 'temporal',\n                        'child': child_id,\n                        'child_birth': child_birth,\n                        'parent': parent_id,\n                        'parent_birth': parent_birth\n                    })\n        \n        return inconsistencies\n    \n    def _get_birth_year(self, individual_id):\n        \"\"\"Get birth year from bio_info.\"\"\"\n        if individual_id in self.id_to_info:\n            # Prefer direct birth_year if available\n            birth_year = self.id_to_info[individual_id].get('birth_year')\n            if birth_year is not None:\n                return birth_year\n                \n            # Alternative: calculate from age if available\n            age = self.id_to_info[individual_id].get('age')\n            if age is not None:\n                # Assuming bio_info was created in current year\n                current_year = 2024  # This would ideally come from a timestamp\n                return current_year - age\n                \n        return None\n    \n    def check_gender_consistency(self):\n        \"\"\"\n        Check if pedigree relationships are gender-consistent.\n        \n        Returns:\n            List of inconsistencies\n        \"\"\"\n        inconsistencies = []\n        \n        for child_id, parents in self.up_node_dict.items():\n            parent_ids = list(parents.keys())\n            \n            # Skip if less than 2 parents\n            if len(parent_ids) < 2:\n                continue\n                \n            # Get parent genders\n            parent_genders = []\n            for parent_id in parent_ids:\n                if parent_id in self.id_to_info:\n                    gender = self.id_to_info[parent_id].get('sex')\n                    if gender:\n                        parent_genders.append(gender)\n            \n            # Check if parents have same gender (M/M or F/F)\n            if len(parent_genders) >= 2 and len(set(parent_genders)) == 1:\n                inconsistencies.append({\n                    'type': 'gender',\n                    'child': child_id,\n                    'parents': parent_ids,\n                    'gender': parent_genders[0]\n                })\n        \n        return inconsistencies\n    \n    def check_max_parents(self, max_parents=2):\n        \"\"\"\n        Check if any individual has more than max_parents.\n        \n        Returns:\n            List of individuals with excess parents\n        \"\"\"\n        excess_parents = []\n        \n        for child_id, parents in self.up_node_dict.items():\n            if len(parents) > max_parents:\n                excess_parents.append({\n                    'individual': child_id,\n                    'parent_count': len(parents),\n                    'parents': list(parents.keys())\n                })\n                \n        return excess_parents\n    \n    def check_disconnected_components(self):\n        \"\"\"\n        Check for disconnected components in the pedigree.\n        \n        Returns:\n            List of components, where each component is a set of individual IDs\n        \"\"\"\n        # Build a graph including both parent->child and child->parent edges\n        graph = {}\n        for id1 in self.up_node_dict:\n            if id1 not in graph:\n                graph[id1] = set()\n                \n            # Add parent->child edges\n            for id2 in self.up_node_dict[id1]:\n                if id2 not in graph:\n                    graph[id2] = set()\n                graph[id1].add(id2)\n                graph[id2].add(id1)\n        \n        # Find connected components\n        components = []\n        visited = set()\n        \n        for start_id in graph:\n            if start_id in visited:\n                continue\n                \n            # BFS to find connected component\n            component = set()\n            queue = [start_id]\n            \n            while queue:\n                current = queue.pop(0)\n                if current in visited:\n                    continue\n                    \n                visited.add(current)\n                component.add(current)\n                \n                for neighbor in graph.get(current, []):\n                    if neighbor not in visited:\n                        queue.append(neighbor)\n                        \n            components.append(component)\n            \n        return components\n    \n    def validate(self, verbose=True):\n        \"\"\"\n        Run all validation checks and report results.\n        \n        Args:\n            verbose: Whether to print detailed results\n            \n        Returns:\n            bool: True if pedigree is valid, False if errors found\n        \"\"\"\n        self.errors = []\n        self.warnings = []\n        \n        # Check for cycles\n        cycles = self.detect_cycles()\n        if cycles:\n            self.errors.append({\n                'check': 'cycles',\n                'message': f\"Found {len(cycles)} cycles in pedigree\",\n                'details': cycles\n            })\n            \n        # Check temporal consistency\n        temporal_issues = self.check_temporal_consistency()\n        if temporal_issues:\n            self.errors.append({\n                'check': 'temporal',\n                'message': f\"Found {len(temporal_issues)} temporal inconsistencies\",\n                'details': temporal_issues\n            })\n            \n        # Check gender consistency\n        gender_issues = self.check_gender_consistency()\n        if gender_issues:\n            # This is a warning rather than error (same-sex parents are possible)\n            self.warnings.append({\n                'check': 'gender',\n                'message': f\"Found {len(gender_issues)} same-gender parent pairs\",\n                'details': gender_issues\n            })\n            \n        # Check max parents\n        excess_parents = self.check_max_parents()\n        if excess_parents:\n            self.errors.append({\n                'check': 'max_parents',\n                'message': f\"Found {len(excess_parents)} individuals with >2 parents\",\n                'details': excess_parents\n            })\n            \n        # Check for disconnected components\n        components = self.check_disconnected_components()\n        if len(components) > 1:\n            # This is a warning rather than error\n            self.warnings.append({\n                'check': 'components',\n                'message': f\"Pedigree has {len(components)} disconnected components\",\n                'details': [list(c) for c in components]\n            })\n            \n        # Print results if verbose\n        if verbose:\n            if not self.errors and not self.warnings:\n                print(\"Pedigree is valid with no issues detected.\")\n            else:\n                if self.errors:\n                    print(f\"Found {len(self.errors)} errors:\")\n                    for i, error in enumerate(self.errors):\n                        print(f\"  Error {i+1}: {error['message']}\")\n                        \n                if self.warnings:\n                    print(f\"Found {len(self.warnings)} warnings:\")\n                    for i, warning in enumerate(self.warnings):\n                        print(f\"  Warning {i+1}: {warning['message']}\")\n        \n        return len(self.errors) == 0\n\n# Test the PedigreeValidator with different pedigree structures\n# 1. Create a valid pedigree\nvalid_pedigree = {\n    1000: {1001: 1, 1002: 1},  # 1000 has parents 1001 and 1002\n    1003: {1001: 1, 1002: 1},  # 1003 has the same parents (sibling of 1000)\n    1004: {1000: 1, 1005: 1},  # 1004 has parents 1000 and 1005\n    1001: {},                  # 1001 has no recorded parents\n    1002: {},                  # 1002 has no recorded parents\n    1005: {}                   # 1005 has no recorded parents\n}\n\n# Bio info for testing temporal and gender consistency\nvalid_bio_info = [\n    {'genotype_id': 1000, 'sex': 'M', 'birth_year': 1975},\n    {'genotype_id': 1001, 'sex': 'M', 'birth_year': 1950},\n    {'genotype_id': 1002, 'sex': 'F', 'birth_year': 1952},\n    {'genotype_id': 1003, 'sex': 'F', 'birth_year': 1977},\n    {'genotype_id': 1004, 'sex': 'M', 'birth_year': 2000},\n    {'genotype_id': 1005, 'sex': 'F', 'birth_year': 1980}\n]\n\n# Test valid pedigree\nprint(\"Testing valid pedigree:\")\nvalidator = PedigreeValidator(valid_pedigree, valid_bio_info)\nis_valid = validator.validate()\nprint(f\"Is pedigree valid? {is_valid}\\n\")\n\n# 2. Create a pedigree with a cycle\ncycle_pedigree = valid_pedigree.copy()\ncycle_pedigree[1001] = {1004: 1}  # Creates a cycle: 1001 -> 1000 -> 1004 -> 1001\n\nprint(\"Testing pedigree with cycle:\")\nvalidator = PedigreeValidator(cycle_pedigree, valid_bio_info)\nis_valid = validator.validate()\nprint(f\"Is pedigree valid? {is_valid}\\n\")\n\n# 3. Create a pedigree with temporal inconsistency\ntemporal_pedigree = valid_pedigree.copy()\ntemporal_bio_info = valid_bio_info.copy()\ntemporal_bio_info[4] = {'genotype_id': 1004, 'sex': 'M', 'birth_year': 1970}  # Child born before parent\n\nprint(\"Testing pedigree with temporal inconsistency:\")\nvalidator = PedigreeValidator(temporal_pedigree, temporal_bio_info)\nis_valid = validator.validate()\nprint(f\"Is pedigree valid? {is_valid}\\n\")\n\n# 4. Create a pedigree with gender inconsistency\ngender_pedigree = valid_pedigree.copy()\ngender_bio_info = valid_bio_info.copy()\ngender_bio_info[1] = {'genotype_id': 1001, 'sex': 'F', 'birth_year': 1950}  # Both parents are female\n\nprint(\"Testing pedigree with gender inconsistency:\")\nvalidator = PedigreeValidator(gender_pedigree, gender_bio_info)\nis_valid = validator.validate()\nprint(f\"Is pedigree valid? {is_valid}\\n\")\n\n# 5. Create a pedigree with disconnected components\ndisconnected_pedigree = valid_pedigree.copy()\ndisconnected_pedigree[1006] = {1007: 1, 1008: 1}  # Add a separate family\ndisconnected_pedigree[1007] = {}\ndisconnected_pedigree[1008] = {}\n\nprint(\"Testing pedigree with disconnected components:\")\nvalidator = PedigreeValidator(disconnected_pedigree, valid_bio_info)\nis_valid = validator.validate()\nprint(f\"Is pedigree valid? {is_valid}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 4: Create a visualization function that renders a pedigree from an up-node dictionary, highlighting different generations and relationship types."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": "# Solution for Exercise 4\nclass PedigreeVisualizer:\n    \"\"\"\n    Enhanced visualization tool for pedigrees, with support for multi-generational\n    layouts, relationship types, and individual metadata.\n    \"\"\"\n    def __init__(self, up_node_dict, bio_info=None, extended_relationships=None):\n        self.up_node_dict = up_node_dict\n        self.bio_info = bio_info or []\n        self.id_to_info = {info['genotype_id']: info for info in self.bio_info} if bio_info else {}\n        self.extended_relationships = extended_relationships or {}\n        \n    def assign_generations(self):\n        \"\"\"\n        Assign generation levels to all individuals in the pedigree.\n        \n        Returns:\n            Dict mapping individual IDs to generation numbers (0 = earliest ancestors)\n        \"\"\"\n        # First identify all individuals with no children (they will be at the latest generation)\n        leaf_ids = set(self.up_node_dict.keys())\n        for parents in self.up_node_dict.values():\n            for parent_id in parents:\n                if parent_id in leaf_ids:\n                    leaf_ids.remove(parent_id)\n        \n        # Assign generations through BFS from leaf nodes\n        generations = {}\n        queue = [(leaf_id, 0) for leaf_id in leaf_ids]\n        visited = set()\n        \n        while queue:\n            ind_id, gen = queue.pop(0)\n            \n            if ind_id in visited:\n                # We've seen this node before - take the maximum generation value\n                generations[ind_id] = max(generations[ind_id], gen)\n                continue\n                \n            visited.add(ind_id)\n            generations[ind_id] = gen\n            \n            # Add parents with incremented generation\n            parents = self.up_node_dict.get(ind_id, {})\n            for parent_id in parents:\n                queue.append((parent_id, gen + 1))\n        \n        # Adjust to make earliest generation 0\n        max_gen = max(generations.values()) if generations else 0\n        return {ind_id: max_gen - gen for ind_id, gen in generations.items()}\n    \n    def create_pedigree_graph(self):\n        \"\"\"Create a NetworkX graph representing the pedigree for visualization.\"\"\"\n        import networkx as nx\n        \n        # Create directed graph\n        G = nx.DiGraph()\n        \n        # Add nodes with metadata\n        for ind_id in self.up_node_dict:\n            # Get metadata from bio_info\n            node_attrs = {'id': ind_id}\n            \n            if ind_id in self.id_to_info:\n                info = self.id_to_info[ind_id]\n                node_attrs.update(info)\n                \n            # Add inferred flag for latent ancestors (negative IDs)\n            node_attrs['inferred'] = int(ind_id) < 0\n            \n            # Add node to graph\n            G.add_node(ind_id, **node_attrs)\n        \n        # Add edges with metadata\n        for child_id, parents in self.up_node_dict.items():\n            for parent_id in parents:\n                # Get relationship type if available\n                edge_attrs = {}\n                \n                # If we have extended relationship info\n                if self.extended_relationships and child_id in self.extended_relationships:\n                    rel_info = self.extended_relationships[child_id].get(parent_id, {})\n                    edge_attrs.update(rel_info)\n                \n                # Add edge from parent to child\n                G.add_edge(parent_id, child_id, **edge_attrs)\n        \n        return G\n    \n    def visualize(self, \n                 figsize=(14, 10), \n                 title='Pedigree Structure', \n                 show_labels=True,\n                 show_sex=True,\n                 show_birth_year=True,\n                 edge_style_field='type',\n                 node_size_field=None,\n                 node_color_field='sex',\n                 generation_layout=True):\n        \"\"\"\n        Create a comprehensive visualization of the pedigree.\n        \n        Args:\n            figsize: Figure size\n            title: Plot title\n            show_labels: Whether to show individual IDs as labels\n            show_sex: Whether to use different colors for male/female\n            show_birth_year: Whether to show birth years in labels\n            edge_style_field: Field to use for edge styles\n            node_size_field: Field to use for node sizes (e.g., 'age')\n            node_color_field: Field to use for node colors\n            generation_layout: Whether to use a generational layout\n            \n        Returns:\n            matplotlib figure and axes\n        \"\"\"\n        import matplotlib.pyplot as plt\n        import networkx as nx\n        import numpy as np\n        \n        # Create graph\n        G = self.create_pedigree_graph()\n        \n        # Assign generations for layout\n        if generation_layout:\n            generations = self.assign_generations()\n            # Add generation info to nodes\n            nx.set_node_attributes(G, generations, 'generation')\n        \n        # Set up figure\n        fig, ax = plt.subplots(figsize=figsize)\n        \n        # Create layout\n        if generation_layout:\n            # Custom layout with generations on y-axis\n            pos = {}\n            generation_groups = {}\n            \n            # Group nodes by generation\n            for node, gen in generations.items():\n                if gen not in generation_groups:\n                    generation_groups[gen] = []\n                generation_groups[gen].append(node)\n            \n            # Position nodes by generation\n            for gen, nodes in generation_groups.items():\n                # Sort nodes within generation (for consistent layout)\n                nodes.sort()\n                x_step = 1.0 / (len(nodes) + 1)\n                for i, node in enumerate(nodes):\n                    pos[node] = ((i + 1) * x_step, gen)\n            \n            # Adjust layout for better spacing\n            pos = {k: (x, y * 2) for k, (x, y) in pos.items()}\n        else:\n            # Use GraphViz for standard layout\n            try:\n                pos = nx.nx_agraph.graphviz_layout(G, prog='dot')\n            except:\n                # Fall back to spring layout\n                pos = nx.spring_layout(G, seed=42)\n        \n        # Prepare node colors\n        if show_sex or node_color_field == 'sex':\n            # Color by sex\n            sex_colors = {'M': '#ADD8E6', 'F': '#FFCCCB'}  # Light blue for male, light pink for female\n            node_colors = []\n            \n            for node in G.nodes():\n                if 'sex' in G.nodes[node]:\n                    sex = G.nodes[node]['sex']\n                    color = sex_colors.get(sex, '#CCCCCC')  # Gray for unknown\n                elif G.nodes[node].get('inferred', False):\n                    color = '#EEEEEE'  # Light gray for inferred\n                else:\n                    color = '#CCCCCC'  # Gray for others\n                node_colors.append(color)\n        elif node_color_field:\n            # Color by specified field\n            values = [G.nodes[node].get(node_color_field, 0) for node in G.nodes()]\n            norm = plt.Normalize(min(values), max(values))\n            cmap = plt.cm.viridis\n            node_colors = [cmap(norm(val)) if val is not None else '#CCCCCC' for val in values]\n        else:\n            # Default colors\n            node_colors = ['#AACCFF' if not G.nodes[node].get('inferred', False) else '#CCCCCC' \n                          for node in G.nodes()]\n            \n        # Prepare node sizes\n        if node_size_field and node_size_field in next(iter(G.nodes(data=True)))[1]:\n            sizes = [G.nodes[node].get(node_size_field, 300) for node in G.nodes()]\n            # Normalize sizes between 200 and 800\n            if len(set(sizes)) > 1:\n                min_size, max_size = min(sizes), max(sizes)\n                node_sizes = [200 + 600 * (s - min_size) / (max_size - min_size) for s in sizes]\n            else:\n                node_sizes = [500 for _ in sizes]\n        else:\n            # Default sizes\n            node_sizes = [500 if not G.nodes[node].get('inferred', False) else 300 \n                         for node in G.nodes()]\n        \n        # Prepare node shapes\n        node_shapes = []\n        for node in G.nodes():\n            if G.nodes[node].get('inferred', False):\n                node_shapes.append('o')  # Circle for inferred\n            else:\n                node_shapes.append('s')  # Square for observed\n        \n        # Draw nodes\n        for shape in set(node_shapes):\n            indices = [i for i, s in enumerate(node_shapes) if s == shape]\n            nx.draw_networkx_nodes(G, pos,\n                                  nodelist=[list(G.nodes())[i] for i in indices],\n                                  node_color=[node_colors[i] for i in indices],\n                                  node_size=[node_sizes[i] for i in indices],\n                                  alpha=0.8,\n                                  node_shape=shape,\n                                  ax=ax)\n        \n        # Prepare edge styles based on relationship type\n        edge_styles = {}\n        if edge_style_field:\n            # Group edges by the specified field\n            for u, v, data in G.edges(data=True):\n                edge_type = data.get(edge_style_field, 'unknown')\n                if edge_type not in edge_styles:\n                    edge_styles[edge_type] = []\n                edge_styles[edge_type].append((u, v))\n        else:\n            # Just one group for all edges\n            edge_styles['default'] = list(G.edges())\n        \n        # Draw edges with different styles\n        style_map = {\n            'biological': {'style': 'solid', 'color': 'blue', 'width': 1.5},\n            'adoptive': {'style': 'dashed', 'color': 'green', 'width': 1.5},\n            'step': {'style': 'dotted', 'color': 'purple', 'width': 1.5},\n            'foster': {'style': 'dashdot', 'color': 'orange', 'width': 1.5},\n            'default': {'style': 'solid', 'color': 'black', 'width': 1.0},\n            'unknown': {'style': 'solid', 'color': 'gray', 'width': 1.0}\n        }\n        \n        for edge_type, edge_list in edge_styles.items():\n            if not edge_list:\n                continue\n                \n            style = style_map.get(edge_type, style_map['default'])\n            nx.draw_networkx_edges(G, pos,\n                                 edgelist=edge_list,\n                                 style=style['style'],\n                                 edge_color=style['color'],\n                                 width=style['width'],\n                                 arrowstyle='-|>',\n                                 arrowsize=15,\n                                 ax=ax)\n        \n        # Prepare node labels\n        if show_labels:\n            labels = {}\n            for node in G.nodes():\n                label_parts = [str(node)]\n                \n                if show_birth_year and 'birth_year' in G.nodes[node]:\n                    label_parts.append(f\"({G.nodes[node]['birth_year']})\")\n                    \n                labels[node] = '\\n'.join(label_parts)\n                \n            nx.draw_networkx_labels(G, pos, labels=labels, font_size=10, ax=ax)\n        \n        # Create a legend for edge styles\n        if edge_styles and len(edge_styles) > 1:\n            legend_elements = []\n            import matplotlib.lines as mlines\n            for edge_type, style in style_map.items():\n                if edge_type in edge_styles and edge_styles[edge_type]:\n                    line = mlines.Line2D([], [], \n                                        color=style['color'],\n                                        linestyle=style['style'],\n                                        linewidth=style['width'],\n                                        label=edge_type.capitalize())\n                    legend_elements.append(line)\n            \n            if legend_elements:\n                ax.legend(handles=legend_elements, loc='upper right')\n        \n        # Create a legend for node colors if using sex\n        if show_sex or node_color_field == 'sex':\n            import matplotlib.patches as mpatches\n            legend_elements = [\n                mpatches.Patch(color='#ADD8E6', label='Male'),\n                mpatches.Patch(color='#FFCCCB', label='Female'),\n                mpatches.Patch(color='#CCCCCC', label='Unknown')\n            ]\n            if any(G.nodes[node].get('inferred', False) for node in G.nodes()):\n                legend_elements.append(mpatches.Patch(color='#EEEEEE', label='Inferred'))\n                \n            ax.legend(handles=legend_elements, loc='upper left')\n        \n        # Add title and clean up axes\n        plt.title(title)\n        plt.axis('off')\n        plt.tight_layout()\n        \n        return fig, ax\n\n# Create test data with extended relationship information\ntest_pedigree = {\n    1000: {1001: 1, 1002: 1},  # 1000 has parents 1001 and 1002\n    1003: {1001: 1, 1002: 1},  # 1003 has parents 1001 and 1002 (sibling of 1000)\n    1004: {1000: 1, 1005: 1},  # 1004 has parents 1000 and 1005\n    1006: {1000: 1, 1005: 1},  # 1006 has parents 1000 and 1005 (sibling of 1004)\n    1007: {1003: 1, 1008: 1},  # 1007 has parents 1003 and 1008\n    1009: {-1: 1, 1007: 1},    # 1009 has parents -1 (inferred) and 1007\n    1001: {},\n    1002: {},\n    1005: {},\n    1008: {},\n    -1: {}\n}\n\n# Bio info with age, sex, and birth year\ntest_bio_info = [\n    {'genotype_id': 1000, 'sex': 'M', 'birth_year': 1950, 'age': 74},\n    {'genotype_id': 1001, 'sex': 'M', 'birth_year': 1925, 'age': 99},\n    {'genotype_id': 1002, 'sex': 'F', 'birth_year': 1927, 'age': 97},\n    {'genotype_id': 1003, 'sex': 'F', 'birth_year': 1952, 'age': 72},\n    {'genotype_id': 1004, 'sex': 'M', 'birth_year': 1975, 'age': 49},\n    {'genotype_id': 1005, 'sex': 'F', 'birth_year': 1955, 'age': 69},\n    {'genotype_id': 1006, 'sex': 'F', 'birth_year': 1977, 'age': 47},\n    {'genotype_id': 1007, 'sex': 'M', 'birth_year': 1980, 'age': 44},\n    {'genotype_id': 1008, 'sex': 'F', 'birth_year': 1982, 'age': 42},\n    {'genotype_id': 1009, 'sex': 'F', 'birth_year': 2005, 'age': 19}\n]\n\n# Extended relationship information\nextended_relationships = {\n    1000: {1001: {'type': 'biological', 'confidence': 0.99},\n           1002: {'type': 'biological', 'confidence': 0.98}},\n    1003: {1001: {'type': 'biological', 'confidence': 0.97},\n           1002: {'type': 'biological', 'confidence': 0.96}},\n    1004: {1000: {'type': 'biological', 'confidence': 0.99},\n           1005: {'type': 'biological', 'confidence': 0.95}},\n    1006: {1000: {'type': 'adoptive', 'confidence': 0.99, 'year': 1980},\n           1005: {'type': 'adoptive', 'confidence': 0.99, 'year': 1980}},\n    1007: {1003: {'type': 'biological', 'confidence': 0.98},\n           1008: {'type': 'biological', 'confidence': 0.97}},\n    1009: {-1: {'type': 'biological', 'confidence': 0.80},\n           1007: {'type': 'biological', 'confidence': 0.95}}\n}\n\n# Create and use the visualizer\nvisualizer = PedigreeVisualizer(test_pedigree, test_bio_info, extended_relationships)\n\n# Basic visualization\nprint(\"Basic Visualization:\")\nvisualizer.visualize(title=\"Basic Pedigree Visualization\")\n\n# Generation layout with relationship types\nprint(\"\\nGeneration Layout with Relationship Types:\")\nvisualizer.visualize(\n    title=\"Pedigree with Generation Layout and Relationship Types\",\n    edge_style_field='type',\n    generation_layout=True,\n    show_birth_year=True\n)\n\n# Visualization with node sizes based on age\nprint(\"\\nVisualization with Node Sizes Based on Age:\")\nvisualizer.visualize(\n    title=\"Pedigree with Node Sizes Based on Age\",\n    edge_style_field='type',\n    node_size_field='age',\n    show_labels=True,\n    generation_layout=True\n)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 5: Implement a memory-efficient version of the relationship coefficient calculator using sparse matrix representations."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": "# Solution for Exercise 5\nimport numpy as np\nimport scipy.sparse as sp\nfrom typing import Dict, Set, List, Tuple, Optional\nimport heapq\nimport time\n\nclass SparseRelationshipCalculator:\n    \"\"\"\n    Memory-efficient relationship coefficient calculator using sparse matrix representations.\n    \n    This implementation uses scipy's sparse matrix functionality to efficiently compute\n    relationship coefficients for large pedigrees with thousands of individuals.\n    \"\"\"\n    def __init__(self, up_node_dict: Dict, max_individuals: Optional[int] = None):\n        \"\"\"\n        Initialize the relationship calculator.\n        \n        Args:\n            up_node_dict: Dictionary mapping child IDs to dictionaries of parent IDs\n            max_individuals: Maximum expected number of individuals (optimization)\n        \"\"\"\n        self.up_node_dict = up_node_dict\n        \n        # Map individual IDs to consecutive indices (0, 1, 2, ...) for matrix operations\n        self.id_to_index = {}\n        self.index_to_id = {}\n        \n        # Build the index mapping\n        all_ids = set(up_node_dict.keys())\n        for child_id, parents in up_node_dict.items():\n            all_ids.update(parents.keys())\n        \n        for idx, ind_id in enumerate(sorted(all_ids)):\n            self.id_to_index[ind_id] = idx\n            self.index_to_id[idx] = ind_id\n            \n        self.n_individuals = len(all_ids)\n        if max_individuals and max_individuals > self.n_individuals:\n            self.n_individuals = max_individuals\n            \n        # Caches for computed values\n        self.path_matrices = {}  # Cache for path matrices at different generations\n        self.kinship_matrix = None  # Will store computed kinship coefficients\n        self.inbreeding_coefficients = None  # Will store inbreeding coefficients\n        \n    def build_parent_matrix(self):\n        \"\"\"\n        Build the parent-child relationship matrix as a sparse matrix.\n        \n        Returns:\n            scipy.sparse.csr_matrix: Parent matrix where M[i,j]=0.5 if j is a parent of i\n        \"\"\"\n        # Initialize sparse matrix in COO format (efficient for construction)\n        row_indices = []\n        col_indices = []\n        values = []\n        \n        # For each child-parent relationship, add 0.5 at the corresponding position\n        for child_id, parents in self.up_node_dict.items():\n            child_idx = self.id_to_index[child_id]\n            \n            for parent_id in parents:\n                parent_idx = self.id_to_index[parent_id]\n                \n                # Child receives 0.5 from each parent\n                row_indices.append(child_idx)\n                col_indices.append(parent_idx)\n                values.append(0.5)\n        \n        # Convert to sparse matrix\n        parent_matrix = sp.coo_matrix(\n            (values, (row_indices, col_indices)),\n            shape=(self.n_individuals, self.n_individuals)\n        )\n        \n        # Convert to CSR format for efficient matrix operations\n        return parent_matrix.tocsr()\n    \n    def compute_path_matrix(self, generations: int):\n        \"\"\"\n        Compute the path matrix for a given number of generations.\n        \n        A path matrix P^n contains the contribution of each ancestor n generations\n        back to each individual.\n        \n        Args:\n            generations: Number of generations to compute\n            \n        Returns:\n            scipy.sparse.csr_matrix: Path matrix for the specified number of generations\n        \"\"\"\n        # Check cache first\n        if generations in self.path_matrices:\n            return self.path_matrices[generations]\n            \n        # Build the parent matrix if not already computed\n        if generations == 1:\n            path_matrix = self.build_parent_matrix()\n        else:\n            # Compute the previous generation path matrix\n            prev_matrix = self.compute_path_matrix(generations - 1)\n            \n            # Multiply by the parent matrix to get the next generation\n            parent_matrix = self.build_parent_matrix()\n            path_matrix = prev_matrix.dot(parent_matrix)\n        \n        # Cache the result\n        self.path_matrices[generations] = path_matrix\n        return path_matrix\n    \n    def compute_kinship_matrix(self, max_generations: int = 10):\n        \"\"\"\n        Compute the complete kinship coefficient matrix using sparse matrix operations.\n        \n        Args:\n            max_generations: Maximum number of generations to consider\n            \n        Returns:\n            scipy.sparse.csr_matrix: Kinship coefficient matrix\n        \"\"\"\n        # Initialize the kinship matrix with identity matrix for self-relationships\n        kinship = sp.eye(self.n_individuals, format='csr')\n        \n        # For each generation, add the contribution\n        for gen in range(1, max_generations + 1):\n            path_matrix = self.compute_path_matrix(gen)\n            \n            # Transpose to get the second path\n            path_matrix_T = path_matrix.transpose()\n            \n            # Calculate product of the two paths (this gives the contribution from\n            # common ancestors at this generation)\n            contribution = path_matrix.multiply(path_matrix_T)\n            \n            # Add to the kinship matrix\n            kinship = kinship + contribution\n            \n            # If the contribution is very small, stop early\n            if contribution.nnz > 0 and contribution.max() < 1e-10:\n                break\n        \n        self.kinship_matrix = kinship\n        return kinship\n    \n    def get_kinship_coefficient(self, id1: int, id2: int):\n        \"\"\"\n        Get the kinship coefficient between two individuals.\n        \n        The kinship coefficient is the probability that random alleles drawn from\n        each individual are identical by descent.\n        \n        Args:\n            id1, id2: Individual IDs to compute kinship between\n            \n        Returns:\n            float: Kinship coefficient between the individuals\n        \"\"\"\n        # Compute the kinship matrix if not already computed\n        if self.kinship_matrix is None:\n            self.compute_kinship_matrix()\n        \n        # Map IDs to indices\n        idx1 = self.id_to_index.get(id1)\n        idx2 = self.id_to_index.get(id2)\n        \n        if idx1 is None or idx2 is None:\n            return 0.0  # One or both individuals not in the pedigree\n        \n        # Get the coefficient from the matrix\n        return self.kinship_matrix[idx1, idx2]\n    \n    def get_relationship_coefficient(self, id1: int, id2: int):\n        \"\"\"\n        Get the relationship coefficient between two individuals.\n        \n        The relationship coefficient is twice the kinship coefficient.\n        \n        Args:\n            id1, id2: Individual IDs\n            \n        Returns:\n            float: Relationship coefficient (2 * kinship coefficient)\n        \"\"\"\n        return 2.0 * self.get_kinship_coefficient(id1, id2)\n    \n    def compute_inbreeding_coefficients(self):\n        \"\"\"\n        Compute inbreeding coefficients for all individuals.\n        \n        The inbreeding coefficient of an individual is the kinship coefficient\n        of its parents.\n        \n        Returns:\n            dict: Mapping from individual IDs to inbreeding coefficients\n        \"\"\"\n        result = {}\n        \n        for child_id, parents in self.up_node_dict.items():\n            parent_ids = list(parents.keys())\n            \n            if len(parent_ids) == 2:\n                # Inbreeding coefficient is the kinship coefficient of the parents\n                inbreeding = self.get_kinship_coefficient(parent_ids[0], parent_ids[1])\n                result[child_id] = inbreeding\n            else:\n                # No inbreeding for individuals with fewer than two parents\n                result[child_id] = 0.0\n        \n        self.inbreeding_coefficients = result\n        return result\n    \n    def get_most_related_pairs(self, n: int = 10):\n        \"\"\"\n        Find the n most closely related pairs of individuals.\n        \n        Args:\n            n: Number of pairs to return\n            \n        Returns:\n            List of (id1, id2, coef) tuples for the most related pairs\n        \"\"\"\n        # Compute the kinship matrix if not already computed\n        if self.kinship_matrix is None:\n            self.compute_kinship_matrix()\n        \n        # Convert to COO format to iterate through non-zero elements\n        coo_matrix = self.kinship_matrix.tocoo()\n        \n        # Use a min heap to track the n highest values\n        heap = []\n        \n        for i, j, value in zip(coo_matrix.row, coo_matrix.col, coo_matrix.data):\n            # Skip self-relationships\n            if i >= j:\n                continue\n                \n            # Convert indices back to IDs\n            id1 = self.index_to_id[i]\n            id2 = self.index_to_id[j]\n            \n            # If heap is not full, add the element\n            if len(heap) < n:\n                heapq.heappush(heap, (value, id1, id2))\n            else:\n                # Replace smallest if this value is larger\n                if value > heap[0][0]:\n                    heapq.heapreplace(heap, (value, id1, id2))\n        \n        # Convert heap to ordered list of (id1, id2, coef) tuples\n        result = [(id1, id2, coef) for coef, id1, id2 in sorted(heap, reverse=True)]\n        return result\n    \n    def clear_cache(self):\n        \"\"\"Clear all cached matrices to free memory.\"\"\"\n        self.path_matrices = {}\n        self.kinship_matrix = None\n        self.inbreeding_coefficients = None\n\n# Benchmark and compare with the original calculator\ndef benchmark_relationship_calculators(pedigree_size=1000, benchmarks=3, max_generations=5):\n    \"\"\"\n    Benchmark the sparse and original relationship calculators.\n    \n    Args:\n        pedigree_size: Number of individuals in the test pedigree\n        benchmarks: Number of benchmarks to run\n        max_generations: Maximum generations to include in the pedigree\n        \n    Returns:\n        Dict with timing results\n    \"\"\"\n    results = {\n        'pedigree_size': pedigree_size,\n        'sparse': {\n            'init_time': [],\n            'matrix_build_time': [],\n            'coefficient_time': [],\n            'memory_estimate': []\n        },\n        'original': {\n            'init_time': [],\n            'coefficient_time': [],\n            'memory_estimate': []\n        }\n    }\n    \n    for b in range(benchmarks):\n        # Generate a test pedigree\n        test_pedigree = generate_test_pedigree(pedigree_size, max_generations)\n        \n        # Get representative pairs for testing\n        all_ids = list(test_pedigree.keys())\n        test_pairs = [(all_ids[i], all_ids[j]) \n                    for i in range(0, len(all_ids), len(all_ids)//10) \n                    for j in range(i+1, len(all_ids), len(all_ids)//10)]\n        \n        # Benchmark sparse calculator\n        start_time = time.time()\n        sparse_calc = SparseRelationshipCalculator(test_pedigree)\n        init_time = time.time() - start_time\n        \n        start_time = time.time()\n        sparse_calc.compute_kinship_matrix()\n        matrix_time = time.time() - start_time\n        \n        start_time = time.time()\n        for id1, id2 in test_pairs:\n            sparse_calc.get_relationship_coefficient(id1, id2)\n        coef_time = time.time() - start_time\n        \n        # Estimate memory usage (very rough estimate)\n        memory_estimate = (sparse_calc.kinship_matrix.data.nbytes + \n                          sparse_calc.kinship_matrix.indptr.nbytes + \n                          sparse_calc.kinship_matrix.indices.nbytes)\n        \n        results['sparse']['init_time'].append(init_time)\n        results['sparse']['matrix_build_time'].append(matrix_time)\n        results['sparse']['coefficient_time'].append(coef_time)\n        results['sparse']['memory_estimate'].append(memory_estimate)\n        \n        # Benchmark original calculator\n        start_time = time.time()\n        orig_calc = RelationshipCalculator(test_pedigree)\n        init_time = time.time() - start_time\n        \n        start_time = time.time()\n        for id1, id2 in test_pairs:\n            orig_calc.get_coefficient(id1, id2)\n        coef_time = time.time() - start_time\n        \n        # Rough estimate of memory for original\n        memory_estimate = len(orig_calc.coefficient_cache) * 24  # approx bytes per entry\n        \n        results['original']['init_time'].append(init_time)\n        results['original']['coefficient_time'].append(coef_time)\n        results['original']['memory_estimate'].append(memory_estimate)\n    \n    # Calculate averages\n    for calc_type in ['sparse', 'original']:\n        for metric in results[calc_type]:\n            results[calc_type][metric + '_avg'] = sum(results[calc_type][metric]) / len(results[calc_type][metric])\n    \n    return results\n\ndef generate_test_pedigree(size, max_generations):\n    \"\"\"Generate a test pedigree of specified size.\"\"\"\n    pedigree = {}\n    next_id = 1\n    \n    # Create founding generation\n    founding_size = max(size // (2 ** max_generations), 10)\n    founders = list(range(next_id, next_id + founding_size))\n    next_id += founding_size\n    \n    # Add founders to pedigree\n    for founder_id in founders:\n        pedigree[founder_id] = {}\n    \n    # Track available parents for each generation\n    available_parents = founders\n    \n    # Create subsequent generations\n    remaining = size - founding_size\n    while remaining > 0 and available_parents:\n        next_generation = []\n        \n        # Randomly pair parents to create children\n        import random\n        random.shuffle(available_parents)\n        \n        for i in range(0, len(available_parents) - 1, 2):\n            parent1 = available_parents[i]\n            parent2 = available_parents[i + 1]\n            \n            # Number of children for this couple (1-3)\n            num_children = min(remaining, random.randint(1, 3))\n            \n            for _ in range(num_children):\n                child_id = next_id\n                next_id += 1\n                \n                # Add child to pedigree with parents\n                pedigree[child_id] = {parent1: 1, parent2: 1}\n                \n                next_generation.append(child_id)\n                remaining -= 1\n                \n                if remaining <= 0:\n                    break\n            \n            if remaining <= 0:\n                break\n        \n        # Next generation becomes available parents\n        available_parents = next_generation\n    \n    return pedigree\n\n# Test with a small pedigree\ntest_pedigree = {\n    1000: {1001: 1, 1002: 1},  # 1000 has parents 1001 and 1002\n    1003: {1001: 1, 1002: 1},  # 1003 has the same parents (sibling of 1000)\n    1004: {1000: 1, 1005: 1},  # 1004 has parents 1000 and 1005\n    1006: {1003: 1, 1007: 1},  # 1006 has parents 1003 and 1007\n    1008: {1004: 1, 1006: 1},  # 1008 has parents 1004 and 1006 (consanguineous)\n    1001: {},\n    1002: {},\n    1005: {},\n    1007: {}\n}\n\n# Initialize calculators\nsparse_calc = SparseRelationshipCalculator(test_pedigree)\norig_calc = RelationshipCalculator(test_pedigree)\n\n# Compare some relationship coefficients\nprint(\"Testing relationship coefficients:\")\ntest_pairs = [\n    (1000, 1003),  # Full siblings\n    (1000, 1004),  # Parent-child\n    (1000, 1006),  # Aunt/uncle-niece/nephew\n    (1004, 1006),  # First cousins\n    (1004, 1008),  # Parent-child\n    (1006, 1008)   # Parent-child\n]\n\nfor id1, id2 in test_pairs:\n    sparse_coef = sparse_calc.get_relationship_coefficient(id1, id2)\n    orig_coef = orig_calc.get_coefficient(id1, id2)\n    \n    relationship = \"Unknown\"\n    if abs(sparse_coef - 0.5) < 0.01:\n        relationship = \"Parent-Child or Full Sibling\"\n    elif abs(sparse_coef - 0.25) < 0.01:\n        relationship = \"Half Sibling, Grandparent, or Avuncular\"\n    elif abs(sparse_coef - 0.125) < 0.01:\n        relationship = \"First Cousin\"\n    \n    print(f\"Relationship {id1}-{id2}: Sparse={sparse_coef:.4f}, Original={orig_coef:.4f}, Type={relationship}\")\n\n# Test performance on larger pedigrees (uncomment to run)\n# results = benchmark_relationship_calculators(1000, 3, 5)\n# print(\"\\nBenchmark Results:\")\n# print(f\"Pedigree size: {results['pedigree_size']}\")\n# print(\"\\nSparse Calculator:\")\n# print(f\"  Avg. Init Time: {results['sparse']['init_time_avg']:.4f} seconds\")\n# print(f\"  Avg. Matrix Build Time: {results['sparse']['matrix_build_time_avg']:.4f} seconds\")\n# print(f\"  Avg. Coefficient Calc Time: {results['sparse']['coefficient_time_avg']:.4f} seconds\")\n# print(f\"  Avg. Memory Usage: {results['sparse']['memory_estimate_avg'] / (1024*1024):.2f} MB\")\n# print(\"\\nOriginal Calculator:\")\n# print(f\"  Avg. Init Time: {results['original']['init_time_avg']:.4f} seconds\")\n# print(f\"  Avg. Coefficient Calc Time: {results['original']['coefficient_time_avg']:.4f} seconds\")\n# print(f\"  Avg. Memory Usage: {results['original']['memory_estimate_avg'] / (1024*1024):.2f} MB\")\n\n# Show memory efficiency comparison\nprint(\"\\nMemory Efficiency:\")\nsparse_calc.compute_kinship_matrix()  # Ensure matrix is computed\nsparse_nnz = sparse_calc.kinship_matrix.nnz\nsparse_size = sparse_calc.n_individuals\nsparse_density = sparse_nnz / (sparse_size * sparse_size)\n\nprint(f\"Sparse matrix size: {sparse_size}x{sparse_size}\")\nprint(f\"Non-zero elements: {sparse_nnz}\")\nprint(f\"Matrix density: {sparse_density:.6f}\")\nprint(f\"Memory savings vs. dense: {(1.0 - sparse_density) * 100:.2f}%\")\n\n# Find most related pairs\nmost_related = sparse_calc.get_most_related_pairs(5)\nprint(\"\\nMost closely related pairs:\")\nfor id1, id2, coef in most_related:\n    relationship = \"Unknown\"\n    if abs(coef - 0.25) < 0.01:\n        relationship = \"Full Siblings or Parent-Child\"\n    elif abs(coef - 0.125) < 0.01:\n        relationship = \"Half Sibling, Grandparent, or Avuncular\" \n    elif abs(coef - 0.0625) < 0.01:\n        relationship = \"First Cousin\"\n        \n    print(f\"{id1}-{id2}: Coefficient={coef:.4f}, Type={relationship}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Tip:** When designing data structures for pedigree reconstruction, always consider the trade-offs between memory usage, computational complexity, and biological accuracy. For large datasets, efficient data structures can make the difference between a reconstruction that completes in minutes versus one that takes days."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Conclusion\n\nIn this lab, we explored the data structures and algorithmic design that power the Bonsai v3 pedigree reconstruction algorithm. We implemented key components such as the IBD segment processing, up-node dictionary, pairwise likelihood calculations, and connection point finding.\n\nKey takeaways:\n- Bonsai v3 introduces a more modular approach with improved support for different IBD data formats\n- The algorithm can work with both phased and unphased IBD segments\n- The PwLogLike class provides a structured approach to calculating likelihoods\n- Connecting pedigrees is handled through a flexible connection point mechanism\n- The algorithm builds pedigrees incrementally, prioritizing high-confidence relationships\n\nThese improvements in v3 make Bonsai more adaptable, efficient, and scalable for complex genealogical analyses.\n\nIn the next lab, we will explore calibration techniques to ensure that Bonsai's likelihood calculations accurately reflect real-world genetic inheritance patterns.",
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}