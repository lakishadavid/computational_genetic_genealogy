{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Lab 22: Interpreting Results and Confidence Measures\n\n## Overview\n\nIn this lab, we'll explore how Bonsai v3 interprets genetic data to make relationship predictions and assigns confidence levels to these predictions. We'll examine the statistical models, likelihood calculations, and confidence measures that help users understand the reliability of inferred relationships and pedigree structures.",
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# üß¨ Google Colab Setup - Run this cell first!\n",
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import networkx as nx\n",
    "from IPython.display import display, HTML, Markdown\n",
    "\n",
    "def is_colab():\n",
    "    '''Check if running in Google Colab'''\n",
    "    try:\n",
    "        import google.colab\n",
    "        return True\n",
    "    except ImportError:\n",
    "        return False\n",
    "\n",
    "if is_colab():\n",
    "    print(\"üî¨ Setting up Google Colab environment...\")\n",
    "    \n",
    "    # Install dependencies\n",
    "    print(\"üì¶ Installing packages...\")\n",
    "    !pip install -q pysam biopython scikit-allel networkx pygraphviz seaborn plotly\n",
    "    !apt-get update -qq && apt-get install -qq samtools bcftools tabix graphviz-dev\n",
    "    \n",
    "    # Create directories\n",
    "    !mkdir -p /content/class_data /content/results\n",
    "    \n",
    "    # Download essential class data\n",
    "    print(\"üì• Downloading class data...\")\n",
    "    S3_BASE = \"https://computational-genetic-genealogy.s3.us-east-2.amazonaws.com/class_data/\"\n",
    "    data_files = [\n",
    "        \"pedigree.fam\", \"pedigree.def\", \n",
    "        \"merged_opensnps_autosomes_ped_sim.seg\",\n",
    "        \"merged_opensnps_autosomes_ped_sim-everyone.fam\",\n",
    "        \"ped_sim_run2.seg\", \"ped_sim_run2-everyone.fam\"\n",
    "    ]\n",
    "    \n",
    "    for file in data_files:\n",
    "        !wget -q -O /content/class_data/{file} {S3_BASE}{file}\n",
    "        print(f\"  ‚úÖ {file}\")\n",
    "    \n",
    "    # Define utility functions\n",
    "    def setup_environment():\n",
    "        return \"/content/class_data\", \"/content/results\"\n",
    "    \n",
    "    def save_results(dataframe, filename, description=\"results\"):\n",
    "        os.makedirs(\"/content/results\", exist_ok=True)\n",
    "        full_path = f\"/content/results/{filename}\"\n",
    "        dataframe.to_csv(full_path, index=False)\n",
    "        display(HTML(f'''\n",
    "        <div style=\"padding: 10px; background-color: #e3f2fd; border-left: 4px solid #2196f3; margin: 10px 0;\">\n",
    "            <p><strong>üíæ Results saved!</strong> To download: \n",
    "            <code>from google.colab import files; files.download('{full_path}')</code></p>\n",
    "        </div>\n",
    "        '''))\n",
    "        return full_path\n",
    "    \n",
    "    def save_plot(plt, filename, description=\"plot\"):\n",
    "        os.makedirs(\"/content/results\", exist_ok=True)\n",
    "        full_path = f\"/content/results/{filename}\"\n",
    "        plt.savefig(full_path, dpi=300, bbox_inches='tight')\n",
    "        plt.show()\n",
    "        display(HTML(f'''\n",
    "        <div style=\"padding: 10px; background-color: #e8f5e8; border-left: 4px solid #4caf50; margin: 10px 0;\">\n",
    "            <p><strong>üìä Plot saved!</strong> To download: \n",
    "            <code>from google.colab import files; files.download('{full_path}')</code></p>\n",
    "        </div>\n",
    "        '''))\n",
    "        return full_path\n",
    "    \n",
    "    print(\"‚úÖ Colab setup complete! Ready to explore genetic genealogy.\")\n",
    "    \n",
    "else:\n",
    "    print(\"üè† Local environment detected\")\n",
    "    def setup_environment():\n",
    "        return \"class_data\", \"results\"\n",
    "    def save_results(df, filename, description=\"\"):\n",
    "        os.makedirs(\"results\", exist_ok=True)\n",
    "        path = f\"results/{filename}\"\n",
    "        df.to_csv(path, index=False)\n",
    "        return path\n",
    "    def save_plot(plt, filename, description=\"\"):\n",
    "        os.makedirs(\"results\", exist_ok=True)\n",
    "        path = f\"results/{filename}\"\n",
    "        plt.savefig(path, dpi=300, bbox_inches='tight')\n",
    "        plt.show()\n",
    "        return path\n",
    "\n",
    "# Set up paths and configure visualization\n",
    "DATA_DIR, RESULTS_DIR = setup_environment()\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "sns.set_context(\"notebook\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup Bonsai module paths\n",
    "if not is_jupyterlite():\n",
    "    # In local environment, add the utils directory to system path\n",
    "    utils_dir = os.getenv('PROJECT_UTILS_DIR', os.path.join(os.path.dirname(DATA_DIR), 'utils'))\n",
    "    bonsaitree_dir = os.path.join(utils_dir, 'bonsaitree')\n",
    "    \n",
    "    # Add to path if it exists and isn't already there\n",
    "    if os.path.exists(bonsaitree_dir) and bonsaitree_dir not in sys.path:\n",
    "        sys.path.append(bonsaitree_dir)\n",
    "        print(f\"Added {bonsaitree_dir} to sys.path\")\n",
    "else:\n",
    "    # In JupyterLite, use a simplified approach\n",
    "    print(\"‚ö†Ô∏è Running in JupyterLite: Some Bonsai functionality may be limited.\")\n",
    "    print(\"This notebook is primarily designed for local execution where the Bonsai codebase is available.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions for exploring modules\n",
    "def display_module_classes(module_name):\n",
    "    \"\"\"Display classes and their docstrings from a module\"\"\"\n",
    "    try:\n",
    "        # Import the module\n",
    "        module = importlib.import_module(module_name)\n",
    "        \n",
    "        # Find all classes\n",
    "        classes = inspect.getmembers(module, inspect.isclass)\n",
    "        \n",
    "        # Filter classes defined in this module (not imported)\n",
    "        classes = [(name, cls) for name, cls in classes if cls.__module__ == module_name]\n",
    "        \n",
    "        # Print info for each class\n",
    "        for name, cls in classes:\n",
    "            print(f\"\\n## {name}\")\n",
    "            \n",
    "            # Get docstring\n",
    "            doc = inspect.getdoc(cls)\n",
    "            if doc:\n",
    "                print(f\"Docstring: {doc}\")\n",
    "            else:\n",
    "                print(\"No docstring available\")\n",
    "            \n",
    "            # Get methods\n",
    "            methods = inspect.getmembers(cls, inspect.isfunction)\n",
    "            if methods:\n",
    "                print(\"\\nMethods:\")\n",
    "                for method_name, method in methods:\n",
    "                    if not method_name.startswith('_'):  # Skip private methods\n",
    "                        print(f\"- {method_name}\")\n",
    "    except ImportError as e:\n",
    "        print(f\"Error importing module {module_name}: {e}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing module {module_name}: {e}\")\n",
    "\n",
    "def display_module_functions(module_name):\n",
    "    \"\"\"Display functions and their docstrings from a module\"\"\"\n",
    "    try:\n",
    "        # Import the module\n",
    "        module = importlib.import_module(module_name)\n",
    "        \n",
    "        # Find all functions\n",
    "        functions = inspect.getmembers(module, inspect.isfunction)\n",
    "        \n",
    "        # Filter functions defined in this module (not imported)\n",
    "        functions = [(name, func) for name, func in functions if func.__module__ == module_name]\n",
    "        \n",
    "        # Print info for each function\n",
    "        for name, func in functions:\n",
    "            if name.startswith('_'):  # Skip private functions\n",
    "                continue\n",
    "                \n",
    "            print(f\"\\n## {name}\")\n",
    "            \n",
    "            # Get signature\n",
    "            sig = inspect.signature(func)\n",
    "            print(f\"Signature: {name}{sig}\")\n",
    "            \n",
    "            # Get docstring\n",
    "            doc = inspect.getdoc(func)\n",
    "            if doc:\n",
    "                print(f\"Docstring: {doc}\")\n",
    "            else:\n",
    "                print(\"No docstring available\")\n",
    "    except ImportError as e:\n",
    "        print(f\"Error importing module {module_name}: {e}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing module {module_name}: {e}\")\n",
    "\n",
    "def view_function_source(module_name, function_name):\n",
    "    \"\"\"Display the source code of a function\"\"\"\n",
    "    try:\n",
    "        # Import the module\n",
    "        module = importlib.import_module(module_name)\n",
    "        \n",
    "        # Get the function\n",
    "        func = getattr(module, function_name)\n",
    "        \n",
    "        # Get the source code\n",
    "        source = inspect.getsource(func)\n",
    "        \n",
    "        # Print the source code\n",
    "        from IPython.display import display, Markdown\n",
    "        display(Markdown(f\"```python\\n{source}\\n```\"))\n",
    "    except ImportError as e:\n",
    "        print(f\"Error importing module {module_name}: {e}\")\n",
    "    except AttributeError:\n",
    "        print(f\"Function {function_name} not found in module {module_name}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing function {function_name}: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check Bonsai Installation\n",
    "\n",
    "Let's verify that the Bonsai v3 module is available for import:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    from utils.bonsaitree.bonsaitree import v3\n",
    "    print(\"‚úÖ Successfully imported Bonsai v3 module\")\n",
    "except ImportError as e:\n",
    "    print(f\"‚ùå Failed to import Bonsai v3 module: {e}\")\n",
    "    print(\"This lab requires access to the Bonsai v3 codebase.\")\n",
    "    print(\"Make sure you've properly set up your environment with the Bonsai repository.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Lab 22: Interpreting Results and Confidence Measures\n\nGenetic genealogy involves making inferences about relationships based on DNA data. However, these inferences come with varying degrees of certainty. Understanding the confidence levels and statistical significance of relationship predictions is crucial for proper interpretation of results.\n\nIn this lab, we'll examine how Bonsai v3 computes confidence measures and helps users interpret the reliability of predictions. We'll explore:\n\n1. **Likelihood-Based Inference**: How Bonsai uses likelihood functions to assess the probability of different relationship types\n2. **Confidence Intervals**: Methods for calculating confidence bounds on relationship degree estimates\n3. **Age-Based Constraints**: How age information adds constraints and confidence to relationship predictions\n4. **Multiple Hypotheses Testing**: Comparing alternative relationship hypotheses\n5. **Visualizing Uncertainty**: Techniques for communicating confidence levels to users\n\nThroughout this lab, we'll utilize the actual functions and methods from the Bonsai v3 codebase that handle confidence measures and result interpretation.",
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Part 1: Likelihood-Based Inference\n\nAt the core of Bonsai's relationship prediction is the computation of likelihoods - statistical measures of how well the observed data fits different relationship hypotheses. Let's explore how Bonsai calculates and interprets these likelihoods:",
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": "# Import the necessary Bonsai modules\ntry:\n    from utils.bonsaitree.bonsaitree.v3 import likelihoods\n    from utils.bonsaitree.bonsaitree.v3 import pedigrees\n    from utils.bonsaitree.bonsaitree.v3 import ibd\n    from utils.bonsaitree.bonsaitree.v3 import druid\n    from utils.bonsaitree.bonsaitree.v3.constants import GENOME_LENGTH\n\n    print(\"‚úÖ Successfully imported Bonsai v3 modules\")\n    \n    # Examine the PwLogLike class for computing relationship likelihoods\n    if hasattr(likelihoods, 'PwLogLike'):\n        print(\"\\nExamining PwLogLike class from likelihoods.py:\")\n        print(f\"Documentation: {likelihoods.PwLogLike.__doc__}\")\n        \n    # Examine DRUID's degree inference function\n    if hasattr(druid, 'infer_degree_generalized_druid'):\n        print(\"\\nExamining DRUID inference function:\")\n        print(f\"Documentation: {druid.infer_degree_generalized_druid.__doc__}\")\n        \n    # Examine confidence interval function\n    if hasattr(likelihoods, 'get_total_ibd_deg_lbd_pt_ubd'):\n        print(\"\\nExamining confidence interval function:\")\n        print(f\"Documentation: {likelihoods.get_total_ibd_deg_lbd_pt_ubd.__doc__}\")\n    \nexcept ImportError as e:\n    print(f\"‚ùå Failed to import Bonsai v3 modules: {e}\")\n    print(\"This lab requires access to the Bonsai v3 codebase.\")\n    print(\"Please ensure the Bonsai repository is properly set up in your environment.\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### 1.1 Understanding Likelihood Functions\n\nLikelihood functions in Bonsai calculate how probable the observed genetic data is under different relationship hypotheses. Let's examine some key functions used for likelihood-based inference:",
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": "# Examine the functions for calculating the log likelihood of IBD data\ntry:\n    # Check if the functions exist\n    if hasattr(likelihoods, 'get_log_total_ibd_pdf'):\n        print(\"Examining get_log_total_ibd_pdf function:\")\n        view_source(likelihoods.get_log_total_ibd_pdf)\n    \n    if hasattr(likelihoods, 'get_total_ibd_deg_lbd_pt_ubd'):\n        print(\"\\nExamining get_total_ibd_deg_lbd_pt_ubd function:\")\n        view_source(likelihoods.get_total_ibd_deg_lbd_pt_ubd)\n\n    # Look for functions in point_predictor module\n    if hasattr(point_predictor, 'point_predictions'):\n        print(\"\\nExamining point_predictions function:\")\n        view_source(point_predictor.point_predictions)\n        \n    # If we have the DRUID module available (for inference)\n    try:\n        from utils.bonsaitree.bonsaitree.v3 import druid\n        print(\"\\nExamining DRUID inference functions:\")\n        if hasattr(druid, 'infer_degree_generalized_druid'):\n            view_source(druid.infer_degree_generalized_druid)\n    except ImportError:\n        print(\"\\nDRUID module not available for examination\")\n        \nexcept Exception as e:\n    print(f\"Error examining functions: {e}\")\n\n# Let's create a simplified explanation of how likelihood functions work\nprint(\"\\n--- Simplified Explanation of Likelihood Functions ---\")\nprint(\"\"\"\nBonsai's likelihood functions calculate how well observed IBD data fits different relationship hypotheses:\n\n1. For each possible relationship (parent-child, siblings, cousins, etc.):\n   - Calculate the expected distribution of IBD sharing\n   - Compare the observed IBD sharing to this distribution\n   - Compute a likelihood score (higher = better fit)\n\n2. The log-likelihood (LL) is used instead of raw likelihood for numerical stability:\n   - LL(relationship | data) = log(P(data | relationship))\n   - Higher log-likelihood indicates better fit\n\n3. For complex pedigrees:\n   - The total likelihood is computed from all pairwise relationships\n   - Pedigree structures with higher overall likelihood are preferred\n   \n4. Confidence measures:\n   - Relative likelihood differences indicate confidence\n   - Larger differences between best and second-best hypothesis = higher confidence\n\"\"\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### 1.2 Working with Likelihood Scores\n\nNow let's create a practical example to demonstrate how to work with likelihood scores and interpret them. We'll simulate some IBD data for different relationships and calculate the likelihoods:",
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": "import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport random\nfrom scipy import stats\nimport math\n\n# Set random seed for reproducibility\nnp.random.seed(42)\nrandom.seed(42)\n\n# Function to simulate IBD data for different relationships\ndef simulate_ibd_data(relationship_type, num_segments=None, add_noise=True):\n    \"\"\"\n    Simulate IBD data for different relationship types.\n    \n    Args:\n        relationship_type: Relationship type (parent-child, siblings, cousins, etc.)\n        num_segments: Number of segments to simulate (if None, uses reasonable default)\n        add_noise: Whether to add random noise to make it more realistic\n        \n    Returns:\n        Dictionary with IBD data including total_cm and segments\n    \"\"\"\n    # Set expected total cM sharing for common relationships (based on empirical averages)\n    relationship_params = {\n        'parent-child': {'mean_cm': 3400, 'std_cm': 100, 'default_segments': 23},\n        'full-siblings': {'mean_cm': 2550, 'std_cm': 180, 'default_segments': 40},\n        'half-siblings': {'mean_cm': 1700, 'std_cm': 160, 'default_segments': 30},\n        '1st-cousins': {'mean_cm': 850, 'std_cm': 120, 'default_segments': 20},\n        '2nd-cousins': {'mean_cm': 212, 'std_cm': 70, 'default_segments': 10},\n        '3rd-cousins': {'mean_cm': 53, 'std_cm': 30, 'default_segments': 5},\n        'unrelated': {'mean_cm': 10, 'std_cm': 10, 'default_segments': 1}\n    }\n    \n    # Check if relationship type is valid\n    if relationship_type not in relationship_params:\n        raise ValueError(f\"Unknown relationship type: {relationship_type}\")\n    \n    # Get parameters for the relationship\n    params = relationship_params[relationship_type]\n    \n    # Determine number of segments\n    if num_segments is None:\n        num_segments = params['default_segments']\n    \n    # Generate total cM with noise if requested\n    if add_noise:\n        total_cm = np.random.normal(params['mean_cm'], params['std_cm'])\n        total_cm = max(0, total_cm)  # Ensure non-negative\n    else:\n        total_cm = params['mean_cm']\n    \n    # Generate segments\n    segments = []\n    remaining_cm = total_cm\n    \n    for i in range(num_segments - 1):\n        # Each segment gets a random proportion of the remaining cM\n        segment_cm = remaining_cm * random.random() * 0.3  # Up to 30% of remaining\n        \n        # Only include segments >= 7 cM (typical minimum threshold)\n        if segment_cm >= 7:\n            segments.append({\n                'chromosome': str(random.randint(1, 22)),  # Random chromosome\n                'start_pos': random.randint(1000000, 100000000),  # Random start position\n                'end_pos': 0,  # Will be set below\n                'cm': segment_cm,\n                'snps': int(segment_cm * 70)  # Approximate SNP count\n            })\n            \n            # Set end position based on cM length (very rough approximation)\n            segments[-1]['end_pos'] = segments[-1]['start_pos'] + int(segment_cm * 1000000)\n        \n        remaining_cm -= segment_cm\n    \n    # Add the last segment with remaining cM\n    if remaining_cm >= 7:\n        segments.append({\n            'chromosome': str(random.randint(1, 22)),\n            'start_pos': random.randint(1000000, 100000000),\n            'end_pos': 0,\n            'cm': remaining_cm,\n            'snps': int(remaining_cm * 70)\n        })\n        segments[-1]['end_pos'] = segments[-1]['start_pos'] + int(remaining_cm * 1000000)\n    \n    # Calculate actual total cM (might differ from target due to thresholds)\n    actual_total_cm = sum(segment['cm'] for segment in segments)\n    \n    return {\n        'total_cm': actual_total_cm,\n        'num_segments': len(segments),\n        'segments': segments,\n        'true_relationship': relationship_type\n    }\n\n# Create simulated data for different relationships\nrelationships = ['parent-child', 'full-siblings', 'half-siblings', \n                '1st-cousins', '2nd-cousins', '3rd-cousins', 'unrelated']\n\nsimulated_data = {}\nfor rel in relationships:\n    # Create 5 samples for each relationship type\n    simulated_data[rel] = [simulate_ibd_data(rel) for _ in range(5)]\n\n# Display summary of the simulated data\nprint(\"Simulated IBD Data Summary:\")\nprint(\"-\" * 70)\nprint(f\"{'Relationship':<15} {'Total cM (Mean ¬± SD)':<25} {'Segments (Mean ¬± SD)':<20}\")\nprint(\"-\" * 70)\n\nfor rel in relationships:\n    # Calculate statistics\n    total_cm_values = [data['total_cm'] for data in simulated_data[rel]]\n    segment_counts = [data['num_segments'] for data in simulated_data[rel]]\n    \n    mean_cm = np.mean(total_cm_values)\n    std_cm = np.std(total_cm_values)\n    mean_segments = np.mean(segment_counts)\n    std_segments = np.std(segment_counts)\n    \n    print(f\"{rel:<15} {mean_cm:>8.1f} ¬± {std_cm:<10.1f} {mean_segments:>8.1f} ¬± {std_segments:<8.1f}\")\n\n# Create visualization of the simulated data\nplt.figure(figsize=(12, 6))\n\n# Create a scatter plot of total cM vs number of segments\nfor rel in relationships:\n    x = [data['total_cm'] for data in simulated_data[rel]]\n    y = [data['num_segments'] for data in simulated_data[rel]]\n    plt.scatter(x, y, alpha=0.7, label=rel)\n\nplt.xlabel('Total cM Shared')\nplt.ylabel('Number of Segments')\nplt.title('IBD Sharing by Relationship Type')\nplt.legend()\nplt.grid(True, alpha=0.3)\nplt.tight_layout()\nplt.show()\n\n# Create a simplified likelihood calculation function \ndef calculate_likelihood_scores(ibd_data):\n    \"\"\"\n    Calculate simplified likelihood scores for different relationship hypotheses.\n    \n    This is a simplified implementation to demonstrate the concept when Bonsai's\n    actual implementation is not available.\n    \n    Args:\n        ibd_data: Dictionary with IBD data including total_cm and num_segments\n        \n    Returns:\n        Dictionary with log-likelihood scores for each relationship type\n    \"\"\"\n    # Expected values for different relationships\n    relationship_params = {\n        'parent-child': {'mean_cm': 3400, 'std_cm': 100},\n        'full-siblings': {'mean_cm': 2550, 'std_cm': 180},\n        'half-siblings': {'mean_cm': 1700, 'std_cm': 160},\n        '1st-cousins': {'mean_cm': 850, 'std_cm': 120},\n        '2nd-cousins': {'mean_cm': 212, 'std_cm': 70},\n        '3rd-cousins': {'mean_cm': 53, 'std_cm': 30},\n        'unrelated': {'mean_cm': 10, 'std_cm': 10}\n    }\n    \n    # Get the observed total cM\n    total_cm = ibd_data['total_cm']\n    \n    # Calculate log-likelihood for each relationship\n    log_likelihoods = {}\n    \n    for rel, params in relationship_params.items():\n        # Calculate log-likelihood using normal distribution as a simplified model\n        # Real Bonsai uses more sophisticated models\n        log_like = stats.norm.logpdf(total_cm, loc=params['mean_cm'], scale=params['std_cm'])\n        \n        # Store the log-likelihood\n        log_likelihoods[rel] = log_like\n    \n    return log_likelihoods\n\n# Calculate normalized likelihoods (for comparing relationships)\ndef normalize_log_likelihoods(log_likelihoods):\n    \"\"\"\n    Normalize log-likelihoods to get posterior probabilities.\n    This assumes equal priors for all relationships.\n    \n    Args:\n        log_likelihoods: Dictionary with log-likelihood values\n        \n    Returns:\n        Dictionary with normalized probabilities\n    \"\"\"\n    # Convert to linear space\n    linear_likelihoods = {rel: math.exp(ll) for rel, ll in log_likelihoods.items()}\n    \n    # Calculate sum for normalization\n    total = sum(linear_likelihoods.values())\n    \n    # Normalize\n    if total > 0:\n        normalized = {rel: val/total for rel, val in linear_likelihoods.items()}\n    else:\n        # Handle numerical underflow\n        max_ll = max(log_likelihoods.values())\n        adjusted_ll = {rel: ll - max_ll for rel, ll in log_likelihoods.items()}\n        linear_adj = {rel: math.exp(ll) for rel, ll in adjusted_ll.items()}\n        total_adj = sum(linear_adj.values())\n        normalized = {rel: val/total_adj for rel, val in linear_adj.items()}\n    \n    return normalized\n\n# Demonstrate likelihood calculation for an example\nexample_relationship = 'half-siblings'\nexample_data = simulated_data[example_relationship][0]  # Take the first example\n\nprint(f\"\\nLikelihood Analysis for a {example_relationship} relationship:\")\nprint(f\"Total cM: {example_data['total_cm']:.1f}\")\nprint(f\"Segments: {example_data['num_segments']}\")\n\n# Calculate log-likelihoods\nlog_likelihoods = calculate_likelihood_scores(example_data)\n\n# Sort by likelihood (highest first)\nsorted_ll = sorted(log_likelihoods.items(), key=lambda x: x[1], reverse=True)\n\nprint(\"\\nLog-Likelihood Scores:\")\nfor rel, ll in sorted_ll:\n    print(f\"{rel:<15}: {ll:.2f}\")\n\n# Calculate normalized probabilities\nnormalized = normalize_log_likelihoods(log_likelihoods)\nsorted_normalized = sorted(normalized.items(), key=lambda x: x[1], reverse=True)\n\nprint(\"\\nNormalized Probabilities:\")\nfor rel, prob in sorted_normalized:\n    print(f\"{rel:<15}: {prob:.4f} ({prob*100:.1f}%)\")\n\n# Calculate confidence as the ratio between top two probabilities\nif len(sorted_normalized) >= 2:\n    top_rel, top_prob = sorted_normalized[0]\n    second_rel, second_prob = sorted_normalized[1]\n    confidence_ratio = top_prob / second_prob if second_prob > 0 else float('inf')\n    \n    print(f\"\\nConfidence Ratio: {confidence_ratio:.2f}\")\n    if confidence_ratio > 100:\n        confidence_level = \"Very High\"\n    elif confidence_ratio > 10:\n        confidence_level = \"High\"\n    elif confidence_ratio > 2:\n        confidence_level = \"Moderate\"\n    else:\n        confidence_level = \"Low\"\n    \n    print(f\"Confidence Level: {confidence_level}\")\n    print(f\"Top relationship is {confidence_ratio:.1f}x more likely than second-best hypothesis\")\n\n# Create a plot of the likelihoods\nplt.figure(figsize=(10, 6))\nrelationships = [rel for rel, _ in sorted_normalized]\nprobabilities = [prob for _, prob in sorted_normalized]\n\nplt.bar(relationships, probabilities, color='skyblue')\nplt.xlabel('Relationship Type')\nplt.ylabel('Probability')\nplt.title('Relationship Probability Distribution')\nplt.xticks(rotation=45)\nplt.grid(axis='y', alpha=0.3)\nplt.tight_layout()\nplt.show()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### 1.3 Using PwLogLike for Relationship Inference\n\nNow let's try to use Bonsai's actual `PwLogLike` class for relationship inference if it's available:",
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Use Bonsai's PwLogLike class for relationship inference\ntry:\n    from utils.bonsaitree.bonsaitree.v3.likelihoods import PwLogLike\n    from utils.bonsaitree.bonsaitree.v3.ibd import IBDSegment\n\n    print(\"‚úÖ Using Bonsai v3's actual PwLogLike class for relationship inference\")\n    \n    # First, convert our simulated IBD data to Bonsai's expected format for unphased IBD segments\n    # Format: [[id1, id2, chromosome, start_bp, end_bp, is_full_ibd, seg_len_cm],...]\n    def convert_to_bonsai_unphased_segments(ibd_data, id1=1, id2=2):\n        \"\"\"Convert simulated IBD data to Bonsai's unphased segment format\"\"\"\n        bonsai_segments = []\n        \n        for segment in ibd_data['segments']:\n            # Create unphased segment record\n            unphased_seg = [\n                id1,                      # id1 \n                id2,                      # id2\n                segment['chromosome'],    # chromosome\n                segment['start_pos'],     # start_bp\n                segment['end_pos'],       # end_bp\n                False,                    # is_full_ibd (defaults to False for simplicity)\n                segment['cm']             # seg_len_cm\n            ]\n            bonsai_segments.append(unphased_seg)\n                \n        return bonsai_segments\n    \n    # Create sample bioinfo for the two individuals\n    bio_info = [\n        {'genotype_id': 1, 'age': 50, 'sex': 'M'},\n        {'genotype_id': 2, 'age': 30, 'sex': 'F'}\n    ]\n    \n    # Convert example data to Bonsai format\n    unphased_segments = convert_to_bonsai_unphased_segments(example_data)\n    print(f\"Created {len(unphased_segments)} Bonsai unphased segments from example data\")\n    \n    # Create a PwLogLike object with our sample data\n    pw_log_like = PwLogLike(\n        bio_info=bio_info,\n        unphased_ibd_seg_list=unphased_segments\n    )\n    \n    # Define relationship tuples to test\n    relationships_to_test = {\n        'parent-child': (0, 1, 1),      # (up, down, num_ancestors)\n        'full-siblings': (1, 1, 2),\n        'half-siblings': (1, 1, 1),\n        '1st-cousins': (2, 2, 1),\n        '2nd-cousins': (3, 3, 1),\n        '3rd-cousins': (4, 4, 1),\n        'unrelated': None               # None indicates no relationship\n    }\n    \n    # Get log-likelihoods for each relationship\n    rel_scores = {}\n    for rel_name, rel_tuple in relationships_to_test.items():\n        try:\n            # Get genetic component of the likelihood\n            gen_ll = pw_log_like.get_pw_gen_ll(\n                node1=1, \n                node2=2, \n                rel_tuple=rel_tuple\n            )\n            \n            # Get age component of the likelihood\n            age_ll = pw_log_like.get_pw_age_ll(\n                node1=1,\n                node2=2,\n                rel_tuple=rel_tuple\n            )\n            \n            # Total log-likelihood is the sum of genetic and age components\n            total_ll = gen_ll + age_ll\n            rel_scores[rel_name] = total_ll\n        except Exception as e:\n            print(f\"Error calculating likelihood for {rel_name}: {e}\")\n            # Fall back to genetic component only\n            try:\n                gen_ll = pw_log_like.get_pw_gen_ll(\n                    node1=1, \n                    node2=2, \n                    rel_tuple=rel_tuple\n                )\n                rel_scores[rel_name] = gen_ll\n            except Exception as e:\n                print(f\"Error calculating genetic likelihood for {rel_name}: {e}\")\n                rel_scores[rel_name] = float('-inf')\n    \n    # Display the log-likelihood scores\n    print(\"\\nBonsai PwLogLike Scores:\")\n    for rel, score in sorted(rel_scores.items(), key=lambda x: x[1], reverse=True):\n        print(f\"{rel:<15}: {score:.2f}\")\n    \n    # Find most likely relationship\n    most_likely_rel = max(rel_scores.items(), key=lambda x: x[1])[0]\n    print(f\"\\nMost likely relationship: {most_likely_rel}\")\n    \n    # Calculate confidence measures\n    # Method 1: Use likelihood ratio between best and second-best hypothesis\n    sorted_scores = sorted(rel_scores.items(), key=lambda x: x[1], reverse=True)\n    if len(sorted_scores) >= 2:\n        top_rel, top_ll = sorted_scores[0]\n        second_rel, second_ll = sorted_scores[1]\n        # Convert log-likelihoods to likelihood ratio\n        ll_ratio = np.exp(top_ll - second_ll)\n        \n        # Method 2: Calculate degree confidence interval\n        if hasattr(likelihoods, 'get_total_ibd_deg_lbd_pt_ubd'):\n            try:\n                # Get relationship degree\n                rel_tuple = relationships_to_test[top_rel]\n                if rel_tuple is not None:\n                    # Use Bonsai's confidence interval function if the relationship has ancestors\n                    a = rel_tuple[2]  # number of ancestors\n                    total_cm = sum(segment['cm'] for segment in example_data['segments'])\n                    \n                    lbd, mlm, ubd = likelihoods.get_total_ibd_deg_lbd_pt_ubd(\n                        a=a,\n                        L=total_cm,\n                        condition=True  # Most IBD detectors condition on observing at least one segment\n                    )\n                    \n                    print(f\"\\nDegree confidence interval (95%):\")\n                    print(f\"Lower bound: {lbd}\")\n                    print(f\"Point estimate: {mlm}\")\n                    print(f\"Upper bound: {ubd}\")\n            except Exception as e:\n                print(f\"Error calculating confidence interval: {e}\")\n        \n        # Interpret confidence\n        print(f\"\\nConfidence measure (likelihood ratio):\")\n        print(f\"{top_rel} is {ll_ratio:.1f}x more likely than {second_rel}\")\n        \n        if ll_ratio > 100:\n            confidence_level = \"Very High\"\n        elif ll_ratio > 10:\n            confidence_level = \"High\"\n        elif ll_ratio > 2:\n            confidence_level = \"Moderate\"\n        else:\n            confidence_level = \"Low\"\n        \n        print(f\"Confidence level: {confidence_level}\")\n        \nexcept ImportError as e:\n    print(f\"‚ùå Failed to import Bonsai classes: {e}\")\n    print(\"Falling back to simplified implementation since Bonsai is not available\")\n    \n    # Falling back to simplified implementation, which we'll only run if Bonsai is unavailable\n    # Calculate simplified likelihood\n    simple_ll = calculate_likelihood_scores(example_data)\n    simple_normalized = normalize_log_likelihoods(simple_ll)\n    \n    # Display results\n    print(\"\\nResults using simplified model (only because Bonsai is unavailable):\")\n    for rel, prob in sorted(simple_normalized.items(), key=lambda x: x[1], reverse=True)[:3]:\n        print(f\"{rel:<15}: {prob:.4f} ({prob*100:.1f}%)\")",
   "metadata": {},
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Implementing confidence interval calculation using Bonsai's methods\ntry:\n    # First try to use Bonsai's actual implementation\n    import numpy as np\n    from utils.bonsaitree.bonsaitree.v3 import likelihoods\n    from utils.bonsaitree.bonsaitree.v3.constants import GENOME_LENGTH, DEG_CI_ALPHA\n    \n    print(\"‚úÖ Using Bonsai v3's methods for confidence interval calculation\")\n    \n    # Function to demonstrate Bonsai's confidence interval calculation\n    def calculate_bonsai_confidence_interval(L, a=1, condition=True, alpha=DEG_CI_ALPHA):\n        \"\"\"\n        Calculate a confidence interval for relationship degree using Bonsai's method.\n        \n        Args:\n            L: Total IBD length in cM\n            a: Number of common ancestors (default: 1)\n            condition: Whether to condition on observing at least one segment (default: True)\n            alpha: Confidence level (default: Bonsai's DEG_CI_ALPHA)\n            \n        Returns:\n            Dictionary with confidence interval information\n        \"\"\"\n        # Use Bonsai's actual function for confidence interval calculation\n        lbd, mlm, ubd = likelihoods.get_total_ibd_deg_lbd_pt_ubd(\n            a=a,\n            L=L,\n            condition=condition,\n            alpha=alpha\n        )\n        \n        return {\n            'lower_bound': lbd,\n            'point_estimate': mlm,\n            'upper_bound': ubd,\n            'confidence_level': 1-alpha\n        }\n    \n    # Use our example data from the previous section\n    total_cm = sum(segment['cm'] for segment in example_data['segments'])\n    print(f\"Total IBD: {total_cm:.1f} cM\")\n    \n    # Calculate confidence interval using Bonsai's method\n    rel_tuple = relationships_to_test.get(example_relationship)\n    if rel_tuple is not None:\n        a = rel_tuple[2]  # Number of common ancestors\n        \n        conf_interval = calculate_bonsai_confidence_interval(total_cm, a=a)\n        \n        # Display the results\n        print(f\"\\nRelationship Confidence Interval ({(1-DEG_CI_ALPHA)*100:.0f}%):\")\n        print(f\"Lower bound: {conf_interval['lower_bound']} meioses\")\n        print(f\"Point estimate: {conf_interval['point_estimate']} meioses\")\n        print(f\"Upper bound: {conf_interval['upper_bound']} meioses\")\n        \n        # Convert meioses to relationship degrees for common relationships\n        meioses_to_rel = {\n            1: \"parent-child\",\n            2: \"full-siblings or grandparent\",\n            3: \"avuncular or half-siblings\",\n            4: \"1st-cousins\",\n            5: \"1st-cousins-once-removed\",\n            6: \"2nd-cousins\",\n            8: \"3rd-cousins\",\n            10: \"4th-cousins\"\n        }\n        \n        # Print degree interpretation\n        print(\"\\nDegree interpretation:\")\n        point_estimate_rel = meioses_to_rel.get(int(conf_interval['point_estimate']), f\"{int(conf_interval['point_estimate'])}-degree\")\n        print(f\"Most likely: {point_estimate_rel}\")\n        \n        # List all plausible relationships within confidence interval\n        plausible_rels = []\n        for m in range(int(conf_interval['lower_bound']), int(conf_interval['upper_bound'])+1):\n            rel = meioses_to_rel.get(m, f\"{m}-degree\")\n            plausible_rels.append(rel)\n        \n        print(f\"Plausible relationships: {', '.join(plausible_rels)}\")\n        \n    # Visualize the confidence interval on a likelihood curve\n    # Try to use Bonsai's actual get_log_total_ibd_pdf function\n    if hasattr(likelihoods, 'get_log_total_ibd_pdf'):\n        plt.figure(figsize=(12, 6))\n        \n        # Calculate likelihoods over a range of meioses values\n        meioses_range = np.arange(1, 15)\n        a = rel_tuple[2] if rel_tuple is not None else 1\n        \n        # Get log-likelihoods using Bonsai's function\n        log_pdfs = likelihoods.get_log_total_ibd_pdf(\n            a=a,\n            m=meioses_range,\n            L=total_cm,\n            condition=True\n        )\n        \n        # Normalize the log PDFs to get the posterior distribution\n        log_pdfs_norm = log_pdfs - np.log(np.sum(np.exp(log_pdfs)))\n        # Convert to probabilities\n        probs = np.exp(log_pdfs_norm)\n        \n        # Plot the probability distribution\n        plt.plot(meioses_range, probs, 'o-', color='blue', markersize=8)\n        \n        # Highlight confidence interval\n        if rel_tuple is not None:\n            min_y = 0\n            max_y = max(probs) * 1.1\n            plt.fill_between([conf_interval['lower_bound'], conf_interval['upper_bound']], \n                            min_y, max_y, color='lightblue', alpha=0.3)\n            \n            # Annotate the plot\n            plt.text(conf_interval['point_estimate'], max(probs),\n                     f\"Point estimate: {conf_interval['point_estimate']:.1f}\",\n                     ha='center', va='bottom')\n        \n        # Add relationship labels\n        for m in meioses_range:\n            rel = meioses_to_rel.get(m, \"\")\n            if rel:\n                plt.annotate(rel, (m, probs[m-1]), \n                            xytext=(0, 10), textcoords='offset points',\n                            ha='center', rotation=45)\n        \n        plt.xlabel('Number of Meioses (m)')\n        plt.ylabel('Posterior Probability')\n        plt.title('Relationship Inference Confidence')\n        plt.grid(True, alpha=0.3)\n        plt.tight_layout()\n        plt.show()\n    \n    # Note on how Bonsai calculates confidence intervals\n    print(\"\\nHow Bonsai Calculates Confidence Intervals:\")\n    print(\"\"\"\nBonsai v3 uses several methods to calculate confidence intervals:\n\n1. Likelihood-based confidence intervals:\n   - Uses the highest posterior density (HPD) approach\n   - Includes all meiosis values that together contain (1-alpha)% of the \n     posterior probability mass\n   - The point estimate is the meiosis count with the highest likelihood\n\n2. Statistical properties:\n   - The width of the confidence interval reflects uncertainty in the estimate\n   - Wider intervals indicate more uncertainty\n   - Narrow intervals indicate confident predictions\n\n3. Implementation steps:\n   - Compute the posterior probability distribution over meiosis values\n   - Sort meiosis values by their posterior probability (highest first)\n   - Include meiosis values until the cumulative probability exceeds the threshold\n   - Find the minimum and maximum meiosis in the included set\n\"\"\")\n\nexcept (ImportError, AttributeError) as e:\n    print(f\"‚ùå Error using Bonsai's confidence interval methods: {e}\")\n    print(\"Implementing simplified confidence interval calculation\")\n    \n    # Only implement this simplified version if Bonsai's actual implementation is unavailable\n    import numpy as np\n    from scipy import stats\n    import matplotlib.pyplot as plt\n\n    # Function to calculate confidence intervals for relationship degree\n    def calculate_confidence_interval(log_likelihoods, confidence_level=0.95):\n        \"\"\"\n        Calculate a confidence interval for the relationship degree based on likelihood scores.\n        \n        This is a simplified implementation that uses the likelihood ratio test approach.\n        \n        Args:\n            log_likelihoods: Dictionary of log-likelihood scores for different relationships\n            confidence_level: Desired confidence level (e.g., 0.95 for 95% confidence)\n            \n        Returns:\n            Dictionary with confidence interval information\n        \"\"\"\n        # Map relationship types to numerical degrees for simplicity\n        degree_mapping = {\n            'parent-child': 1,\n            'full-siblings': 1,\n            'half-siblings': 1.5,\n            '1st-cousins': 2,\n            '2nd-cousins': 3,\n            '3rd-cousins': 4,\n            'unrelated': 10\n        }\n        \n        # Get the best relationship and its log-likelihood\n        best_rel = max(log_likelihoods.items(), key=lambda x: x[1])[0]\n        best_ll = log_likelihoods[best_rel]\n        \n        # Calculate critical value for likelihood ratio test\n        # For degree inference, a chi-square critical value is often used\n        # with 1 degree of freedom\n        critical_value = stats.chi2.ppf(confidence_level, 1)\n        \n        # Find relationships that are not significantly different from the best\n        plausible_rels = []\n        for rel, ll in log_likelihoods.items():\n            # Twice the log-likelihood difference is approximately chi-square distributed\n            lr_stat = 2 * (best_ll - ll)\n            \n            if lr_stat <= critical_value:\n                plausible_rels.append((rel, degree_mapping.get(rel, 0), ll))\n        \n        # Sort by degree\n        plausible_rels.sort(key=lambda x: x[1])\n        \n        # Extract min and max degrees\n        min_degree = min(degree for _, degree, _ in plausible_rels)\n        max_degree = max(degree for _, degree, _ in plausible_rels)\n        \n        return {\n            'best_relationship': best_rel,\n            'best_degree': degree_mapping.get(best_rel, 0),\n            'min_degree': min_degree,\n            'max_degree': max_degree,\n            'confidence_level': confidence_level,\n            'plausible_relationships': [rel for rel, _, _ in plausible_rels]\n        }\n\n    # Using our example data from the previous section\n    conf_interval = calculate_confidence_interval(log_likelihoods, confidence_level=0.95)\n\n    # Display the results\n    print(f\"Relationship Confidence Interval (95%):\")\n    print(f\"Best estimate: {conf_interval['best_relationship']} (Degree {conf_interval['best_degree']})\")\n    print(f\"Degree range: {conf_interval['min_degree']} to {conf_interval['max_degree']}\")\n    print(f\"Plausible relationships: {', '.join(conf_interval['plausible_relationships']}\")\n\n    # Visualize the confidence interval\n    plt.figure(figsize=(10, 6))\n\n    # Plot degree vs negative log-likelihood\n    degrees = []\n    neg_ll = []\n    rel_labels = []\n\n    for rel, ll in log_likelihoods.items():\n        if rel in degree_mapping:\n            degrees.append(degree_mapping[rel])\n            neg_ll.append(-ll)  # Negative log-likelihood\n            rel_labels.append(rel)\n\n    # Sort by degree\n    sorted_indices = np.argsort(degrees)\n    degrees = [degrees[i] for i in sorted_indices]\n    neg_ll = [neg_ll[i] for i in sorted_indices]\n    rel_labels = [rel_labels[i] for i in sorted_indices]\n\n    # Plot\n    plt.plot(degrees, neg_ll, 'o-', color='blue', markersize=8)\n\n    # Highlight confidence interval\n    min_y = min(neg_ll) - 5\n    max_y = max(neg_ll) + 5\n    plt.fill_between([conf_interval['min_degree'], conf_interval['max_degree']], \n                    min_y, max_y, color='lightblue', alpha=0.3)\n\n    # Add relationship labels\n    for i, txt in enumerate(rel_labels):\n        plt.annotate(txt, (degrees[i], neg_ll[i]), \n                    xytext=(5, 5), textcoords='offset points')\n\n    plt.xlabel('Relationship Degree')\n    plt.ylabel('Negative Log-Likelihood')\n    plt.title('Relationship Degree Confidence Interval (95%)')\n    plt.grid(True, alpha=0.3)\n    plt.tight_layout()\n    plt.show()",
   "metadata": {},
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Part 3: Age-Based Constraints in Relationship Inference\n\nAge information provides valuable constraints that can significantly improve relationship inference. Age differences between individuals impose logical constraints on possible relationships. For example:\n\n- Parent-child relationships require approximately 15-45 years age difference\n- Sibling relationships typically involve individuals with age differences less than 30 years\n- First cousin relationships typically involve similar ages, or age differences up to 30 years\n\nBonsai v3 can incorporate age information to refine likelihood calculations and improve confidence in relationship predictions. Let's explore how:",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Demonstrate how Bonsai uses age constraints for relationship inference\ntry:\n    # First try to use Bonsai's actual implementation\n    from utils.bonsaitree.bonsaitree.v3.likelihoods import get_age_log_like, get_age_mean_std_for_rel_tuple\n\n    print(\"‚úÖ Using Bonsai v3's actual age-based constraint functions\")\n    \n    # Create a function to demonstrate how Bonsai incorporates age information\n    def calculate_age_constrained_likelihoods_bonsai(genetic_log_likelihoods, id1_age, id2_age, relationships_to_test):\n        \"\"\"\n        Incorporate age information into relationship likelihoods using Bonsai's actual functions.\n        \n        Args:\n            genetic_log_likelihoods: Dictionary of log-likelihoods from genetic data\n            id1_age: Age of person 1\n            id2_age: Age of person 2\n            relationships_to_test: Dictionary mapping relationship names to relationship tuples\n            \n        Returns:\n            Dictionary of updated log-likelihoods\n        \"\"\"\n        updated_log_likelihoods = {}\n        \n        # For each relationship\n        for rel_name, rel_tuple in relationships_to_test.items():\n            if rel_name in genetic_log_likelihoods:\n                genetic_ll = genetic_log_likelihoods[rel_name]\n                \n                # Get age log-likelihood using Bonsai's function\n                age_ll = get_age_log_like(\n                    age1=id1_age,\n                    age2=id2_age,\n                    rel_tuple=rel_tuple\n                )\n                \n                # Add log probabilities (equivalent to multiplying in linear space)\n                updated_ll = genetic_ll + age_ll\n                updated_log_likelihoods[rel_name] = updated_ll\n        \n        return updated_log_likelihoods\n    \n    # Get the age distributions for different relationships from Bonsai\n    def plot_bonsai_age_distributions(relationships_to_test):\n        \"\"\"Plot age difference distributions for different relationships using Bonsai's model\"\"\"\n        plt.figure(figsize=(12, 6))\n        \n        age_diffs = range(-60, 60)\n        \n        for rel_name, rel_tuple in relationships_to_test.items():\n            if rel_tuple is not None:  # Skip 'unrelated'\n                # Get mean and standard deviation for the relationship\n                mean, std = get_age_mean_std_for_rel_tuple(rel_tuple)\n                \n                # Calculate probability for each age difference\n                probs = [np.exp(scipy.stats.norm.logpdf(diff, loc=mean, scale=std)) for diff in age_diffs]\n                \n                plt.plot(age_diffs, probs, label=rel_name)\n        \n        plt.xlabel('Age Difference (years)')\n        plt.ylabel('Probability Density')\n        plt.title('Bonsai Age Difference Distributions by Relationship Type')\n        plt.legend()\n        plt.grid(True, alpha=0.3)\n        plt.tight_layout()\n        plt.show()\n    \n    # Plot the age distributions using Bonsai's model\n    plot_bonsai_age_distributions(relationships_to_test)\n    \n    # Demonstrate how age information affects relationship predictions\n    # Create different scenarios with different age differences\n    age_scenarios = {\n        'scenario1': {'id1_age': 70, 'id2_age': 45, 'description': 'Typical parent-child age difference'},\n        'scenario2': {'id1_age': 45, 'id2_age': 42, 'description': 'Typical sibling age difference'},\n        'scenario3': {'id1_age': 45, 'id2_age': 20, 'description': 'Typical avuncular age difference'},\n        'scenario4': {'id1_age': 45, 'id2_age': 45, 'description': 'Same age (could be cousins or siblings)'}\n    }\n    \n    # Get genetic log-likelihoods from our previous example\n    genetic_ll = rel_scores\n    \n    # Create a table to compare results\n    results = []\n    \n    for scenario_name, scenario in age_scenarios.items():\n        # Get age-constrained likelihoods using Bonsai's function\n        age_constrained_ll = calculate_age_constrained_likelihoods_bonsai(\n            genetic_log_likelihoods=genetic_ll,\n            id1_age=scenario['id1_age'],\n            id2_age=scenario['id2_age'],\n            relationships_to_test=relationships_to_test\n        )\n        \n        # Find best relationship with genetic data only\n        best_rel_genetic = max(genetic_ll.items(), key=lambda x: x[1])[0]\n        \n        # Find best relationship with age constraints\n        best_rel_constrained = max(age_constrained_ll.items(), key=lambda x: x[1])[0]\n        \n        # Calculate likelihood ratios for top two relationships\n        # For genetic only\n        genetic_sorted = sorted(genetic_ll.items(), key=lambda x: x[1], reverse=True)\n        genetic_ratio = np.exp(genetic_sorted[0][1] - genetic_sorted[1][1]) if len(genetic_sorted) > 1 else float('inf')\n        \n        # For age constrained\n        constrained_sorted = sorted(age_constrained_ll.items(), key=lambda x: x[1], reverse=True)\n        constrained_ratio = np.exp(constrained_sorted[0][1] - constrained_sorted[1][1]) if len(constrained_sorted) > 1 else float('inf')\n        \n        # Store results\n        results.append({\n            'scenario': scenario_name,\n            'description': scenario['description'],\n            'age_diff': scenario['id1_age'] - scenario['id2_age'],\n            'genetic_best': best_rel_genetic,\n            'constrained_best': best_rel_constrained,\n            'genetic_ratio': genetic_ratio,\n            'constrained_ratio': constrained_ratio,\n        })\n    \n    # Display results as a table\n    result_df = pd.DataFrame(results)\n    print(\"\\nEffect of Age Difference on Relationship Prediction using Bonsai:\")\n    print(result_df[['scenario', 'description', 'age_diff', 'genetic_best', 'constrained_best', 'genetic_ratio', 'constrained_ratio']])\n    \n    # Create visualization of genetic vs age+genetic for one scenario\n    chosen_scenario = 'scenario1'  # Parent-child age difference\n    scenario = age_scenarios[chosen_scenario]\n    \n    # Calculate constrained likelihoods for this scenario\n    age_constrained_ll = calculate_age_constrained_likelihoods_bonsai(\n        genetic_log_likelihoods=genetic_ll,\n        id1_age=scenario['id1_age'],\n        id2_age=scenario['id2_age'],\n        relationships_to_test=relationships_to_test\n    )\n    \n    # Convert log-likelihoods to normalized probabilities\n    def normalize_log_likelihoods_bonsai(log_likelihoods):\n        \"\"\"Normalize log-likelihoods to get posterior probabilities\"\"\"\n        # Convert to linear space\n        linear_likelihoods = {rel: np.exp(ll) for rel, ll in log_likelihoods.items()}\n        \n        # Calculate sum for normalization\n        total = sum(linear_likelihoods.values())\n        \n        # Normalize\n        if total > 0:\n            normalized = {rel: val/total for rel, val in linear_likelihoods.items()}\n        else:\n            # Handle numerical underflow\n            max_ll = max(log_likelihoods.values())\n            adjusted_ll = {rel: ll - max_ll for rel, ll in log_likelihoods.items()}\n            linear_adj = {rel: np.exp(ll) for rel, ll in adjusted_ll.items()}\n            total_adj = sum(linear_adj.values())\n            normalized = {rel: val/total_adj for rel, val in linear_adj.items()}\n        \n        return normalized\n    \n    # Get normalized probabilities\n    genetic_norm = normalize_log_likelihoods_bonsai(genetic_ll)\n    age_constrained_norm = normalize_log_likelihoods_bonsai(age_constrained_ll)\n    \n    # Prepare for visualization\n    rel_order = ['parent-child', 'full-siblings', 'half-siblings', \n                '1st-cousins', '2nd-cousins', '3rd-cousins', 'unrelated']\n    \n    # Only include relationships that exist in both dictionaries\n    rel_order = [rel for rel in rel_order if rel in genetic_norm and rel in age_constrained_norm]\n    \n    genetic_probs = [genetic_norm.get(rel, 0) for rel in rel_order]\n    constrained_probs = [age_constrained_norm.get(rel, 0) for rel in rel_order]\n    \n    # Create bar chart comparison\n    plt.figure(figsize=(12, 6))\n    x = np.arange(len(rel_order))\n    width = 0.35\n    \n    plt.bar(x - width/2, genetic_probs, width, label='Genetic Data Only')\n    plt.bar(x + width/2, constrained_probs, width, label='Genetic + Age Data')\n    \n    plt.xlabel('Relationship Type')\n    plt.ylabel('Probability')\n    plt.title(f'Impact of Age Information (Age Diff: {scenario[\"id1_age\"] - scenario[\"id2_age\"]} years)')\n    plt.xticks(x, rel_order, rotation=45)\n    plt.legend()\n    plt.grid(axis='y', alpha=0.3)\n    plt.tight_layout()\n    plt.show()\n    \n    # Summary of how Bonsai uses age information\n    print(\"\\nHow Bonsai v3 Uses Age Information:\")\n    print(\"\"\"\nBonsai v3 systematically incorporates age information in several ways:\n\n1. Empirical Age Distribution Modeling:\n   - Maintains a database of age differences for each relationship type\n   - Models the distribution of age differences using normal distributions\n   - Parameters estimated from empirical data\n\n2. Age Likelihood Component:\n   - Log-likelihood component for age: log(P(age_diff | relationship))\n   - Combined with genetic component: total_ll = genetic_ll + age_ll\n   - Mathematically principled Bayesian approach\n\n3. Hard Constraints:\n   - Some relationships naturally imply constraints (e.g., parents must be older than children)\n   - Impossible age differences receive -infinity log-likelihood\n\n4. Pedigree Consistency:\n   - In complex pedigrees, ensures age consistency across the entire structure\n   - Identifies and penalizes age-inconsistent pedigrees\n\"\"\")\n    \nexcept (ImportError, AttributeError) as e:\n    print(f\"‚ùå Error using Bonsai's age constraint functions: {e}\")\n    print(\"Implementing simplified age-constraint calculation\")\n    \n    # Only implement the simplified version if Bonsai's implementation is unavailable\n    import numpy as np\n    import pandas as pd\n    import matplotlib.pyplot as plt\n    import random\n    from scipy import stats\n\n    # Define typical age difference distributions for various relationships\n    def get_age_difference_probability(relationship, age_diff):\n        \"\"\"\n        Calculate probability of a given age difference for a specific relationship.\n        \n        Args:\n            relationship: Relationship type\n            age_diff: Age difference in years (can be negative)\n        \n        Returns:\n            Probability density at the given age difference\n        \"\"\"\n        # Parameters for distributions based on typical age differences\n        rel_params = {\n            'parent-child': {'mean': 30, 'std': 5, 'min': 15, 'max': 50, 'direction': 1},\n            'full-siblings': {'mean': 0, 'std': 5, 'min': -30, 'max': 30, 'direction': 0},\n            'half-siblings': {'mean': 0, 'std': 7, 'min': -40, 'max': 40, 'direction': 0},\n            '1st-cousins': {'mean': 0, 'std': 8, 'min': -40, 'max': 40, 'direction': 0},\n            '2nd-cousins': {'mean': 0, 'std': 10, 'min': -50, 'max': 50, 'direction': 0},\n            '3rd-cousins': {'mean': 0, 'std': 12, 'min': -60, 'max': 60, 'direction': 0},\n            'unrelated': {'mean': 0, 'std': 20, 'min': -100, 'max': 100, 'direction': 0}\n        }\n        \n        # Check if relationship exists\n        if relationship not in rel_params:\n            return 0.0\n        \n        params = rel_params[relationship]\n        \n        # Apply directional constraint for parent-child\n        if params['direction'] == 1 and age_diff < 0:\n            return 0.0  # Parent must be older than child\n        \n        # Check if age difference is in allowed range\n        if age_diff < params['min'] or age_diff > params['max']:\n            return 0.0\n        \n        # Calculate probability using normal distribution\n        prob = stats.norm.pdf(age_diff, loc=params['mean'], scale=params['std'])\n        \n        # Normalize to account for truncation\n        norm_factor = stats.norm.cdf(params['max'], loc=params['mean'], scale=params['std']) - \\\n                    stats.norm.cdf(params['min'], loc=params['mean'], scale=params['std'])\n        \n        if norm_factor > 0:\n            prob = prob / norm_factor\n        \n        return prob\n\n    # Create a function to incorporate age information into relationship likelihoods\n    def calculate_age_constrained_likelihoods(genetic_log_likelihoods, age_diff):\n        \"\"\"\n        Incorporate age information into relationship likelihoods.\n        \n        Args:\n            genetic_log_likelihoods: Dictionary of log-likelihoods from genetic data\n            age_diff: Age difference in years (person1 age - person2 age)\n            \n        Returns:\n            Dictionary of updated log-likelihoods\n        \"\"\"\n        # Start with genetic log-likelihoods\n        updated_log_likelihoods = {}\n        \n        # For each relationship\n        for rel, genetic_ll in genetic_log_likelihoods.items():\n            # Get age probability\n            age_prob = get_age_difference_probability(rel, age_diff)\n            \n            # Convert to log space\n            if age_prob > 0:\n                age_log_prob = np.log(age_prob)\n                \n                # Add log probabilities (equivalent to multiplying in linear space)\n                updated_ll = genetic_ll + age_log_prob\n                updated_log_likelihoods[rel] = updated_ll\n            else:\n                # Age difference is impossible for this relationship\n                updated_log_likelihoods[rel] = float('-inf')  # Effectively zero probability\n        \n        return updated_log_likelihoods\n\n    # Create a function to visualize age distributions for different relationships\n    def plot_age_diff_distributions():\n        \"\"\"Plot age difference distributions for different relationships\"\"\"\n        relationships = ['parent-child', 'full-siblings', 'half-siblings', \n                        '1st-cousins', '2nd-cousins', '3rd-cousins']\n        \n        age_diffs = range(-60, 60)\n        \n        plt.figure(figsize=(12, 6))\n        \n        for rel in relationships:\n            probs = [get_age_difference_probability(rel, diff) for diff in age_diffs]\n            plt.plot(age_diffs, probs, label=rel)\n        \n        plt.xlabel('Age Difference (years)')\n        plt.ylabel('Probability Density')\n        plt.title('Age Difference Distributions by Relationship Type')\n        plt.legend()\n        plt.grid(True, alpha=0.3)\n        plt.tight_layout()\n        plt.show()\n\n    # Plot age difference distributions\n    plot_age_diff_distributions()\n\n    # Demonstrate how age information affects relationship predictions\n    # Example: Using our half-sibling example and varying age differences\n    example_genetic_ll = log_likelihoods  # From our previous example\n\n    # Try different age differences\n    age_differences = [-40, -20, 0, 20, 40]\n\n    # Create a table to compare results\n    results = []\n\n    for age_diff in age_differences:\n        # Calculate age-constrained likelihoods\n        age_constrained_ll = calculate_age_constrained_likelihoods(example_genetic_ll, age_diff)\n        \n        # Find best relationship\n        best_rel = \"none\"\n        best_ll = float('-inf')\n        \n        for rel, ll in age_constrained_ll.items():\n            if ll > best_ll and ll > float('-inf'):\n                best_rel = rel\n                best_ll = ll\n        \n        # Store results\n        results.append({\n            'age_diff': age_diff,\n            'best_relationship': best_rel,\n            'log_likelihood': best_ll if best_ll > float('-inf') else \"N/A\"\n        })\n\n    # Display results as a table\n    result_df = pd.DataFrame(results)\n    print(\"\\nEffect of Age Difference on Relationship Prediction (simplified model):\")\n    print(result_df)\n\n    # Example of how age information is incorporated\n    print(\"\\nSimplified model of how age information can be incorporated:\")\n    print(\"1. Prior probabilities: Adjust relationship priors based on age differences\")\n    print(\"2. Likelihood modifiers: Multiply genetic likelihoods by age-based factors\")\n    print(\"3. Hard constraints: Rule out impossible relationships based on age\")\n    print(\"4. Pedigree consistency: Ensure the entire pedigree has age-consistent connections\")\n\n    # Example: Visualize how adding age constraints improves confidence\n    # Using our half-sibling example and a specific age difference\n    chosen_age_diff = 2  # Example: 2 years apart (consistent with siblings)\n\n    # Calculate age-constrained likelihoods\n    age_constrained_ll = calculate_age_constrained_likelihoods(example_genetic_ll, chosen_age_diff)\n\n    # Calculate normalized probabilities for both\n    genetic_only_norm = normalize_log_likelihoods(example_genetic_ll)\n    age_constrained_norm = normalize_log_likelihoods(age_constrained_ll)\n\n    # Prepare data for visualization\n    rel_order = ['parent-child', 'full-siblings', 'half-siblings', \n                '1st-cousins', '2nd-cousins', '3rd-cousins', 'unrelated']\n\n    genetic_only_probs = [genetic_only_norm.get(rel, 0) for rel in rel_order]\n    age_constrained_probs = [age_constrained_norm.get(rel, 0) for rel in rel_order]\n\n    # Create bar chart comparison\n    plt.figure(figsize=(12, 6))\n    x = np.arange(len(rel_order))\n    width = 0.35\n\n    plt.bar(x - width/2, genetic_only_probs, width, label='Genetic Data Only')\n    plt.bar(x + width/2, age_constrained_probs, width, label='Genetic + Age Data')\n\n    plt.xlabel('Relationship Type')\n    plt.ylabel('Probability')\n    plt.title(f'Impact of Age Information (Age Diff: {chosen_age_diff} years)')\n    plt.xticks(x, rel_order, rotation=45)\n    plt.legend()\n    plt.grid(axis='y', alpha=0.3)\n    plt.tight_layout()\n    plt.show()",
   "metadata": {},
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Part 4: Multiple Hypothesis Testing in Relationship Inference\n\nIn genetic genealogy, we often need to compare multiple relationship hypotheses to determine the most likely connection between individuals. This is especially important when:\n\n1. The genetic evidence alone is ambiguous between two or more relationship types\n2. We have prior information about potential relationships from traditional genealogy\n3. We need to evaluate whether specific individuals fit into a particular pedigree structure\n\nBonsai v3 implements systematic approaches to test multiple relationship hypotheses and determine which is most consistent with the observed genetic data.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Multiple Hypothesis Testing for Relationship Inference\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport random\nfrom scipy import stats\nfrom scipy.special import logsumexp\nimport math\n\n# Define a function to create specific relationship hypotheses\ndef create_relationship_hypothesis(relationship_type):\n    \"\"\"\n    Create a formal relationship hypothesis structure.\n    \n    Args:\n        relationship_type: Type of relationship (e.g., 'parent-child', 'full-siblings')\n        \n    Returns:\n        Dictionary representing the relationship hypothesis\n    \"\"\"\n    # Map of relationship types to degree and specific properties\n    relationship_props = {\n        'parent-child': {\n            'degree': 1,\n            'expected_total_cm': 3400,\n            'expected_segments': 23,\n            'cm_std': 100,\n            'description': \"Parent-child relationship\"\n        },\n        'full-siblings': {\n            'degree': 1,\n            'expected_total_cm': 2550,\n            'expected_segments': 40, \n            'cm_std': 180,\n            'description': \"Full siblings sharing both parents\"\n        },\n        'half-siblings': {\n            'degree': 1.5,\n            'expected_total_cm': 1700,\n            'expected_segments': 30,\n            'cm_std': 160,\n            'description': \"Half siblings sharing one parent\"\n        },\n        '1st-cousins': {\n            'degree': 2,\n            'expected_total_cm': 850,\n            'expected_segments': 20,\n            'cm_std': 120,\n            'description': \"First cousins sharing grandparents\"\n        },\n        '2nd-cousins': {\n            'degree': 3,\n            'expected_total_cm': 212,\n            'expected_segments': 10,\n            'cm_std': 70,\n            'description': \"Second cousins sharing great-grandparents\"\n        },\n        '3rd-cousins': {\n            'degree': 4,\n            'expected_total_cm': 53,\n            'expected_segments': 5,\n            'cm_std': 30,\n            'description': \"Third cousins sharing great-great-grandparents\"\n        },\n        'unrelated': {\n            'degree': 10,\n            'expected_total_cm': 10,\n            'expected_segments': 1,\n            'cm_std': 10,\n            'description': \"No known relationship\"\n        }\n    }\n    \n    if relationship_type not in relationship_props:\n        raise ValueError(f\"Unknown relationship type: {relationship_type}\")\n    \n    # Create the hypothesis\n    hypothesis = {\n        'name': relationship_type,\n        'degree': relationship_props[relationship_type]['degree'],\n        'expected_ibd': relationship_props[relationship_type]['expected_total_cm'],\n        'expected_segments': relationship_props[relationship_type]['expected_segments'],\n        'std_dev': relationship_props[relationship_type]['cm_std'],\n        'description': relationship_props[relationship_type]['description'],\n        'prior_probability': 1.0  # Default uniform prior\n    }\n    \n    return hypothesis\n\n# Calculate the likelihood of observed data under a specific hypothesis\ndef calculate_hypothesis_loglikelihood(hypothesis, observed_data):\n    \"\"\"\n    Calculate the log-likelihood of observed data under a given relationship hypothesis.\n    \n    Args:\n        hypothesis: Dictionary representing the relationship hypothesis\n        observed_data: Dictionary with observed IBD data\n        \n    Returns:\n        Log-likelihood score\n    \"\"\"\n    # Extract relevant information\n    observed_total_cm = observed_data['total_cm']\n    observed_segments = observed_data['num_segments']\n    \n    # Use a normal distribution for total cM (simplified model)\n    ll_total_cm = stats.norm.logpdf(\n        observed_total_cm, \n        loc=hypothesis['expected_ibd'], \n        scale=hypothesis['std_dev']\n    )\n    \n    # Use a Poisson distribution for segment count (simplified model)\n    ll_segments = stats.poisson.logpmf(\n        observed_segments,\n        mu=hypothesis['expected_segments']\n    )\n    \n    # Combine the log-likelihoods (assuming independence)\n    # In practice, these are correlated, but this simplification works for demonstration\n    total_ll = ll_total_cm + ll_segments\n    \n    return total_ll\n\n# Compute the Bayes factor for comparing two hypotheses\ndef compute_bayes_factor(h1_ll, h2_ll):\n    \"\"\"\n    Compute the Bayes factor for comparing two hypotheses.\n    \n    Args:\n        h1_ll: Log-likelihood of hypothesis 1\n        h2_ll: Log-likelihood of hypothesis 2\n        \n    Returns:\n        Bayes factor (in linear space)\n    \"\"\"\n    # Bayes factor = P(data|H1) / P(data|H2)\n    # In log space: BF = exp(log(P(data|H1)) - log(P(data|H2)))\n    log_bf = h1_ll - h2_ll\n    return math.exp(log_bf)\n\n# Calculate posterior probabilities for multiple hypotheses\ndef calculate_posterior_probabilities(hypotheses, observed_data):\n    \"\"\"\n    Calculate posterior probabilities for multiple hypotheses.\n    \n    Args:\n        hypotheses: List of hypothesis dictionaries\n        observed_data: Dictionary with observed IBD data\n        \n    Returns:\n        List of hypotheses with posterior probabilities added\n    \"\"\"\n    # Calculate log-likelihood for each hypothesis\n    for h in hypotheses:\n        h['log_likelihood'] = calculate_hypothesis_loglikelihood(h, observed_data)\n    \n    # Calculate log posterior (log likelihood + log prior)\n    for h in hypotheses:\n        h['log_posterior'] = h['log_likelihood'] + math.log(h['prior_probability'])\n    \n    # Normalize the posteriors\n    log_posteriors = [h['log_posterior'] for h in hypotheses]\n    log_norm_factor = logsumexp(log_posteriors)\n    \n    for h in hypotheses:\n        h['posterior_probability'] = math.exp(h['log_posterior'] - log_norm_factor)\n    \n    # Sort by posterior probability (descending)\n    hypotheses.sort(key=lambda x: x['posterior_probability'], reverse=True)\n    \n    return hypotheses\n\n# Create several relationship hypotheses to test\ndef test_multiple_hypotheses(observed_data, relationship_types=None, prior_probs=None):\n    \"\"\"\n    Test multiple relationship hypotheses and calculate posterior probabilities.\n    \n    Args:\n        observed_data: Dictionary with observed IBD data\n        relationship_types: List of relationship types to test\n        prior_probs: Dictionary mapping relationship types to prior probabilities\n        \n    Returns:\n        DataFrame with hypothesis testing results\n    \"\"\"\n    if relationship_types is None:\n        # Default to testing all common relationships\n        relationship_types = [\n            'parent-child', 'full-siblings', 'half-siblings', \n            '1st-cousins', '2nd-cousins', '3rd-cousins', 'unrelated'\n        ]\n    \n    # Create hypothesis objects\n    hypotheses = []\n    for rel_type in relationship_types:\n        hypothesis = create_relationship_hypothesis(rel_type)\n        \n        # Apply custom prior if provided\n        if prior_probs and rel_type in prior_probs:\n            hypothesis['prior_probability'] = prior_probs[rel_type]\n            \n        hypotheses.append(hypothesis)\n    \n    # Ensure priors sum to 1\n    prior_sum = sum(h['prior_probability'] for h in hypotheses)\n    for h in hypotheses:\n        h['prior_probability'] /= prior_sum\n    \n    # Calculate posterior probabilities\n    results = calculate_posterior_probabilities(hypotheses, observed_data)\n    \n    # Create a DataFrame for display\n    result_df = pd.DataFrame([\n        {\n            'Relationship': h['name'],\n            'Degree': h['degree'],\n            'Prior': h['prior_probability'],\n            'Log-Likelihood': h['log_likelihood'],\n            'Posterior': h['posterior_probability']\n        }\n        for h in results\n    ])\n    \n    return result_df\n\n# Visualize hypothesis testing results\ndef visualize_hypothesis_results(result_df, title=None):\n    \"\"\"Visualize hypothesis testing results\"\"\"\n    plt.figure(figsize=(12, 6))\n    \n    # Create a bar chart of posterior probabilities\n    plt.bar(result_df['Relationship'], result_df['Posterior'], color='skyblue')\n    \n    plt.xlabel('Relationship Hypothesis')\n    plt.ylabel('Posterior Probability')\n    plt.title(title or 'Posterior Probabilities for Relationship Hypotheses')\n    plt.xticks(rotation=45)\n    plt.grid(axis='y', alpha=0.3)\n    plt.tight_layout()\n    \n    # Add value labels on bars\n    for i, v in enumerate(result_df['Posterior']):\n        plt.text(i, v + 0.01, f\"{v:.3f}\", ha='center')\n    \n    plt.show()\n\n# Demonstrate hypothesis testing with our example data\nprint(\"Testing multiple relationship hypotheses on example data:\")\nprint(f\"Total cM: {example_data['total_cm']:.1f}\")\nprint(f\"Number of segments: {example_data['num_segments']}\")\n\n# Test with uniform priors\nresult_uniform = test_multiple_hypotheses(example_data)\nprint(\"\\nResults with uniform priors:\")\nprint(result_uniform)\n\n# Visualize results\nvisualize_hypothesis_results(result_uniform, title=\"Posterior Probabilities (Uniform Priors)\")\n\n# Now test with informative priors based on other information\n# Example: We have documentary evidence suggesting half-siblings or 1st cousins\ninformative_priors = {\n    'parent-child': 0.05,\n    'full-siblings': 0.1,\n    'half-siblings': 0.4,  # Higher prior for half-siblings\n    '1st-cousins': 0.3,    # Higher prior for 1st cousins\n    '2nd-cousins': 0.1,\n    '3rd-cousins': 0.03,\n    'unrelated': 0.02\n}\n\nresult_informative = test_multiple_hypotheses(example_data, prior_probs=informative_priors)\nprint(\"\\nResults with informative priors:\")\nprint(result_informative)\n\n# Visualize results with informative priors\nvisualize_hypothesis_results(result_informative, title=\"Posterior Probabilities (Informative Priors)\")\n\n# Compare the effect of priors\nprint(\"\\nEffect of Priors on Hypothesis Testing:\")\nprint(\"Uniform Priors - Top relationship: \" + \n      f\"{result_uniform.iloc[0]['Relationship']} (P={result_uniform.iloc[0]['Posterior']:.3f})\")\nprint(\"Informative Priors - Top relationship: \" + \n      f\"{result_informative.iloc[0]['Relationship']} (P={result_informative.iloc[0]['Posterior']:.3f})\")\n\n# Calculate Bayes factor for top two hypotheses (with uniform priors)\nif len(result_uniform) >= 2:\n    top_rel = result_uniform.iloc[0]['Relationship']\n    second_rel = result_uniform.iloc[1]['Relationship']\n    \n    top_ll = result_uniform.iloc[0]['Log-Likelihood']\n    second_ll = result_uniform.iloc[1]['Log-Likelihood']\n    \n    bf = compute_bayes_factor(top_ll, second_ll)\n    \n    print(f\"\\nBayes Factor ({top_rel} vs. {second_rel}): {bf:.2f}\")\n    \n    # Interpret Bayes factor\n    if bf > 100:\n        interpretation = \"Very strong evidence for \" + top_rel\n    elif bf > 10:\n        interpretation = \"Strong evidence for \" + top_rel\n    elif bf > 3:\n        interpretation = \"Moderate evidence for \" + top_rel\n    else:\n        interpretation = \"Weak or inconclusive evidence\"\n        \n    print(f\"Interpretation: {interpretation}\")",
   "metadata": {},
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Part 5: Visualizing Uncertainty in Relationship Predictions\n\nEffectively communicating uncertainty in relationship predictions is crucial for users to make informed decisions. Bonsai v3 implements various visualization techniques to represent confidence levels and uncertainty in its predictions:\n\n1. **Probability Distributions**: Visualizing the full distribution of relationship probabilities\n2. **Confidence Regions**: Highlighting confidence intervals in visualizations\n3. **Color Coding**: Using color to encode confidence levels in pedigree visualizations\n4. **Alternative Hypotheses**: Showing multiple plausible pedigree structures\n\nLet's explore how Bonsai helps users interpret results by visually representing uncertainty:",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Visualizing Uncertainty in Relationship Predictions\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport matplotlib.patches as mpatches\nimport networkx as nx\nimport random\nfrom matplotlib.colors import LinearSegmentedColormap\n\n# Create a dataset of multiple samples with various confidence levels\nnp.random.seed(42)\nrandom.seed(42)\n\n# Function to generate a dataset with varying confidence levels\ndef generate_test_dataset(n_samples=20):\n    \"\"\"\n    Generate a dataset of IBD comparisons with varying confidence levels.\n    \n    Args:\n        n_samples: Number of sample pairs to generate\n        \n    Returns:\n        DataFrame with relationship predictions and confidence metrics\n    \"\"\"\n    # List of possible true relationships\n    relationships = ['parent-child', 'full-siblings', 'half-siblings', \n                    '1st-cousins', '2nd-cousins', '3rd-cousins', 'unrelated']\n    \n    # List to store results\n    results = []\n    \n    for i in range(n_samples):\n        # Select a random true relationship\n        true_rel = random.choice(relationships)\n        \n        # Generate a sample for this relationship\n        sample_data = simulate_ibd_data(true_rel)\n        \n        # Calculate likelihoods\n        ll_scores = calculate_likelihood_scores(sample_data)\n        \n        # Calculate normalized probabilities\n        probs = normalize_log_likelihoods(ll_scores)\n        \n        # Find predicted relationship (highest probability)\n        pred_rel = max(probs.items(), key=lambda x: x[1])[0]\n        pred_prob = probs[pred_rel]\n        \n        # Calculate confidence ratio (top / second)\n        sorted_probs = sorted(probs.items(), key=lambda x: x[1], reverse=True)\n        if len(sorted_probs) >= 2 and sorted_probs[1][1] > 0:\n            conf_ratio = sorted_probs[0][1] / sorted_probs[1][1]\n        else:\n            conf_ratio = float('inf')\n        \n        # Calculate entropy (measure of uncertainty)\n        entropy = -sum(p * np.log2(p) for p in probs.values() if p > 0)\n        \n        # Store result\n        results.append({\n            'id': f\"Sample-{i+1}\",\n            'true_relationship': true_rel,\n            'predicted_relationship': pred_rel,\n            'prediction_probability': pred_prob,\n            'confidence_ratio': conf_ratio,\n            'entropy': entropy,\n            'total_cm': sample_data['total_cm'],\n            'num_segments': sample_data['num_segments'],\n            'correct': true_rel == pred_rel\n        })\n    \n    # Convert to DataFrame\n    df = pd.DataFrame(results)\n    \n    # Add a confidence category for easier visualization\n    df['confidence_category'] = pd.cut(\n        df['confidence_ratio'], \n        bins=[0, 2, 10, 100, float('inf')],\n        labels=['Low', 'Moderate', 'High', 'Very High']\n    )\n    \n    return df\n\n# Generate test dataset\ntest_data = generate_test_dataset(n_samples=30)\n\n# Display summary of test data\nprint(\"Generated test dataset with varying confidence levels:\")\nprint(test_data[['id', 'true_relationship', 'predicted_relationship', \n                'confidence_ratio', 'confidence_category', 'correct']].head(10))\n\n# Calculate overall accuracy\naccuracy = test_data['correct'].mean()\nprint(f\"\\nOverall prediction accuracy: {accuracy:.2%}\")\n\n# Calculate accuracy by confidence category\naccuracy_by_conf = test_data.groupby('confidence_category')['correct'].mean()\nprint(\"\\nAccuracy by confidence category:\")\nprint(accuracy_by_conf)\n\n# 1. Visualize relationship prediction probability vs confidence ratio\nplt.figure(figsize=(10, 6))\nplt.scatter(\n    test_data['prediction_probability'], \n    test_data['confidence_ratio'],\n    c=test_data['correct'].map({True: 'green', False: 'red'}),\n    alpha=0.7,\n    s=80\n)\n\nplt.xscale('linear')\nplt.yscale('log')\nplt.xlabel('Prediction Probability')\nplt.ylabel('Confidence Ratio (log scale)')\nplt.title('Relationship Prediction Confidence')\nplt.grid(True, alpha=0.3)\n\n# Add legend\ngreen_patch = mpatches.Patch(color='green', label='Correct Prediction')\nred_patch = mpatches.Patch(color='red', label='Incorrect Prediction')\nplt.legend(handles=[green_patch, red_patch])\n\n# Add confidence region thresholds\nplt.axhline(y=2, color='gray', linestyle='--', alpha=0.5)\nplt.axhline(y=10, color='gray', linestyle='--', alpha=0.5)\nplt.axhline(y=100, color='gray', linestyle='--', alpha=0.5)\n\nplt.text(0.05, 1.5, 'Low Confidence', ha='left', va='top', alpha=0.7)\nplt.text(0.05, 5, 'Moderate Confidence', ha='left', va='top', alpha=0.7)\nplt.text(0.05, 50, 'High Confidence', ha='left', va='top', alpha=0.7)\nplt.text(0.05, 500, 'Very High Confidence', ha='left', va='top', alpha=0.7)\n\nplt.tight_layout()\nplt.show()\n\n# 2. Visualize prediction accuracy by confidence category\nplt.figure(figsize=(10, 6))\nsns.barplot(\n    x=accuracy_by_conf.index,\n    y=accuracy_by_conf.values,\n    palette=['#ff9999', '#ffcc99', '#99cc99', '#66bb66']\n)\n\nplt.xlabel('Confidence Category')\nplt.ylabel('Prediction Accuracy')\nplt.title('Accuracy by Confidence Level')\nplt.ylim(0, 1.05)\n\n# Add value labels on bars\nfor i, v in enumerate(accuracy_by_conf):\n    plt.text(i, v + 0.02, f\"{v:.2%}\", ha='center')\n\nplt.tight_layout()\nplt.show()\n\n# 3. Visualize a heat map of confidence by true and predicted relationship\n# Create a pivot table of average confidence ratio by true and predicted relationship\nconf_heatmap = test_data.pivot_table(\n    index='true_relationship',\n    columns='predicted_relationship',\n    values='confidence_ratio',\n    aggfunc='mean'\n)\n\n# Plot heatmap\nplt.figure(figsize=(12, 8))\nsns.heatmap(\n    conf_heatmap,\n    annot=True,\n    fmt=\".1f\",\n    cmap=\"YlGnBu\",\n    linewidths=.5,\n    cbar_kws={'label': 'Avg. Confidence Ratio'}\n)\n\nplt.title('Confidence by True vs. Predicted Relationship')\nplt.tight_layout()\nplt.show()\n\n# 4. Visualize relationship probabilities for a specific example\n# Let's select a sample with moderate confidence\nmoderate_conf_sample = test_data[test_data['confidence_category'] == 'Moderate'].iloc[0]\nsample_id = moderate_conf_sample['id']\n\n# Get the sample data\nsample_idx = int(sample_id.split('-')[1]) - 1\nsample_rel = moderate_conf_sample['true_relationship']\nsample_data = simulate_ibd_data(sample_rel, num_segments=test_data.loc[sample_idx, 'num_segments'])\n\n# Calculate the probabilities\nsample_ll = calculate_likelihood_scores(sample_data)\nsample_probs = normalize_log_likelihoods(sample_ll)\n\n# Plot the probability distribution\nplt.figure(figsize=(12, 6))\n\n# Sort relationships by probability\nsorted_rels = sorted(sample_probs.items(), key=lambda x: x[1], reverse=True)\nrels = [r for r, _ in sorted_rels]\nprobs = [p for _, p in sorted_rels]\n\n# Create colormap based on confidence\ncolors = []\nfor i, (rel, prob) in enumerate(sorted_rels):\n    if i == 0:\n        colors.append('#3366cc')  # Blue for highest probability\n    else:\n        # Calculate confidence ratio with top relationship\n        ratio = sorted_rels[0][1] / prob if prob > 0 else float('inf')\n        if ratio > 100:\n            colors.append('#cccccc')  # Light gray for very low probability\n        elif ratio > 10:\n            colors.append('#999999')  # Medium gray\n        else:\n            colors.append('#666666')  # Dark gray for more plausible alternatives\n\nplt.bar(rels, probs, color=colors)\nplt.xlabel('Relationship Type')\nplt.ylabel('Probability')\nplt.title(f'Relationship Probability Distribution (Sample {sample_id})')\nplt.xticks(rotation=45)\nplt.grid(axis='y', alpha=0.3)\n\n# Add value labels on bars\nfor i, v in enumerate(probs):\n    plt.text(i, v + 0.01, f\"{v:.3f}\", ha='center')\n\n# Highlight true relationship\ntrue_rel_idx = rels.index(moderate_conf_sample['true_relationship']) if moderate_conf_sample['true_relationship'] in rels else -1\nif true_rel_idx >= 0:\n    plt.gca().get_xticklabels()[true_rel_idx].set_color('red')\n    plt.gca().get_xticklabels()[true_rel_idx].set_weight('bold')\n\nplt.tight_layout()\nplt.show()\n\n# 5. Simulate a simple pedigree visualization with confidence color coding\ndef visualize_pedigree_with_confidence(relationships_with_confidence):\n    \"\"\"\n    Create a simplified pedigree visualization with confidence color coding.\n    \n    Args:\n        relationships_with_confidence: List of (id1, id2, relationship, confidence) tuples\n    \"\"\"\n    # Create network graph\n    G = nx.Graph()\n    \n    # Add all unique individuals\n    all_ids = set()\n    for id1, id2, _, _ in relationships_with_confidence:\n        all_ids.add(id1)\n        all_ids.add(id2)\n    \n    for person_id in all_ids:\n        G.add_node(person_id)\n    \n    # Create colormap for confidence levels\n    confidence_cmap = LinearSegmentedColormap.from_list(\n        'confidence', \n        [(0, '#ff9999'), (0.33, '#ffcc99'), (0.67, '#99cc99'), (1, '#339933')]\n    )\n    \n    # Add edges with color based on confidence\n    edge_colors = []\n    edge_widths = []\n    edge_labels = {}\n    \n    for id1, id2, rel, conf in relationships_with_confidence:\n        G.add_edge(id1, id2)\n        \n        # Normalize confidence for color mapping (0-100 scale)\n        norm_conf = min(1.0, conf / 100)\n        edge_colors.append(confidence_cmap(norm_conf))\n        \n        # Edge width based on relationship closeness\n        if rel == 'parent-child' or rel == 'full-siblings':\n            width = 3.0\n        elif rel == 'half-siblings' or rel == '1st-cousins':\n            width = 2.0\n        else:\n            width = 1.0\n        \n        edge_widths.append(width)\n        \n        # Edge label with relationship and confidence\n        conf_str = f\"{conf:.1f}\"\n        if conf > 100:\n            conf_str = \">100\"\n        edge_labels[(id1, id2)] = f\"{rel}\\n({conf_str}x)\"\n    \n    # Create plot\n    plt.figure(figsize=(12, 10))\n    pos = nx.spring_layout(G, seed=42)  # Use spring layout\n    \n    # Draw nodes\n    nx.draw_networkx_nodes(G, pos, node_size=500, node_color='lightblue')\n    \n    # Draw edges with custom colors and widths\n    for i, (u, v) in enumerate(G.edges()):\n        nx.draw_networkx_edges(\n            G, pos, edgelist=[(u, v)], \n            width=edge_widths[i], \n            edge_color=[edge_colors[i]]\n        )\n    \n    # Draw node labels\n    nx.draw_networkx_labels(G, pos, font_size=10)\n    \n    # Draw edge labels\n    nx.draw_networkx_edge_labels(\n        G, pos, \n        edge_labels=edge_labels,\n        font_size=8\n    )\n    \n    plt.title(\"Pedigree with Confidence Visualization\")\n    plt.axis('off')\n    plt.tight_layout()\n    plt.show()\n\n# Create a sample pedigree with confidence values\nexample_pedigree = [\n    ('A', 'B', 'parent-child', 150.0),    # Very high confidence\n    ('A', 'C', 'parent-child', 120.0),    # Very high confidence\n    ('B', 'D', 'parent-child', 80.0),     # High confidence\n    ('C', 'E', 'parent-child', 15.0),     # Moderate confidence\n    ('D', 'F', 'parent-child', 4.0),      # Low confidence\n    ('E', 'F', '1st-cousins', 2.5),       # Low confidence\n    ('G', 'F', 'half-siblings', 6.0)      # Moderate confidence\n]\n\n# Visualize pedigree with confidence color coding\nvisualize_pedigree_with_confidence(example_pedigree)\n\nprint(\"\\nInterpreting Visualization Confidence:\")\nprint(\"- Green connections: High confidence relationships\")\nprint(\"- Yellow connections: Moderate confidence relationships\")\nprint(\"- Red connections: Low confidence relationships\")\nprint(\"- Edge thickness indicates relationship closeness\")\nprint(\"- Labels show relationship type and confidence ratio\")\n\nprint(\"\\nBest Practices for Interpreting Results:\")\nprint(\"1. Focus on high-confidence predictions first (green connections)\")\nprint(\"2. Use age information to disambiguate uncertain relationships\")\nprint(\"3. Consider alternative hypotheses for low-confidence predictions\")\nprint(\"4. Look for patterns of errors (e.g., consistent under-estimation of degree)\")\nprint(\"5. Integrate documentary evidence to refine genetic predictions\")\nprint(\"6. Recognize that small segments (<7 cM) may increase false positives\")\nprint(\"7. Consider endogamy and population structure as potential confounders\")",
   "metadata": {},
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Conclusion: Making Sense of Genetic Relationship Predictions\n\nIn this lab, we've explored how Bonsai v3 interprets genetic data to infer relationships between individuals and assigns confidence to these predictions. We've covered:\n\n1. **Likelihood-Based Inference**: We learned how Bonsai calculates likelihoods for different relationship hypotheses and uses these to determine the most probable relationship.\n\n2. **Confidence Intervals**: We explored methods for calculating confidence bounds on relationship degree estimates, providing a range of plausible relationships rather than a single point estimate.\n\n3. **Age-Based Constraints**: We examined how age information can significantly enhance the accuracy of relationship predictions by ruling out implausible relationships.\n\n4. **Multiple Hypothesis Testing**: We demonstrated systematic approaches for comparing alternative relationship hypotheses using Bayes factors and posterior probabilities.\n\n5. **Visualizing Uncertainty**: We explored various techniques for visually representing confidence levels in relationship predictions, helping users understand the reliability of results.\n\nEffective interpretation of genetic relationship data requires understanding both the statistical models and their limitations. By properly interpreting confidence measures and incorporating multiple sources of evidence, users can make more informed decisions about family connections based on genetic data.\n\nRemember these key principles:\n\n- Relationship predictions are probabilistic, not deterministic\n- Higher confidence (larger likelihood ratios) indicates more reliable predictions\n- Multiple sources of evidence (genetic data, age information, documentary evidence) should be integrated\n- Visual representations help communicate uncertainty effectively\n- Always consider alternative explanations for ambiguous predictions\n\nAs computational methods for genetic genealogy continue to advance, the ability to accurately assess and communicate confidence in relationship predictions remains crucial for responsible interpretation of results.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": "# Try to use Bonsai's PwLogLike class for relationship inference\ntry:\n    from utils.bonsaitree.bonsaitree.v3.likelihoods import PwLogLike\n    from utils.bonsaitree.bonsaitree.v3.ibd import IBDSegment\n\n    print(\"‚úÖ Successfully imported PwLogLike and IBDSegment classes\")\n    \n    # Convert our simulated IBD data to Bonsai's IBDSegment format\n    def convert_to_bonsai_segments(ibd_data):\n        \"\"\"Convert simulated IBD data to Bonsai IBDSegment objects\"\"\"\n        bonsai_segments = []\n        \n        for segment in ibd_data['segments']:\n            try:\n                ibd_seg = IBDSegment(\n                    chrom=segment['chromosome'],\n                    start_pos=segment['start_pos'],\n                    end_pos=segment['end_pos'],\n                    cm_length=segment['cm'],\n                    snp_count=segment['snps']\n                )\n                bonsai_segments.append(ibd_seg)\n            except Exception as e:\n                print(f\"Error creating IBDSegment: {e}\")\n                \n        return bonsai_segments\n    \n    # Using the example data we created earlier\n    bonsai_segments = convert_to_bonsai_segments(example_data)\n    print(f\"Created {len(bonsai_segments)} Bonsai IBDSegment objects\")\n    \n    # Create a PwLogLike object to calculate likelihoods\n    try:\n        # This is a simplified call - actual parameters may vary based on the implementation\n        pw_log_like = PwLogLike(bonsai_segments)\n        \n        # Get likelihood scores for different degrees\n        rel_scores = {}\n        for degree in range(1, 5):  # Typically 1st to 4th degree\n            score = pw_log_like.get_loglike(degree)\n            rel_scores[f\"degree-{degree}\"] = score\n            \n        print(\"\\nBonsai PwLogLike Scores:\")\n        for rel, score in sorted(rel_scores.items(), key=lambda x: x[1], reverse=True):\n            print(f\"{rel:<10}: {score:.2f}\")\n        \n        # Get the most likely degree\n        most_likely_degree = max(rel_scores.items(), key=lambda x: x[1])[0]\n        print(f\"\\nMost likely relationship degree: {most_likely_degree}\")\n        \n        # Calculate confidence interval if available\n        if hasattr(pw_log_like, 'get_degree_conf_int'):\n            conf_int = pw_log_like.get_degree_conf_int(0.95)  # 95% confidence interval\n            print(f\"95% Confidence Interval: {conf_int}\")\n    \n    except Exception as e:\n        print(f\"Error using PwLogLike: {e}\")\n        print(\"The PwLogLike interface may have changed. Check the class documentation.\")\n    \nexcept ImportError as e:\n    print(f\"‚ùå Failed to import Bonsai classes: {e}\")\n    print(\"Falling back to simplified implementation...\")\n    \n    # Fallback to our simplified implementation\n    print(\"\\nUsing simplified likelihood calculation:\")\n    \n    # Calculate simplified likelihood\n    simple_ll = calculate_likelihood_scores(example_data)\n    simple_normalized = normalize_log_likelihoods(simple_ll)\n    \n    # Display results\n    for rel, prob in sorted(simple_normalized.items(), key=lambda x: x[1], reverse=True)[:3]:\n        print(f\"{rel:<15}: {prob:.4f} ({prob*100:.1f}%)\")\n    \n    # Plot simplified relationship probabilities\n    plt.figure(figsize=(10, 6))\n    \n    # Get top 5 relationships\n    top_rels = sorted(simple_normalized.items(), key=lambda x: x[1], reverse=True)[:5]\n    rels = [r for r, _ in top_rels]\n    probs = [p for _, p in top_rels]\n    \n    # Create bar chart\n    plt.bar(rels, probs, color='lightgreen')\n    plt.xlabel('Relationship Type')\n    plt.ylabel('Probability')\n    plt.title('Top 5 Relationship Probabilities (Simplified Model)')\n    plt.xticks(rotation=45)\n    plt.grid(axis='y', alpha=0.3)\n    plt.tight_layout()\n    plt.show()"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}