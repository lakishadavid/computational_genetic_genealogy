{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from collections import Counter\n",
    "import logging\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import subprocess\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from IPython.display import display, HTML\n",
    "import IPython\n",
    "import pandas as pd\n",
    "import boto3\n",
    "\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_comp_gen_dir():\n",
    "    \"\"\"Find the computational_genetic_genealogy directory by searching up from current directory.\"\"\"\n",
    "    current = Path.cwd()\n",
    "    \n",
    "    # Search up through parent directories\n",
    "    while current != current.parent:\n",
    "        # Check if target directory exists in current path\n",
    "        target = current / 'computational_genetic_genealogy'\n",
    "        if target.is_dir():\n",
    "            return target\n",
    "        # Move up one directory\n",
    "        current = current.parent\n",
    "    \n",
    "    raise FileNotFoundError(\"Could not find computational_genetic_genealogy directory\")\n",
    "\n",
    "def load_env_file():\n",
    "    \"\"\"Find and load the .env file from the computational_genetic_genealogy directory.\"\"\"\n",
    "    try:\n",
    "        # Find the computational_genetic_genealogy directory\n",
    "        comp_gen_dir = find_comp_gen_dir()\n",
    "        \n",
    "        # Look for .env file\n",
    "        env_path = comp_gen_dir / '.env'\n",
    "        if not env_path.exists():\n",
    "            print(f\"Warning: No .env file found in {comp_gen_dir}\")\n",
    "            return None\n",
    "        \n",
    "        # Load the .env file\n",
    "        load_dotenv(env_path, override=True)\n",
    "        print(f\"Loaded environment variables from: {env_path}\")\n",
    "        return env_path\n",
    "        \n",
    "    except FileNotFoundError as e:\n",
    "        print(f\"Error: {e}\")\n",
    "        return None\n",
    "\n",
    "# Use the function\n",
    "env_path = load_env_file()\n",
    "\n",
    "working_directory = os.getenv('PROJECT_WORKING_DIR', default=None)\n",
    "data_directory = os.getenv('PROJECT_DATA_DIR', default=None)\n",
    "references_directory = os.getenv('PROJECT_REFERENCES_DIR', default=None)\n",
    "results_directory = os.getenv('PROJECT_RESULTS_DIR', default=None)\n",
    "utils_directory = os.getenv('PROJECT_UTILS_DIR', default=None)\n",
    "\n",
    "os.environ[\"WORKING_DIRECTORY\"] = working_directory\n",
    "os.environ[\"DATA_DIRECTORY\"] = data_directory\n",
    "os.environ[\"REFERENCES_DIRECTORY\"] = references_directory\n",
    "os.environ[\"RESULTS_DIRECTORY\"] = results_directory\n",
    "os.environ[\"UTILS_DIRECTORY\"] = utils_directory\n",
    "\n",
    "print(f\"Working Directory: {working_directory}\")\n",
    "print(f\"Data Directory: {data_directory}\")\n",
    "print(f\"References Directory: {references_directory}\")\n",
    "print(f\"Results Directory: {results_directory}\")\n",
    "print(f\"Utils Directory: {utils_directory}\")\n",
    "\n",
    "os.chdir(working_directory)\n",
    "print(f\"The current directory is {os.getcwd()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def configure_logging(log_filename, log_file_debug_level=\"INFO\", console_debug_level=\"INFO\"):\n",
    "    \"\"\"\n",
    "    Configure logging for both file and console handlers.\n",
    "\n",
    "    Args:\n",
    "        log_filename (str): Path to the log file where logs will be written.\n",
    "        log_file_debug_level (str): Logging level for the file handler.\n",
    "        console_debug_level (str): Logging level for the console handler.\n",
    "    \"\"\"\n",
    "    # Create a root logger\n",
    "    logger = logging.getLogger()\n",
    "    logger.setLevel(logging.DEBUG)  # Capture all messages at the root level\n",
    "\n",
    "    # Convert level names to numeric levels\n",
    "    file_level = getattr(logging, log_file_debug_level.upper(), logging.INFO)\n",
    "    console_level = getattr(logging, console_debug_level.upper(), logging.INFO)\n",
    "\n",
    "    # File handler: Logs messages at file_level and above to the file\n",
    "    file_handler = logging.FileHandler(log_filename)\n",
    "    file_handler.setLevel(file_level)\n",
    "    file_formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')\n",
    "    file_handler.setFormatter(file_formatter)\n",
    "\n",
    "    # Console handler: Logs messages at console_level and above to the console\n",
    "    console_handler = logging.StreamHandler(sys.stdout)\n",
    "    console_handler.setLevel(console_level)\n",
    "    console_formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')\n",
    "    console_handler.setFormatter(console_formatter)\n",
    "\n",
    "    # Add handlers to the root logger\n",
    "    logger.addHandler(file_handler)\n",
    "    logger.addHandler(console_handler)\n",
    "    \n",
    "def clear_logger():\n",
    "    \"\"\"Remove all handlers from the root logger.\"\"\"\n",
    "    logger = logging.getLogger()\n",
    "    for handler in logger.handlers[:]:\n",
    "        logger.removeHandler(handler)\n",
    "        \n",
    "log_filename = os.path.join(results_directory, \"lab6.log\")\n",
    "print(f\"The Lab 6 log file is located at {log_filename}.\")\n",
    "\n",
    "# Ensure the results_directory exists\n",
    "if not os.path.exists(results_directory):\n",
    "    os.makedirs(results_directory)\n",
    "\n",
    "# Check if the file exists; if not, create it\n",
    "if not os.path.exists(log_filename):\n",
    "    with open(log_filename, 'w') as file:\n",
    "        pass  # The file is now created.\n",
    "    \n",
    "clear_logger() # Clear the logger before reconfiguring it\n",
    "configure_logging(log_filename, log_file_debug_level=\"INFO\", console_debug_level=\"INFO\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you are returning back to this point after running subsequent labs, you need the results from the ped-sim notebook and quality control. Alternatively (e.g, if you don't have those results), run the following cell to copy prepared results from the instructor's run of the ped-sim notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy required files from class_data to results directory\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "# List of files to copy\n",
    "files_to_copy = [\n",
    "    \"ped_sim_run2-everyone.fam\",\n",
    "    \"ped_sim_run2.seg\",\n",
    "    \"ped_sim_run2.seg_dict.txt\",\n",
    "    \"pedigree.fam\",\n",
    "    \"merged_opensnps_data.vcf.gz\",\n",
    "    \"merged_opensnps_data.vcf.gz.tbi\",\n",
    "    \"merged_opensnps_data_autosomes.vcf.gz\",\n",
    "    \"merged_opensnps_data_autosomes.vcf.gz.tbi\",\n",
    "]\n",
    "\n",
    "directories_to_copy = [\n",
    "    \"merged_opensnps_data_autosomes\",\n",
    "]\n",
    "\n",
    "# Copy each file\n",
    "for file in files_to_copy:\n",
    "    source = os.path.join(data_directory, \"class_data\", file)\n",
    "    destination = os.path.join(results_directory, file)\n",
    "    shutil.copy2(source, destination)\n",
    "    print(f\"Copied {file} to {results_directory}\")\n",
    "    \n",
    "# Copy each directory\n",
    "for directory in directories_to_copy:\n",
    "    source = os.path.join(data_directory, \"class_data\", directory)\n",
    "    destination = os.path.join(results_directory, directory)\n",
    "    shutil.copytree(source, destination, dirs_exist_ok=True)\n",
    "    print(f\"Copied {directory} to {results_directory}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Select your VCF file**\n",
    "\n",
    "In the next cell, uncomment the file you want to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vcf_file = os.path.join(results_directory, \"merged_sample_autosomes_unphased.vcf.gz\")\n",
    "# vcf_directory = os.path.join(results_directory, \"real_data_autosomes\")\n",
    "\n",
    "# vcf_file = os.path.join(results_directory, \"ped_sim_run2_autosomes.vcf.gz\")\n",
    "# vcf_directory = os.path.join(results_directory, \"ped_sim_run2_autosomes\")\n",
    "\n",
    "vcf_file = os.path.join(results_directory, \"merged_opensnps_data_autosomes.vcf.gz\")\n",
    "vcf_directory = os.path.join(results_directory, \"merged_opensnps_data_autosomes\")\n",
    "\n",
    "# Check if the VCF file exists\n",
    "if not os.path.exists(vcf_file):\n",
    "    print(f\"VCF file not found: {vcf_file}\")\n",
    "else:\n",
    "    print(f\"VCF file found: {vcf_file}\")\n",
    "    \n",
    "# Check if the VCF directory exists\n",
    "if not os.path.exists(vcf_directory):\n",
    "    print(f\"VCF directory not found: {vcf_directory}\")\n",
    "else:\n",
    "    print(f\"VCF directory found: {vcf_directory}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the Refined-IBD Detection Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash -s \"$vcf_file\" \"$vcf_directory\" \"$results_directory\"\n",
    "\n",
    "vcf_file=\"$1\"\n",
    "vcf_directory=\"$2\"\n",
    "results_directory=\"$3\"\n",
    "\n",
    "# Extract the file prefix (removing .vcf.gz extension)\n",
    "output_prefix=\"${vcf_file%.vcf.gz}\"\n",
    "# Get base name of the VCF file\n",
    "base_name=$(basename \"$output_prefix\")\n",
    "# base_name remove \"_autosomes\" suffixif present\n",
    "base_name=${base_name/_autosomes/}\n",
    "\n",
    "# Define the Refined-IBD executable path\n",
    "refined_ibd=\"${UTILS_DIRECTORY}/refined-ibd.17Jan20.102.jar\"\n",
    "merge_ibd_segments=\"${UTILS_DIRECTORY}/merge-ibd-segments.17Jan20.102.jar\"\n",
    "\n",
    "# Ensure the Refined-IBD executable exists\n",
    "if [[ ! -f \"${refined_ibd}\" ]]; then\n",
    "    echo \"Error: Refined-IBD executable not found: ${refined_ibd}\" >&2\n",
    "fi\n",
    "\n",
    "# Ensure the Merge-IBD-Segments executable exists\n",
    "if [[ ! -f \"${merge_ibd_segments}\" ]]; then\n",
    "    echo \"Error: Merge-IBD-Segments executable not found: ${merge_ibd_segments}\" >&2\n",
    "fi\n",
    "\n",
    "# Create a directory for storing segment output files\n",
    "mkdir -p \"${vcf_directory}/segments\"\n",
    "\n",
    "# Run Refined-IBD analysis in loop by chromosome\n",
    "for run in {1..3}; do\n",
    "    for chr in {1..22}; do\n",
    "        phased_samples=\"${vcf_directory}/phased_samples/${base_name}_phased_chr${chr}.vcf.gz\"\n",
    "        if [[ ! -f \"${phased_samples}\" ]]; then\n",
    "            echo \"No matching VCF file found\" >&2\n",
    "            exit 1\n",
    "        fi\n",
    "\n",
    "        java -jar \"${refined_ibd}\" gt=\"${phased_samples}\" \\\n",
    "            map=\"${REFERENCES_DIRECTORY}/genetic_maps/beagle_genetic_maps/plink.chr${chr}.GRCh38.map\" \\\n",
    "            lod=4 \\\n",
    "            length=3 \\\n",
    "            out=\"${vcf_directory}/segments/temp_${base_name}_refinedibd_chr${chr}_run${run}.seg\" \\\n",
    "            nthreads=4\n",
    "    done\n",
    "done\n",
    "\n",
    "# Merge IBD segments for each chromosome\n",
    "gap_threshold=0.6  # Adjust as needed\n",
    "discord_threshold=1  # Adjust as needed\n",
    "\n",
    "for chr in {1..22}; do\n",
    "    echo \"Processing chromosome ${chr}\"\n",
    "    phased_samples=\"${vcf_directory}/phased_samples/${base_name}_phased_chr${chr}.vcf.gz\"\n",
    "    genetic_map=\"${REFERENCES_DIRECTORY}/genetic_maps/beagle_genetic_maps/plink.chr${chr}.GRCh38.map\"\n",
    "    merged_output=\"${vcf_directory}/segments/temp_${base_name}_refinedibd_chr${chr}.seg\"\n",
    "\n",
    "    # Concatenate all three runs of Refined IBD for this chromosome\n",
    "    zcat \"${vcf_directory}/segments/temp_${base_name}_refinedibd_chr${chr}_run1.seg.ibd.gz\" \\\n",
    "        \"${vcf_directory}/segments/temp_${base_name}_refinedibd_chr${chr}_run2.seg.ibd.gz\" \\\n",
    "        \"${vcf_directory}/segments/temp_${base_name}_refinedibd_chr${chr}_run3.seg.ibd.gz\" | \\\n",
    "    java -jar \"${merge_ibd_segments}\" \"${phased_samples}\" \"${genetic_map}\" \"${gap_threshold}\" \"${discord_threshold}\" > \"${merged_output}\"\n",
    "done\n",
    "\n",
    "# Create or empty the final merged output file\n",
    ": > \"${vcf_directory}/segments/${base_name}_autosomes_refinedibd.seg\"\n",
    "\n",
    "# Concatenate all merged chromosome-specific IBD files\n",
    "for chr in {1..22}; do\n",
    "    merged_file=\"${vcf_directory}/segments/temp_${base_name}_refinedibd_chr${chr}.seg\"\n",
    "    if [[ -f \"${merged_file}\" ]]; then\n",
    "        cat \"${merged_file}\" >> \"${results_directory}/${base_name}_autosomes_refinedibd.seg\"\n",
    "    else\n",
    "        echo \"Warning: File for chromosome ${chr} not found during final merging.\" >&2\n",
    "    fi\n",
    "done\n",
    "\n",
    "# Remove intermediate merged chromosome-specific files\n",
    "if [[ -f \"${vcf_directory}/segments/${base_name}_autosomes_refinedibd.seg\" ]]; then\n",
    "    for chr in {1..22}; do\n",
    "        rm -f \"${vcf_directory}/segments/temp_${base_name}_refinedibd_chr${chr}\"*.seg.ibd.gz\n",
    "        rm -f \"${vcf_directory}/segments/temp_${base_name}_refinedibd_chr${chr}\"*.seg.hbd.gz\n",
    "        rm -f \"${vcf_directory}/segments/temp_${base_name}_refinedibd_chr${chr}\"*.seg.log\n",
    "        rm -f \"${vcf_directory}/segments/temp_${base_name}_refinedibd_chr${chr}.seg\"\n",
    "    done\n",
    "fi\n",
    "\n",
    "echo \"Final merged IBD file: ${vcf_directory}/segments//${base_name}_autosomes_refinedibd.seg\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explore The Segments Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prefix = vcf_file.split(\".vcf.gz\")[0]\n",
    "print(prefix)\n",
    "base_name = os.path.basename(prefix)\n",
    "print(base_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to segments file\n",
    "segments = os.path.join(results_directory, f\"{base_name}_refinedibd.seg\")\n",
    "print(f\"The segments file is located at {segments}\")\n",
    "\n",
    "# Only proceed if the file exists\n",
    "if os.path.exists(segments):\n",
    "    segments_temp = pd.read_csv(segments, sep=\"\\t\", header=None)\n",
    "    segments_temp.columns = [\n",
    "        \"id1\", \"sample1_haplotype\", \"id2\", \"sample2_haplotype\",\n",
    "        \"chrom\", \"phys_start_pos\", \"phys_end_pos\", \n",
    "        \"lod_score\", \"genetic_length\"\n",
    "        ]\n",
    "    segments = segments_temp.sort_values(\n",
    "        by=[\"chrom\", \"phys_start_pos\", \"phys_end_pos\"],\n",
    "        ascending=[True, True, True]\n",
    "    )\n",
    "    segments = segments.reset_index(drop=True)\n",
    "    output_file = os.path.join(results_directory, \"merged_opensnps_data_autosomes_refinedibd.csv\")\n",
    "    segments.to_csv(output_file, sep=\"\\t\", index=False, header=False)\n",
    "    segments.info() \n",
    "else:\n",
    "    print(\"Cannot proceed without segments file.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "segments.head() # You can enter a number greater than 5 to view more rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "segments[\"genetic_length\"].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter segments on min_length=7, min_markers=436, max_error_density=0.004,\n",
    "\n",
    "filtered_segments_7cM = segments[segments[\"genetic_length\"] >= 7].copy()\n",
    "\n",
    "filtered_segments_7cM[\"genetic_length\"].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# First ensure id1 and id2 are consistently ordered\n",
    "filtered_segments_7cM[[\"id1\", \"id2\"]] = filtered_segments_7cM.apply(\n",
    "    lambda row: pd.Series((row[\"id1\"], row[\"id2\"])) if row[\"id1\"] < row[\"id2\"] \n",
    "    else pd.Series((row[\"id2\"], row[\"id1\"])), axis=1\n",
    ")\n",
    "\n",
    "pair_counts = filtered_segments_7cM.groupby([\"id1\", \"id2\"]).size().reset_index(name=\"pair_count\")\n",
    "pair_count_distribution = pair_counts[\"pair_count\"].value_counts().reset_index()\n",
    "pair_count_distribution.columns = [\"Number of Segments\", \"Number of Pairs\"]\n",
    "pair_count_distribution = pair_count_distribution.reset_index(drop=True)\n",
    "display(pair_count_distribution.style.hide(axis=\"index\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by id pairs and calculate all metrics at once\n",
    "aggregated_segments = filtered_segments_7cM.groupby([\"id1\", \"id2\"]).agg(\n",
    "    total_genetic_length=(\"genetic_length\", \"sum\"),\n",
    "    num_segments=(\"genetic_length\", \"count\"),\n",
    "    largest_segment=(\"genetic_length\", \"max\")\n",
    ").reset_index()\n",
    "\n",
    "# Check distribution of values\n",
    "display(aggregated_segments.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Define columns to plot\n",
    "columns = [\"total_genetic_length\", \"num_segments\", \"largest_segment\"]\n",
    "\n",
    "# Create subplots\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "# Histogram for each metric\n",
    "for i, col in enumerate(columns):\n",
    "    sns.histplot(aggregated_segments[col], bins=30, kde=True, ax=axes[i], edgecolor=\"black\")\n",
    "    axes[i].set_title(f\"Distribution of {col.replace('_', ' ').title()}\")\n",
    "    axes[i].set_xlabel(col.replace('_', ' ').title())\n",
    "    axes[i].set_ylabel(\"Frequency\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Box Plot\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "for i, col in enumerate(columns):\n",
    "    sns.boxplot(y=aggregated_segments[col], ax=axes[i])\n",
    "    axes[i].set_title(f\"Box Plot of {col.replace('_', ' ').title()}\")\n",
    "    axes[i].set_ylabel(col.replace('_', ' ').title())\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
