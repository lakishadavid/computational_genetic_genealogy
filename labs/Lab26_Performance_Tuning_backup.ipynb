{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 26: Performance Tuning for Large-Scale Applications\n",
    "\n",
    "## Overview\n",
    "\n",
    "This notebook explores advanced performance tuning techniques for deploying Bonsai v3 in large-scale applications. We'll examine the computational challenges of processing extensive datasets and complex pedigrees, and explore strategies to optimize performance while maintaining accuracy.\n",
    "\n",
    "**Learning Objectives:**\n",
    "- Understand the performance scaling challenges in genetic genealogy computation\n",
    "- Learn systematic profiling and benchmarking techniques for Python code\n",
    "- Implement algorithmic optimizations for core Bonsai functions\n",
    "- Explore memory optimization techniques for large datasets\n",
    "- Apply intelligent precision-performance tradeoffs in relationship inference\n",
    "\n",
    "**Prerequisites:**\n",
    "- Completion of Lab 12: Relationship Assessment\n",
    "- Completion of Lab 14: Optimizing Pedigrees\n",
    "- Familiarity with Python performance concepts\n",
    "\n",
    "**Estimated completion time:** 60-90 minutes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# ðŸ§¬ Google Colab Setup - Run this cell first!\n",
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import networkx as nx\n",
    "from IPython.display import display, HTML, Markdown\n",
    "\n",
    "def is_colab():\n",
    "    '''Check if running in Google Colab'''\n",
    "    try:\n",
    "        import google.colab\n",
    "        return True\n",
    "    except ImportError:\n",
    "        return False\n",
    "\n",
    "if is_colab():\n",
    "    print(\"ðŸ”¬ Setting up Google Colab environment...\")\n",
    "    \n",
    "    # Install dependencies\n",
    "    print(\"ðŸ“¦ Installing packages...\")\n",
    "    !pip install -q pysam biopython scikit-allel networkx pygraphviz seaborn plotly\n",
    "    !apt-get update -qq && apt-get install -qq samtools bcftools tabix graphviz-dev\n",
    "    \n",
    "    # Create directories\n",
    "    !mkdir -p /content/class_data /content/results\n",
    "    \n",
    "    # Download essential class data\n",
    "    print(\"ðŸ“¥ Downloading class data...\")\n",
    "    S3_BASE = \"https://computational-genetic-genealogy.s3.us-east-2.amazonaws.com/class_data/\"\n",
    "    data_files = [\n",
    "        \"pedigree.fam\", \"pedigree.def\", \n",
    "        \"merged_opensnps_autosomes_ped_sim.seg\",\n",
    "        \"merged_opensnps_autosomes_ped_sim-everyone.fam\",\n",
    "        \"ped_sim_run2.seg\", \"ped_sim_run2-everyone.fam\"\n",
    "    ]\n",
    "    \n",
    "    for file in data_files:\n",
    "        !wget -q -O /content/class_data/{file} {S3_BASE}{file}\n",
    "        print(f\"  âœ… {file}\")\n",
    "    \n",
    "    # Define utility functions\n",
    "    def setup_environment():\n",
    "        return \"/content/class_data\", \"/content/results\"\n",
    "    \n",
    "    def save_results(dataframe, filename, description=\"results\"):\n",
    "        os.makedirs(\"/content/results\", exist_ok=True)\n",
    "        full_path = f\"/content/results/{filename}\"\n",
    "        dataframe.to_csv(full_path, index=False)\n",
    "        display(HTML(f'''\n",
    "        <div style=\"padding: 10px; background-color: #e3f2fd; border-left: 4px solid #2196f3; margin: 10px 0;\">\n",
    "            <p><strong>ðŸ’¾ Results saved!</strong> To download: \n",
    "            <code>from google.colab import files; files.download('{full_path}')</code></p>\n",
    "        </div>\n",
    "        '''))\n",
    "        return full_path\n",
    "    \n",
    "    def save_plot(plt, filename, description=\"plot\"):\n",
    "        os.makedirs(\"/content/results\", exist_ok=True)\n",
    "        full_path = f\"/content/results/{filename}\"\n",
    "        plt.savefig(full_path, dpi=300, bbox_inches='tight')\n",
    "        plt.show()\n",
    "        display(HTML(f'''\n",
    "        <div style=\"padding: 10px; background-color: #e8f5e8; border-left: 4px solid #4caf50; margin: 10px 0;\">\n",
    "            <p><strong>ðŸ“Š Plot saved!</strong> To download: \n",
    "            <code>from google.colab import files; files.download('{full_path}')</code></p>\n",
    "        </div>\n",
    "        '''))\n",
    "        return full_path\n",
    "    \n",
    "    print(\"âœ… Colab setup complete! Ready to explore genetic genealogy.\")\n",
    "    \n",
    "else:\n",
    "    print(\"ðŸ  Local environment detected\")\n",
    "    def setup_environment():\n",
    "        return \"class_data\", \"results\"\n",
    "    def save_results(df, filename, description=\"\"):\n",
    "        os.makedirs(\"results\", exist_ok=True)\n",
    "        path = f\"results/{filename}\"\n",
    "        df.to_csv(path, index=False)\n",
    "        return path\n",
    "    def save_plot(plt, filename, description=\"\"):\n",
    "        os.makedirs(\"results\", exist_ok=True)\n",
    "        path = f\"results/{filename}\"\n",
    "        plt.savefig(path, dpi=300, bbox_inches='tight')\n",
    "        plt.show()\n",
    "        return path\n",
    "\n",
    "# Set up paths and configure visualization\n",
    "DATA_DIR, RESULTS_DIR = setup_environment()\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "sns.set_context(\"notebook\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Setup Bonsai module paths\n",
    "if not is_jupyterlite():\n",
    "    # In local environment, add the utils directory to system path\n",
    "    utils_dir = os.getenv('PROJECT_UTILS_DIR', os.path.join(os.path.dirname(DATA_DIR), 'utils'))\n",
    "    bonsaitree_dir = os.path.join(utils_dir, 'bonsaitree')\n",
    "    \n",
    "    # Add to path if it exists and isn't already there\n",
    "    if os.path.exists(bonsaitree_dir) and bonsaitree_dir not in sys.path:\n",
    "        sys.path.append(bonsaitree_dir)\n",
    "        print(f\"Added {bonsaitree_dir} to sys.path\")\n",
    "else:\n",
    "    # In JupyterLite, use a simplified approach\n",
    "    print(\"âš ï¸ Running in JupyterLite: Some Bonsai functionality may be limited.\")\n",
    "    print(\"This notebook is primarily designed for local execution where the Bonsai codebase is available.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Helper functions for exploring modules\n",
    "def display_module_classes(module_name):\n",
    "    \"\"\"Display classes and their docstrings from a module\"\"\"\n",
    "    try:\n",
    "        # Import the module\n",
    "        module = importlib.import_module(module_name)\n",
    "        \n",
    "        # Find all classes\n",
    "        classes = inspect.getmembers(module, inspect.isclass)\n",
    "        \n",
    "        # Filter classes defined in this module (not imported)\n",
    "        classes = [(name, cls) for name, cls in classes if cls.__module__ == module_name]\n",
    "        \n",
    "        if not classes:\n",
    "            print(f\"No classes found in module {module_name}\")\n",
    "            return\n",
    "            \n",
    "        # Print info for each class\n",
    "        for name, cls in classes:\n",
    "            display(Markdown(f\"### Class: {name}\"))\n",
    "            \n",
    "            # Get docstring\n",
    "            doc = inspect.getdoc(cls)\n",
    "            if doc:\n",
    "                display(Markdown(f\"**Documentation:**\\n{doc}\"))\n",
    "            else:\n",
    "                display(Markdown(\"*No documentation available*\"))\n",
    "            \n",
    "            # Get methods\n",
    "            methods = inspect.getmembers(cls, inspect.isfunction)\n",
    "            public_methods = [(method_name, method) for method_name, method in methods \n",
    "                             if not method_name.startswith('_')]\n",
    "            \n",
    "            if public_methods:\n",
    "                display(Markdown(\"**Public Methods:**\"))\n",
    "                for method_name, method in public_methods:\n",
    "                    sig = inspect.signature(method)\n",
    "                    display(Markdown(f\"- `{method_name}{sig}`\"))\n",
    "            else:\n",
    "                display(Markdown(\"*No public methods*\"))\n",
    "            \n",
    "            display(Markdown(\"---\"))\n",
    "    except ImportError as e:\n",
    "        print(f\"Error importing module {module_name}: {e}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing module {module_name}: {e}\")\n",
    "\n",
    "def display_module_functions(module_name):\n",
    "    \"\"\"Display functions and their docstrings from a module\"\"\"\n",
    "    try:\n",
    "        # Import the module\n",
    "        module = importlib.import_module(module_name)\n",
    "        \n",
    "        # Find all functions\n",
    "        functions = inspect.getmembers(module, inspect.isfunction)\n",
    "        \n",
    "        # Filter functions defined in this module (not imported)\n",
    "        functions = [(name, func) for name, func in functions if func.__module__ == module_name]\n",
    "        \n",
    "        if not functions:\n",
    "            print(f\"No functions found in module {module_name}\")\n",
    "            return\n",
    "            \n",
    "        # Filter public functions\n",
    "        public_functions = [(name, func) for name, func in functions if not name.startswith('_')]\n",
    "        \n",
    "        if not public_functions:\n",
    "            print(f\"No public functions found in module {module_name}\")\n",
    "            return\n",
    "            \n",
    "        # Print info for each function\n",
    "        for name, func in public_functions:                \n",
    "            display(Markdown(f\"### Function: {name}\"))\n",
    "            \n",
    "            # Get signature\n",
    "            sig = inspect.signature(func)\n",
    "            display(Markdown(f\"**Signature:** `{name}{sig}`\"))\n",
    "            \n",
    "            # Get docstring\n",
    "            doc = inspect.getdoc(func)\n",
    "            if doc:\n",
    "                display(Markdown(f\"**Documentation:**\\n{doc}\"))\n",
    "            else:\n",
    "                display(Markdown(\"*No documentation available*\"))\n",
    "                \n",
    "            display(Markdown(\"---\"))\n",
    "    except ImportError as e:\n",
    "        print(f\"Error importing module {module_name}: {e}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing module {module_name}: {e}\")\n",
    "\n",
    "def view_function_source(module_name, function_name):\n",
    "    \"\"\"Display the source code of a function\"\"\"\n",
    "    try:\n",
    "        # Import the module\n",
    "        module = importlib.import_module(module_name)\n",
    "        \n",
    "        # Get the function\n",
    "        func = getattr(module, function_name)\n",
    "        \n",
    "        # Get the source code\n",
    "        source = inspect.getsource(func)\n",
    "        \n",
    "        # Print the source code with syntax highlighting\n",
    "        display(Markdown(f\"### Source code for `{function_name}`\\n```python\\n{source}\\n```\"))\n",
    "    except ImportError as e:\n",
    "        print(f\"Error importing module {module_name}: {e}\")\n",
    "    except AttributeError:\n",
    "        print(f\"Function {function_name} not found in module {module_name}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing function {function_name}: {e}\")\n",
    "\n",
    "def view_class_source(module_name, class_name):\n",
    "    \"\"\"Display the source code of a class\"\"\"\n",
    "    try:\n",
    "        # Import the module\n",
    "        module = importlib.import_module(module_name)\n",
    "        \n",
    "        # Get the class\n",
    "        cls = getattr(module, class_name)\n",
    "        \n",
    "        # Get the source code\n",
    "        source = inspect.getsource(cls)\n",
    "        \n",
    "        # Print the source code with syntax highlighting\n",
    "        display(Markdown(f\"### Source code for class `{class_name}`\\n```python\\n{source}\\n```\"))\n",
    "    except ImportError as e:\n",
    "        print(f\"Error importing module {module_name}: {e}\")\n",
    "    except AttributeError:\n",
    "        print(f\"Class {class_name} not found in module {module_name}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing class {class_name}: {e}\")\n",
    "\n",
    "def explore_module(module_name):\n",
    "    \"\"\"Display a comprehensive overview of a module with classes and functions\"\"\"\n",
    "    try:\n",
    "        # Import the module\n",
    "        module = importlib.import_module(module_name)\n",
    "        \n",
    "        # Module docstring\n",
    "        doc = inspect.getdoc(module)\n",
    "        display(Markdown(f\"# Module: {module_name}\"))\n",
    "        \n",
    "        if doc:\n",
    "            display(Markdown(f\"**Module Documentation:**\\n{doc}\"))\n",
    "        else:\n",
    "            display(Markdown(\"*No module documentation available*\"))\n",
    "            \n",
    "        display(Markdown(\"---\"))\n",
    "        \n",
    "        # Display classes\n",
    "        display(Markdown(\"## Classes\"))\n",
    "        display_module_classes(module_name)\n",
    "        \n",
    "        # Display functions\n",
    "        display(Markdown(\"## Functions\"))\n",
    "        display_module_functions(module_name)\n",
    "        \n",
    "    except ImportError as e:\n",
    "        print(f\"Error importing module {module_name}: {e}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error exploring module {module_name}: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check Bonsai Installation\n",
    "\n",
    "Let's verify that the Bonsai v3 module is available for import:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "try:\n",
    "    from bonsaitree import v3\n",
    "    print(\"âœ… Successfully imported Bonsai v3 module\")\n",
    "    \n",
    "    # Print Bonsai version information if available\n",
    "    if hasattr(v3, \"__version__\"):\n",
    "        print(f\"Bonsai v3 version: {v3.__version__}\")\n",
    "    \n",
    "    # List key submodules\n",
    "    print(\"\\nAvailable Bonsai submodules:\")\n",
    "    for module_name in dir(v3):\n",
    "        if not module_name.startswith(\"_\") and not module_name.startswith(\"__\"):\n",
    "            print(f\"- {module_name}\")\n",
    "except ImportError as e:\n",
    "    print(f\"âŒ Failed to import Bonsai v3 module: {e}\")\n",
    "    print(\"This lab requires access to the Bonsai v3 codebase.\")\n",
    "    print(\"Make sure you've properly set up your environment with the Bonsai repository.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "As genetic genealogy datasets grow in size and complexity, computational performance becomes a critical factor in the practical application of tools like Bonsai v3. Large datasets with thousands of individuals and millions of IBD segments can challenge even the most efficiently designed algorithms, requiring careful optimization and performance tuning.\n",
    "\n",
    "In this lab, we'll explore systematic approaches to identifying and addressing performance bottlenecks in Bonsai v3. We'll examine how to profile code to locate inefficiencies, implement algorithmic optimizations to improve computational efficiency, and apply memory optimization techniques to handle large-scale datasets.\n",
    "\n",
    "**Key concepts we'll cover:**\n",
    "- Understanding the computational complexity challenges in genetic genealogy\n",
    "- Applying profiling tools to identify performance bottlenecks\n",
    "- Implementing algorithmic optimizations for core Bonsai functions\n",
    "- Optimizing memory usage for large datasets\n",
    "- Making intelligent precision-performance tradeoffs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Performance Scaling Challenges\n",
    "\n",
    "### Theory and Background\n",
    "\n",
    "Genetic genealogy applications face several distinct performance scaling challenges:\n",
    "\n",
    "1. **Quadratic Growth of Pairwise Comparisons**: The number of potential relationships grows quadratically with the number of individuals. With n individuals, there are n(n-1)/2 possible pairs to analyze. For example:\n",
    "   - 100 individuals: 4,950 pairs\n",
    "   - 1,000 individuals: 499,500 pairs\n",
    "   - 10,000 individuals: 49,995,000 pairs\n",
    "\n",
    "2. **IBD Segment Analysis Complexity**: Each pair of individuals may share multiple IBD segments, and analyzing these segments requires comparing genetic data across multiple positions.\n",
    "\n",
    "3. **Pedigree Structure Optimization**: Finding the optimal pedigree structure that explains observed genetic relationships is a combinatorial problem with factorial growth.\n",
    "\n",
    "4. **Memory Requirements**: Storing genetic data, IBD segments, and relationship information for large datasets can quickly exceed available memory.\n",
    "\n",
    "The computational complexity of key operations in Bonsai v3 can be summarized as follows:\n",
    "\n",
    "| Operation | Time Complexity | Space Complexity | Scaling Factor |\n",
    "|-----------|-----------------|------------------|----------------|\n",
    "| IBD Detection | O(nÂ²) | O(nÂ²) | Number of individuals (n) |\n",
    "| Relationship Inference | O(nÂ²) | O(nÂ²) | Number of individuals (n) |\n",
    "| Pedigree Construction | O(nÂ³) | O(nÂ²) | Number of individuals (n) |\n",
    "| Pedigree Optimization | O(nÂ³) | O(nÂ²) | Number of individuals (n) |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation in Bonsai v3\n",
    "\n",
    "Let's examine how these performance challenges manifest in the Bonsai v3 codebase. We'll focus on understanding the computational bottlenecks in key modules and functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Load the Bonsai modules we'll be examining\n",
    "try:\n",
    "    from bonsaitree.v3 import pwlogl\n",
    "    from bonsaitree.v3 import pedigree\n",
    "    from bonsaitree.v3 import relationships\n",
    "    \n",
    "    print(\"âœ… Successfully imported Bonsai modules for performance analysis\")\n",
    "except ImportError as e:\n",
    "    print(f\"âŒ Failed to import Bonsai modules: {e}\")\n",
    "    print(\"Will proceed with theoretical discussion.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simulating Computation Scaling with Dataset Size\n",
    "\n",
    "Let's create a simple simulation to illustrate how computation time grows with dataset size:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Simulate the computational complexity of different operations\n",
    "def simulate_operation_scaling():\n",
    "    \"\"\"Simulate how different operations scale with dataset size\"\"\"\n",
    "    # Dataset sizes to test\n",
    "    dataset_sizes = [10, 50, 100, 200, 500, 1000]\n",
    "    \n",
    "    # Time units (arbitrary)\n",
    "    linear_times = []\n",
    "    quadratic_times = []\n",
    "    cubic_times = []\n",
    "    \n",
    "    # Base unit of computation (arbitrary constant)\n",
    "    base_unit = 0.001\n",
    "    \n",
    "    # Calculate time for each operation and dataset size\n",
    "    for n in dataset_sizes:\n",
    "        # Linear time complexity: O(n)\n",
    "        linear_times.append(base_unit * n)\n",
    "        \n",
    "        # Quadratic time complexity: O(nÂ²)\n",
    "        quadratic_times.append(base_unit * n * n)\n",
    "        \n",
    "        # Cubic time complexity: O(nÂ³)\n",
    "        cubic_times.append(base_unit * n * n * n)\n",
    "    \n",
    "    # Create a DataFrame for easy display\n",
    "    scaling_df = pd.DataFrame({\n",
    "        'Dataset Size': dataset_sizes,\n",
    "        'Linear (O(n))': linear_times,\n",
    "        'Quadratic (O(nÂ²))': quadratic_times,\n",
    "        'Cubic (O(nÂ³))': cubic_times\n",
    "    })\n",
    "    \n",
    "    # Display the data\n",
    "    display(scaling_df)\n",
    "    \n",
    "    # Visualize the scaling\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    \n",
    "    # Plot each complexity on the same graph\n",
    "    plt.plot(dataset_sizes, linear_times, 'o-', label='Linear (O(n))')\n",
    "    plt.plot(dataset_sizes, quadratic_times, 's-', label='Quadratic (O(nÂ²))')\n",
    "    plt.plot(dataset_sizes, cubic_times, '^-', label='Cubic (O(nÂ³))')\n",
    "    \n",
    "    plt.xlabel('Dataset Size (Number of Individuals)')\n",
    "    plt.ylabel('Computation Time (arbitrary units)')\n",
    "    plt.title('Scaling of Computation Time with Dataset Size')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    \n",
    "    # Use log scale for better visualization\n",
    "    plt.yscale('log')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Create a second plot focusing on the smaller dataset sizes\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    \n",
    "    # Use only the first few dataset sizes for better visibility\n",
    "    small_sizes = dataset_sizes[:4]  # Up to 200 individuals\n",
    "    \n",
    "    plt.plot(small_sizes, linear_times[:4], 'o-', label='Linear (O(n))')\n",
    "    plt.plot(small_sizes, quadratic_times[:4], 's-', label='Quadratic (O(nÂ²))')\n",
    "    plt.plot(small_sizes, cubic_times[:4], '^-', label='Cubic (O(nÂ³))')\n",
    "    \n",
    "    plt.xlabel('Dataset Size (Number of Individuals)')\n",
    "    plt.ylabel('Computation Time (arbitrary units)')\n",
    "    plt.title('Scaling of Computation Time (Small Dataset Sizes)')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Run the simulation\n",
    "simulate_operation_scaling()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1: Identifying Performance Bottlenecks\n",
    "\n",
    "Let's analyze a typical Bonsai v3 workflow to identify potential performance bottlenecks based on computational complexity.\n",
    "\n",
    "**Task:** Examine the code snippets below and identify the potential performance bottlenecks, explaining why they would be problematic for large datasets.\n",
    "\n",
    "**Hint:** Look for nested loops, large data structures, and operations that would scale poorly with dataset size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Example 1: Pairwise relationship likelihood calculation\n",
    "def pairwise_relationship_likelihood_bottleneck(all_individuals, segments_dict):\n",
    "    \"\"\"Example of a pairwise calculation that would be a bottleneck\"\"\"\n",
    "    # This is a simplified example for illustration purposes\n",
    "    n_individuals = len(all_individuals)\n",
    "    relationship_matrix = np.zeros((n_individuals, n_individuals))\n",
    "    \n",
    "    for i in range(n_individuals):\n",
    "        for j in range(i+1, n_individuals):\n",
    "            id1 = all_individuals[i]\n",
    "            id2 = all_individuals[j]\n",
    "            \n",
    "            # Get shared segments\n",
    "            pair_key = frozenset([id1, id2])\n",
    "            if pair_key not in segments_dict:\n",
    "                continue\n",
    "                \n",
    "            segments = segments_dict[pair_key]\n",
    "            \n",
    "            # Calculate likelihoods for various relationships\n",
    "            # This would involve multiple calculations per segment\n",
    "            relationship_matrix[i, j] = calculate_max_likelihood(segments)\n",
    "            relationship_matrix[j, i] = relationship_matrix[i, j]  # Symmetric\n",
    "    \n",
    "    return relationship_matrix\n",
    "\n",
    "def calculate_max_likelihood(segments):\n",
    "    \"\"\"Placeholder for likelihood calculation\"\"\"\n",
    "    # In a real implementation, this would calculate likelihoods\n",
    "    # for multiple relationship types\n",
    "    return len(segments) * 0.01\n",
    "\n",
    "# Example 2: Optimizing pedigree structure\n",
    "def optimize_pedigree_bottleneck(pedigree, all_individuals, relationship_scores):\n",
    "    \"\"\"Example of a pedigree optimization that would be a bottleneck\"\"\"\n",
    "    # Simplified example for illustration\n",
    "    best_score = compute_pedigree_score(pedigree, relationship_scores)\n",
    "    \n",
    "    # Try all possible individual swaps to improve the pedigree\n",
    "    improved = True\n",
    "    while improved:\n",
    "        improved = False\n",
    "        \n",
    "        for i in range(len(all_individuals)):\n",
    "            for j in range(i+1, len(all_individuals)):\n",
    "                # Try swapping positions of individuals i and j\n",
    "                new_pedigree = swap_individuals(pedigree, i, j)\n",
    "                new_score = compute_pedigree_score(new_pedigree, relationship_scores)\n",
    "                \n",
    "                if new_score > best_score:\n",
    "                    pedigree = new_pedigree\n",
    "                    best_score = new_score\n",
    "                    improved = True\n",
    "    \n",
    "    return pedigree\n",
    "\n",
    "def compute_pedigree_score(pedigree, relationship_scores):\n",
    "    \"\"\"Placeholder for pedigree scoring\"\"\"\n",
    "    # In a real implementation, this would evaluate the pedigree quality\n",
    "    return sum(relationship_scores.values())\n",
    "\n",
    "def swap_individuals(pedigree, i, j):\n",
    "    \"\"\"Placeholder for individual swapping\"\"\"\n",
    "    # In a real implementation, this would create a new pedigree\n",
    "    # with individuals i and j swapped\n",
    "    return pedigree.copy()\n",
    "\n",
    "# Example 3: Memory-intensive segment handling\n",
    "def process_all_segments_bottleneck(segment_data, genetic_map):\n",
    "    \"\"\"Example of memory-intensive segment processing\"\"\"\n",
    "    # Simplified example for illustration\n",
    "    all_segments = []\n",
    "    \n",
    "    # Load all segment data into memory\n",
    "    for segment in segment_data:\n",
    "        # Enhance segment with additional data\n",
    "        segment['genetic_distance'] = lookup_genetic_distance(segment, genetic_map)\n",
    "        segment['shared_snps'] = calculate_shared_snps(segment)\n",
    "        \n",
    "        all_segments.append(segment)\n",
    "    \n",
    "    # Process all segments\n",
    "    results = []\n",
    "    for segment in all_segments:\n",
    "        # Multiple calculations per segment\n",
    "        results.append(analyze_segment(segment))\n",
    "    \n",
    "    return results\n",
    "\n",
    "def lookup_genetic_distance(segment, genetic_map):\n",
    "    \"\"\"Placeholder for genetic distance lookup\"\"\"\n",
    "    return segment['end_pos'] - segment['start_pos'] / 1_000_000\n",
    "\n",
    "def calculate_shared_snps(segment):\n",
    "    \"\"\"Placeholder for SNP counting\"\"\"\n",
    "    return (segment['end_pos'] - segment['start_pos']) // 1000\n",
    "\n",
    "def analyze_segment(segment):\n",
    "    \"\"\"Placeholder for segment analysis\"\"\"\n",
    "    return {'length': segment['genetic_distance'], 'quality': segment['shared_snps'] / 100}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bottleneck Analysis\n",
    "\n",
    "For each example, identify the performance bottlenecks and explain why they would be problematic for large datasets. Consider both time and memory complexity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Your Analysis:\n",
    "\n",
    "**Example 1: Pairwise relationship likelihood calculation**\n",
    "- Bottleneck 1:\n",
    "\n",
    "\n",
    "**Example 2: Optimizing pedigree structure**\n",
    "- Bottleneck 1:\n",
    "\n",
    "\n",
    "**Example 3: Memory-intensive segment handling**\n",
    "- Bottleneck 1:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Profiling and Benchmarking Methodology\n",
    "\n",
    "### Theory and Background\n",
    "\n",
    "Profiling is the process of systematically measuring the performance characteristics of code to identify bottlenecks and inefficiencies. Effective profiling helps focus optimization efforts on the parts of the code that will yield the greatest improvements.\n",
    "\n",
    "Key profiling metrics include:\n",
    "\n",
    "1. **Time Profiling**:\n",
    "   - Function call counts and time spent in each function\n",
    "   - Line-by-line execution time \n",
    "   - Call graph analysis to understand the call stack\n",
    "\n",
    "2. **Memory Profiling**:\n",
    "   - Memory allocation patterns\n",
    "   - Peak memory usage\n",
    "   - Object lifetime and reference patterns\n",
    "\n",
    "3. **I/O Profiling**:\n",
    "   - Disk read/write operations\n",
    "   - Network traffic\n",
    "   - Database queries\n",
    "\n",
    "Python provides several powerful profiling tools:\n",
    "\n",
    "1. **cProfile**: A built-in deterministic profiler that measures function call times\n",
    "2. **memory_profiler**: A package that measures line-by-line memory usage\n",
    "3. **line_profiler**: A package that provides line-by-line time profiling\n",
    "4. **Scalene**: A high-performance CPU and memory profiler\n",
    "5. **PyInstrument**: A low-overhead call stack profiler\n",
    "\n",
    "Let's explore how to use these tools to profile Bonsai v3 code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation in Bonsai v3\n",
    "\n",
    "We'll now look at how to apply profiling tools to understand the performance characteristics of Bonsai v3 functions. Let's start with a simple example using cProfile."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Define a wrapper for cProfile to easily profile functions\n",
    "def profile_function(func, *args, **kwargs):\n",
    "    \"\"\"Profile a function using cProfile and display sorted results.\n",
    "    \n",
    "    Args:\n",
    "        func: The function to profile\n",
    "        *args, **kwargs: Arguments to pass to the function\n",
    "        \n",
    "    Returns:\n",
    "        The result of the function call\n",
    "    \"\"\"\n",
    "    profiler = cProfile.Profile()\n",
    "    profiler.enable()\n",
    "    \n",
    "    result = func(*args, **kwargs)\n",
    "    \n",
    "    profiler.disable()\n",
    "    s = StringIO()\n",
    "    ps = pstats.Stats(profiler, stream=s).sort_stats('cumtime')\n",
    "    ps.print_stats(20)  # Print top 20 functions by cumulative time\n",
    "    \n",
    "    print(s.getvalue())\n",
    "    return result\n",
    "\n",
    "# Define a sample function to profile (simulating a Bonsai operation)\n",
    "def sample_bonsai_operation(n_individuals=100, n_segments=500):\n",
    "    \"\"\"Simulate a complex Bonsai operation for profiling purposes.\n",
    "    \n",
    "    Args:\n",
    "        n_individuals: Number of individuals to simulate\n",
    "        n_segments: Number of segments per individual pair\n",
    "        \n",
    "    Returns:\n",
    "        A dictionary of results\n",
    "    \"\"\"\n",
    "    # Create synthetic data\n",
    "    individuals = [f\"ind_{i}\" for i in range(n_individuals)]\n",
    "    \n",
    "    # Create segments (simplified)\n",
    "    segments_dict = {}\n",
    "    for i in range(n_individuals):\n",
    "        for j in range(i+1, n_individuals):\n",
    "            id1 = individuals[i]\n",
    "            id2 = individuals[j]\n",
    "            \n",
    "            # Generate random segments for this pair\n",
    "            pair_segments = []\n",
    "            for _ in range(np.random.poisson(n_segments / 10)):\n",
    "                chr_num = np.random.randint(1, 23)\n",
    "                start = np.random.randint(1, 200_000_000)\n",
    "                length = np.random.exponential(5_000_000)\n",
    "                end = start + length\n",
    "                \n",
    "                segment = {\n",
    "                    'chr': chr_num,\n",
    "                    'start_pos': start,\n",
    "                    'end_pos': end,\n",
    "                    'cm_length': length / 1_000_000\n",
    "                }\n",
    "                pair_segments.append(segment)\n",
    "            \n",
    "            if pair_segments:\n",
    "                segments_dict[frozenset([id1, id2])] = pair_segments\n",
    "    \n",
    "    # Calculate pairwise statistics (intentionally inefficient for demonstration)\n",
    "    pair_stats = {}\n",
    "    for pair, segments in segments_dict.items():\n",
    "        # Calculate total segments and length\n",
    "        total_length = sum(seg['cm_length'] for seg in segments)\n",
    "        \n",
    "        # Perform an expensive operation (simulating likelihood calculation)\n",
    "        likelihoods = {}\n",
    "        for rel_type in ['parent-child', 'full-sibling', 'half-sibling', 'first-cousin']:\n",
    "            # Simulate relationship likelihood calculation\n",
    "            likelihoods[rel_type] = expensive_calculation(segments, rel_type)\n",
    "        \n",
    "        pair_stats[pair] = {\n",
    "            'total_segments': len(segments),\n",
    "            'total_length': total_length,\n",
    "            'likelihoods': likelihoods\n",
    "        }\n",
    "    \n",
    "    return {\n",
    "        'n_individuals': n_individuals,\n",
    "        'n_pairs': len(segments_dict),\n",
    "        'pair_stats': pair_stats\n",
    "    }\n",
    "\n",
    "def expensive_calculation(segments, rel_type):\n",
    "    \"\"\"Simulate an expensive calculation for profiling purposes.\n",
    "    \n",
    "    Args:\n",
    "        segments: List of segment dictionaries\n",
    "        rel_type: Relationship type\n",
    "        \n",
    "    Returns:\n",
    "        A likelihood value\n",
    "    \"\"\"\n",
    "    # Make this function artificially slow for demonstration\n",
    "    result = 0\n",
    "    for segment in segments:\n",
    "        # Simulate complex calculation\n",
    "        for _ in range(100):\n",
    "            result += np.sin(segment['cm_length']) * np.cos(segment['start_pos'] / 1e6)\n",
    "            if rel_type == 'parent-child':\n",
    "                result *= 1.01\n",
    "            elif rel_type == 'full-sibling':\n",
    "                result *= 0.99\n",
    "                \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Profile our sample operation with a small dataset\n",
    "print(\"Profiling sample Bonsai operation with 20 individuals:\")\n",
    "result = profile_function(sample_bonsai_operation, n_individuals=20, n_segments=100)\n",
    "\n",
    "print(f\"\\nProcessed {result['n_individuals']} individuals and {result['n_pairs']} pairs.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Memory Profiling Example\n",
    "\n",
    "Now let's demonstrate how to use memory profiling to track memory usage:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Memory profiling example\n",
    "@memory_profiler.profile\n",
    "def memory_intensive_operation(n_size=1000):\n",
    "    \"\"\"A function that demonstrates memory usage patterns.\n",
    "    \n",
    "    Args:\n",
    "        n_size: Size parameter controlling memory usage\n",
    "        \n",
    "    Returns:\n",
    "        Sum of all values created\n",
    "    \"\"\"\n",
    "    # Create a large list\n",
    "    print(\"Creating large list...\")\n",
    "    large_list = [i * 2 for i in range(n_size * 1000)]\n",
    "    \n",
    "    # Create a large dictionary\n",
    "    print(\"Creating large dictionary...\")\n",
    "    large_dict = {i: np.random.random(100) for i in range(n_size)}\n",
    "    \n",
    "    # Create a large numpy array\n",
    "    print(\"Creating large numpy array...\")\n",
    "    large_array = np.random.random((n_size, n_size))\n",
    "    \n",
    "    # Calculate something using all the structures\n",
    "    print(\"Performing calculations...\")\n",
    "    result = sum(large_list) + sum(sum(values) for values in large_dict.values()) + np.sum(large_array)\n",
    "    \n",
    "    # Clean up to reduce memory (demonstrate memory release)\n",
    "    print(\"Cleaning up...\")\n",
    "    del large_list\n",
    "    del large_dict\n",
    "    del large_array\n",
    "    \n",
    "    return result\n",
    "\n",
    "# Run the memory-intensive operation\n",
    "print(\"Running memory-intensive operation...\")\n",
    "result = memory_intensive_operation(500)\n",
    "print(f\"Operation result: {result}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2: Building a Benchmark Framework\n",
    "\n",
    "Benchmarking is the process of measuring the performance of code under controlled conditions to establish baseline metrics and track improvements. Let's create a simple benchmarking framework for Bonsai operations.\n",
    "\n",
    "**Task:** Complete the benchmark framework below to measure the performance of different operations across varying dataset sizes.\n",
    "\n",
    "**Hint:** Focus on tracking both time and memory usage for each operation and dataset size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Exercise 2: Complete the benchmark framework\n",
    "class BonsaiBenchmark:\n",
    "    \"\"\"Framework for benchmarking Bonsai operations.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        \"\"\"Initialize the benchmark framework.\"\"\"\n",
    "        self.results = {}\n",
    "    \n",
    "    def benchmark_operation(self, operation_func, dataset_sizes, repeat=3, **kwargs):\n",
    "        \"\"\"Benchmark an operation across multiple dataset sizes.\n",
    "        \n",
    "        Args:\n",
    "            operation_func: Function to benchmark\n",
    "            dataset_sizes: List of dataset sizes to test\n",
    "            repeat: Number of times to repeat each benchmark\n",
    "            **kwargs: Additional arguments to pass to the operation function\n",
    "            \n",
    "        Returns:\n",
    "            DataFrame with benchmark results\n",
    "        \"\"\"\n",
    "        # TODO: Implement benchmarking logic\n",
    "        # 1. For each dataset size, run the operation 'repeat' times\n",
    "        # 2. Measure execution time for each run\n",
    "        # 3. Track memory usage using memory_profiler (optional, can be challenging)\n",
    "        # 4. Record and return results\n",
    "        \n",
    "        operation_name = operation_func.__name__\n",
    "        results = []\n",
    "        \n",
    "        print(f\"Benchmarking {operation_name}...\")\n",
    "        \n",
    "        for size in dataset_sizes:\n",
    "            print(f\"  Dataset size: {size}\")\n",
    "            \n",
    "            # Run multiple times for reliable timing\n",
    "            run_times = []\n",
    "            for i in range(repeat):\n",
    "                # Measure execution time\n",
    "                start_time = time.time()\n",
    "                operation_func(n_individuals=size, **kwargs)\n",
    "                end_time = time.time()\n",
    "                \n",
    "                execution_time = end_time - start_time\n",
    "                run_times.append(execution_time)\n",
    "                print(f\"    Run {i+1}/{repeat}: {execution_time:.4f} seconds\")\n",
    "            \n",
    "            # Calculate statistics\n",
    "            avg_time = np.mean(run_times)\n",
    "            min_time = np.min(run_times)\n",
    "            max_time = np.max(run_times)\n",
    "            std_time = np.std(run_times)\n",
    "            \n",
    "            # Store results\n",
    "            results.append({\n",
    "                'Operation': operation_name,\n",
    "                'Dataset Size': size,\n",
    "                'Average Time (s)': avg_time,\n",
    "                'Min Time (s)': min_time,\n",
    "                'Max Time (s)': max_time,\n",
    "                'Std Dev (s)': std_time\n",
    "            })\n",
    "        \n",
    "        # Create a DataFrame with results\n",
    "        results_df = pd.DataFrame(results)\n",
    "        self.results[operation_name] = results_df\n",
    "        \n",
    "        return results_df\n",
    "    \n",
    "    def plot_results(self, operation_name=None, log_scale=True):\n",
    "        \"\"\"Plot benchmark results.\n",
    "        \n",
    "        Args:\n",
    "            operation_name: Name of operation to plot (if None, plot all)\n",
    "            log_scale: Whether to use logarithmic scale for time axis\n",
    "        \"\"\"\n",
    "        # TODO: Implement plotting logic\n",
    "        # 1. Create a plot showing execution time vs dataset size\n",
    "        # 2. Include error bars for timing variability\n",
    "        # 3. If multiple operations are being compared, show them on the same plot\n",
    "        \n",
    "        plt.figure(figsize=(10, 6))\n",
    "        \n",
    "        if operation_name is not None and operation_name in self.results:\n",
    "            # Plot specific operation\n",
    "            df = self.results[operation_name]\n",
    "            plt.errorbar(\n",
    "                df['Dataset Size'], \n",
    "                df['Average Time (s)'], \n",
    "                yerr=df['Std Dev (s)'],\n",
    "                marker='o',\n",
    "                label=operation_name\n",
    "            )\n",
    "        else:\n",
    "            # Plot all operations\n",
    "            for op_name, df in self.results.items():\n",
    "                plt.errorbar(\n",
    "                    df['Dataset Size'], \n",
    "                    df['Average Time (s)'], \n",
    "                    yerr=df['Std Dev (s)'],\n",
    "                    marker='o',\n",
    "                    label=op_name\n",
    "                )\n",
    "        \n",
    "        plt.xlabel('Dataset Size (Number of Individuals)')\n",
    "        plt.ylabel('Execution Time (seconds)')\n",
    "        plt.title('Benchmark Results: Execution Time vs Dataset Size')\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        plt.legend()\n",
    "        \n",
    "        if log_scale:\n",
    "            plt.yscale('log')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "    def compare_operations(self, operations, dataset_size, repeat=3, **kwargs):\n",
    "        \"\"\"Compare multiple operations on the same dataset size.\n",
    "        \n",
    "        Args:\n",
    "            operations: List of operation functions to compare\n",
    "            dataset_size: Size of dataset to use\n",
    "            repeat: Number of times to repeat each benchmark\n",
    "            **kwargs: Additional arguments to pass to the operation functions\n",
    "            \n",
    "        Returns:\n",
    "            DataFrame with comparison results\n",
    "        \"\"\"\n",
    "        # TODO: Implement comparison logic\n",
    "        # 1. Run each operation on the same dataset size\n",
    "        # 2. Compare execution times\n",
    "        # 3. Return and visualize results\n",
    "        \n",
    "        comparison_results = []\n",
    "        \n",
    "        print(f\"Comparing operations on dataset size {dataset_size}...\")\n",
    "        \n",
    "        for operation_func in operations:\n",
    "            operation_name = operation_func.__name__\n",
    "            print(f\"  Running {operation_name}...\")\n",
    "            \n",
    "            # Run multiple times for reliable timing\n",
    "            run_times = []\n",
    "            for i in range(repeat):\n",
    "                # Measure execution time\n",
    "                start_time = time.time()\n",
    "                operation_func(n_individuals=dataset_size, **kwargs)\n",
    "                end_time = time.time()\n",
    "                \n",
    "                execution_time = end_time - start_time\n",
    "                run_times.append(execution_time)\n",
    "                print(f\"    Run {i+1}/{repeat}: {execution_time:.4f} seconds\")\n",
    "            \n",
    "            # Calculate statistics\n",
    "            avg_time = np.mean(run_times)\n",
    "            min_time = np.min(run_times)\n",
    "            max_time = np.max(run_times)\n",
    "            std_time = np.std(run_times)\n",
    "            \n",
    "            # Store results\n",
    "            comparison_results.append({\n",
    "                'Operation': operation_name,\n",
    "                'Dataset Size': dataset_size,\n",
    "                'Average Time (s)': avg_time,\n",
    "                'Min Time (s)': min_time,\n",
    "                'Max Time (s)': max_time,\n",
    "                'Std Dev (s)': std_time\n",
    "            })\n",
    "        \n",
    "        # Create a DataFrame with results\n",
    "        comparison_df = pd.DataFrame(comparison_results)\n",
    "        \n",
    "        # Visualize the comparison\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.bar(\n",
    "            comparison_df['Operation'],\n",
    "            comparison_df['Average Time (s)'],\n",
    "            yerr=comparison_df['Std Dev (s)'],\n",
    "            capsize=5\n",
    "        )\n",
    "        plt.xlabel('Operation')\n",
    "        plt.ylabel('Execution Time (seconds)')\n",
    "        plt.title(f'Operation Performance Comparison (Dataset Size: {dataset_size})')\n",
    "        plt.xticks(rotation=45, ha='right')\n",
    "        plt.grid(True, axis='y', alpha=0.3)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        return comparison_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Test the benchmark framework with our sample operation\n",
    "benchmark = BonsaiBenchmark()\n",
    "\n",
    "# Define a variant of our sample operation with different characteristics\n",
    "def optimized_bonsai_operation(n_individuals=100, n_segments=500):\n",
    "    \"\"\"A more optimized version of the sample operation.\"\"\"\n",
    "    # Simplified implementation with better performance characteristics\n",
    "    # (Still slow enough to demonstrate benchmarking)\n",
    "    individuals = [f\"ind_{i}\" for i in range(n_individuals)]\n",
    "    \n",
    "    # Pre-compute some values to avoid redundant calculations\n",
    "    rel_types = ['parent-child', 'full-sibling', 'half-sibling', 'first-cousin']\n",
    "    rel_factors = {'parent-child': 1.01, 'full-sibling': 0.99, 'half-sibling': 1.0, 'first-cousin': 0.98}\n",
    "    \n",
    "    # Create segments more efficiently\n",
    "    segments_dict = {}\n",
    "    for i in range(n_individuals):\n",
    "        for j in range(i+1, n_individuals):\n",
    "            id1 = individuals[i]\n",
    "            id2 = individuals[j]\n",
    "            \n",
    "            n_pair_segments = np.random.poisson(n_segments / 10)\n",
    "            if n_pair_segments > 0:\n",
    "                # Generate all segments at once instead of in a loop\n",
    "                chr_nums = np.random.randint(1, 23, n_pair_segments)\n",
    "                starts = np.random.randint(1, 200_000_000, n_pair_segments)\n",
    "                lengths = np.random.exponential(5_000_000, n_pair_segments)\n",
    "                ends = starts + lengths\n",
    "                cm_lengths = lengths / 1_000_000\n",
    "                \n",
    "                pair_segments = [\n",
    "                    {\n",
    "                        'chr': chr_nums[k],\n",
    "                        'start_pos': starts[k],\n",
    "                        'end_pos': ends[k],\n",
    "                        'cm_length': cm_lengths[k]\n",
    "                    }\n",
    "                    for k in range(n_pair_segments)\n",
    "                ]\n",
    "                \n",
    "                segments_dict[frozenset([id1, id2])] = pair_segments\n",
    "    \n",
    "    # Calculate pairwise statistics more efficiently\n",
    "    pair_stats = {}\n",
    "    for pair, segments in segments_dict.items():\n",
    "        # Calculate total length once\n",
    "        total_length = sum(seg['cm_length'] for seg in segments)\n",
    "        \n",
    "        # Pre-compute values shared across relationship calculations\n",
    "        segment_values = np.array([np.sin(seg['cm_length']) * np.cos(seg['start_pos'] / 1e6) for seg in segments])\n",
    "        base_likelihood = np.sum(segment_values) * 100\n",
    "        \n",
    "        # Calculate likelihoods for all relationship types at once\n",
    "        likelihoods = {rel_type: base_likelihood * rel_factors[rel_type] for rel_type in rel_types}\n",
    "        \n",
    "        pair_stats[pair] = {\n",
    "            'total_segments': len(segments),\n",
    "            'total_length': total_length,\n",
    "            'likelihoods': likelihoods\n",
    "        }\n",
    "    \n",
    "    return {\n",
    "        'n_individuals': n_individuals,\n",
    "        'n_pairs': len(segments_dict),\n",
    "        'pair_stats': pair_stats\n",
    "    }\n",
    "\n",
    "# Run the benchmark\n",
    "dataset_sizes = [10, 20, 50, 100]\n",
    "results_original = benchmark.benchmark_operation(sample_bonsai_operation, dataset_sizes, repeat=2, n_segments=100)\n",
    "results_optimized = benchmark.benchmark_operation(optimized_bonsai_operation, dataset_sizes, repeat=2, n_segments=100)\n",
    "\n",
    "# Plot the results\n",
    "benchmark.plot_results()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "<cell_type>markdown</cell_type>## Part 3: Algorithmic Optimization Strategies\n\n### Theory and Background\n\nAlgorithmic optimization is often the most effective approach for improving performance in computational genetics applications. By reducing the computational complexity of key operations, we can achieve significant speedups that scale well with dataset size.\n\nKey algorithmic optimization strategies include:\n\n1. **Early Termination**: Stopping computations as soon as a conclusive result is reached\n2. **Pruning**: Eliminating branches of computation that cannot lead to optimal solutions\n3. **Memoization**: Caching results of expensive function calls\n4. **Precomputation**: Calculating and storing values that will be needed multiple times\n5. **Approximation**: Using faster, approximate methods when exact solutions are not required\n6. **Divide and Conquer**: Breaking problems into smaller, more manageable subproblems\n\nLet's explore how these strategies can be applied to Bonsai v3.",
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "<cell_type>markdown</cell_type>### Implementation in Bonsai v3\n\nLet's examine and optimize some of the computationally intensive functions in Bonsai v3. We'll focus on the pairwise likelihood calculations, which are often a major bottleneck in relationship inference.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Example: Optimizing the relationship likelihood calculation\ndef original_compute_likelihoods(segments, relationship_models):\n    \"\"\"Original (unoptimized) function to compute relationship likelihoods.\n    \n    Args:\n        segments: List of IBD segments shared between two individuals\n        relationship_models: Dictionary of relationship models\n        \n    Returns:\n        Dictionary of relationship likelihoods\n    \"\"\"\n    likelihoods = {}\n    \n    for rel_type, model in relationship_models.items():\n        # Compute likelihood for each relationship type\n        likelihood = 0\n        \n        # Process each segment\n        for segment in segments:\n            # Expensive calculation for each segment\n            segment_likelihood = compute_segment_likelihood(segment, model)\n            likelihood += segment_likelihood\n        \n        likelihoods[rel_type] = likelihood\n    \n    return likelihoods\n\ndef optimized_compute_likelihoods(segments, relationship_models, early_termination_threshold=0.01):\n    \"\"\"Optimized function to compute relationship likelihoods with several optimizations.\n    \n    Args:\n        segments: List of IBD segments shared between two individuals\n        relationship_models: Dictionary of relationship models\n        early_termination_threshold: Threshold for early termination\n        \n    Returns:\n        Dictionary of relationship likelihoods\n    \"\"\"\n    likelihoods = {}\n    \n    # Optimization 1: Precomputation\n    # Precompute segment features that will be used by all relationship models\n    segment_features = []\n    for segment in segments:\n        features = precompute_segment_features(segment)\n        segment_features.append(features)\n    \n    # Optimization 2: Early termination based on segment count\n    if len(segments) == 0:\n        # If no segments, set all likelihoods to minimum value\n        return {rel_type: float('-inf') for rel_type in relationship_models}\n    \n    # Optimization 3: Sort relationships by computational cost\n    # Process cheaper models first to allow for earlier filtering\n    rel_types = sorted(relationship_models.keys(), \n                      key=lambda r: relationship_computation_cost(r))\n    \n    # Optimization 4: Track best likelihood for early termination\n    best_likelihood = float('-inf')\n    best_rel_type = None\n    \n    for rel_type in rel_types:\n        model = relationship_models[rel_type]\n        \n        # Skip unlikely relationships based on segment count heuristic\n        if skip_unlikely_relationship(rel_type, len(segments)):\n            likelihoods[rel_type] = float('-inf')\n            continue\n        \n        # Compute likelihood using precomputed features\n        likelihood = 0\n        for features in segment_features:\n            segment_likelihood = compute_segment_likelihood_from_features(features, model)\n            likelihood += segment_likelihood\n            \n            # Optimization 5: Early termination within segment processing\n            # If this relationship is already much worse than the best, stop computing\n            if best_likelihood - likelihood > early_termination_threshold * len(segments):\n                likelihood = float('-inf')\n                break\n        \n        likelihoods[rel_type] = likelihood\n        \n        # Update best likelihood for early termination check\n        if likelihood > best_likelihood:\n            best_likelihood = likelihood\n            best_rel_type = rel_type\n    \n    return likelihoods\n\n# Helper functions\ndef precompute_segment_features(segment):\n    \"\"\"Precompute features for a segment that will be used by multiple relationship models.\"\"\"\n    # Simulate an expensive computation\n    length_feature = segment.get('cm_length', 0)\n    position_feature = (segment.get('end_pos', 0) - segment.get('start_pos', 0)) / 1e6\n    density_feature = segment.get('snp_count', 100) / position_feature if position_feature > 0 else 0\n    \n    return {\n        'length': length_feature,\n        'position': position_feature,\n        'density': density_feature\n    }\n\ndef compute_segment_likelihood(segment, model):\n    \"\"\"Compute the likelihood of a segment under a relationship model.\"\"\"\n    # Simulate the original expensive computation without precomputation\n    length = segment.get('cm_length', 0)\n    position = (segment.get('end_pos', 0) - segment.get('start_pos', 0)) / 1e6\n    density = segment.get('snp_count', 100) / position if position > 0 else 0\n    \n    # Complex calculation based on the model\n    return model.get('weight', 1.0) * (length * 0.1 + position * 0.01 + density * 0.001)\n\ndef compute_segment_likelihood_from_features(features, model):\n    \"\"\"Compute the likelihood of a segment using precomputed features.\"\"\"\n    # Same calculation but using precomputed features\n    return model.get('weight', 1.0) * (features['length'] * 0.1 + \n                                     features['position'] * 0.01 + \n                                     features['density'] * 0.001)\n\ndef relationship_computation_cost(rel_type):\n    \"\"\"Estimate the computational cost of a relationship type.\"\"\"\n    # In a real implementation, this would depend on the complexity of the model\n    cost_map = {\n        'unrelated': 1,\n        'parent-child': 2,\n        'full-sibling': 3,\n        'half-sibling': 4,\n        'first-cousin': 5,\n        'second-cousin': 6\n    }\n    return cost_map.get(rel_type, 10)  # Default for unknown relationships\n\ndef skip_unlikely_relationship(rel_type, segment_count):\n    \"\"\"Determine if a relationship is unlikely based on segment count.\"\"\"\n    # Simple heuristic: different relationships have expected segment count ranges\n    if rel_type == 'parent-child' and segment_count < 10:\n        return True\n    if rel_type == 'full-sibling' and segment_count < 5:\n        return True\n    if rel_type == 'unrelated' and segment_count > 15:\n        return True\n    return False",
   "metadata": {},
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Test the original and optimized functions with a benchmark\ndef benchmark_likelihood_computation(n_segments=50, n_trials=10):\n    \"\"\"Compare the performance of original and optimized likelihood computation.\"\"\"\n    # Create test data\n    segments = []\n    for i in range(n_segments):\n        segment = {\n            'chr': np.random.randint(1, 23),\n            'start_pos': np.random.randint(1, 200_000_000),\n            'end_pos': np.random.randint(200_000_001, 250_000_000),\n            'cm_length': np.random.uniform(1, 20),\n            'snp_count': np.random.randint(100, 10000)\n        }\n        segments.append(segment)\n    \n    # Create relationship models\n    relationship_models = {\n        'parent-child': {'weight': 1.2, 'params': {'a': 0.5, 'b': 0.3}},\n        'full-sibling': {'weight': 1.1, 'params': {'a': 0.4, 'b': 0.4}},\n        'half-sibling': {'weight': 1.0, 'params': {'a': 0.3, 'b': 0.5}},\n        'first-cousin': {'weight': 0.9, 'params': {'a': 0.2, 'b': 0.6}},\n        'second-cousin': {'weight': 0.8, 'params': {'a': 0.1, 'b': 0.7}},\n        'unrelated': {'weight': 0.5, 'params': {'a': 0.0, 'b': 1.0}}\n    }\n    \n    # Benchmark the original function\n    original_times = []\n    for i in range(n_trials):\n        start_time = time.time()\n        original_likelihoods = original_compute_likelihoods(segments, relationship_models)\n        end_time = time.time()\n        original_times.append(end_time - start_time)\n    \n    # Benchmark the optimized function\n    optimized_times = []\n    for i in range(n_trials):\n        start_time = time.time()\n        optimized_likelihoods = optimized_compute_likelihoods(segments, relationship_models)\n        end_time = time.time()\n        optimized_times.append(end_time - start_time)\n    \n    # Display the results\n    original_avg = np.mean(original_times)\n    optimized_avg = np.mean(optimized_times)\n    speedup = original_avg / optimized_avg if optimized_avg > 0 else float('inf')\n    \n    print(f\"Performance comparison with {n_segments} segments:\")\n    print(f\"  Original: {original_avg:.6f} seconds (avg)\")\n    print(f\"  Optimized: {optimized_avg:.6f} seconds (avg)\")\n    print(f\"  Speedup: {speedup:.2f}x\")\n    \n    # Compare the actual likelihoods to ensure correctness\n    original_likelihoods = original_compute_likelihoods(segments, relationship_models)\n    optimized_likelihoods = optimized_compute_likelihoods(segments, relationship_models)\n    \n    print(\"\\nLikelihood comparison (to verify correctness):\")\n    for rel in relationship_models:\n        if rel in original_likelihoods and rel in optimized_likelihoods:\n            # Infinite values may differ, but that's okay\n            if original_likelihoods[rel] == float('-inf') and optimized_likelihoods[rel] == float('-inf'):\n                print(f\"  {rel}: Both methods return -inf\")\n            elif original_likelihoods[rel] == float('-inf') or optimized_likelihoods[rel] == float('-inf'):\n                print(f\"  {rel}: DIFFERENT! Original: {original_likelihoods[rel]}, Optimized: {optimized_likelihoods[rel]}\")\n            else:\n                diff = abs(original_likelihoods[rel] - optimized_likelihoods[rel])\n                rel_diff = diff / abs(original_likelihoods[rel]) if original_likelihoods[rel] != 0 else float('inf')\n                print(f\"  {rel}: Original: {original_likelihoods[rel]:.4f}, Optimized: {optimized_likelihoods[rel]:.4f}, Diff: {rel_diff:.6f}\")\n        else:\n            print(f\"  {rel}: Missing from one of the results!\")\n    \n    # Create a dictionary to return benchmark results\n    benchmark_results = {\n        'n_segments': n_segments,\n        'n_trials': n_trials,\n        'original_avg': original_avg,\n        'optimized_avg': optimized_avg,\n        'speedup': speedup,\n        'original_times': original_times,\n        'optimized_times': optimized_times\n    }\n    \n    return benchmark_results\n\n# Test with different numbers of segments\nbenchmark_results = {}\nfor n_segments in [10, 50, 100, 200]:\n    print(f\"\\nBenchmarking with {n_segments} segments...\")\n    benchmark_results[n_segments] = benchmark_likelihood_computation(n_segments)\n\n# Visualize the scaling with number of segments\nplt.figure(figsize=(10, 6))\n\nsegment_sizes = list(benchmark_results.keys())\noriginal_times = [benchmark_results[n]['original_avg'] for n in segment_sizes]\noptimized_times = [benchmark_results[n]['optimized_avg'] for n in segment_sizes]\n\nplt.plot(segment_sizes, original_times, 'o-', label='Original Implementation')\nplt.plot(segment_sizes, optimized_times, 's-', label='Optimized Implementation')\n\nplt.xlabel('Number of Segments')\nplt.ylabel('Execution Time (seconds)')\nplt.title('Performance Scaling with Number of Segments')\nplt.legend()\nplt.grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n\n# Plot the speedup\nplt.figure(figsize=(10, 6))\n\nspeedups = [benchmark_results[n]['speedup'] for n in segment_sizes]\n\nplt.plot(segment_sizes, speedups, 'o-')\nplt.axhline(y=1, color='r', linestyle='--', alpha=0.3, label='No Speedup')\n\nplt.xlabel('Number of Segments')\nplt.ylabel('Speedup Factor (Original Time / Optimized Time)')\nplt.title('Performance Speedup with Optimized Implementation')\nplt.grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()",
   "metadata": {},
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "<cell_type>markdown</cell_type>### Exercise 3: Implementing a Memoization Optimization\n\nMemoization is a powerful optimization technique that caches the results of expensive function calls to avoid redundant calculations. Let's implement a memoization decorator for Bonsai calculations.\n\n**Task:** Complete the memoization decorator and apply it to an expensive function that would benefit from caching.\n\n**Hint:** Use a dictionary to store function results based on input arguments, and ensure proper handling of mutable arguments.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Exercise 3: Implement a memoization decorator\nimport functools\n\ndef memoize(func):\n    \"\"\"Memoization decorator to cache function results.\n    \n    Args:\n        func: Function to be memoized\n        \n    Returns:\n        Wrapped function with caching\n    \"\"\"\n    # TODO: Implement the memoization decorator\n    # 1. Create a cache to store function results\n    # 2. Create a wrapper function that checks the cache before computing\n    # 3. Store results in the cache after computation\n    # 4. Handle the case of mutable arguments\n    \n    # Create cache\n    cache = {}\n    \n    @functools.wraps(func)\n    def wrapper(*args, **kwargs):\n        # Create a key for the cache\n        # For mutable arguments, we need to make them hashable\n        key_parts = []\n        \n        # Process positional arguments\n        for arg in args:\n            try:\n                # Try to use the argument directly as a key\n                hash(arg)\n                key_parts.append(arg)\n            except TypeError:\n                # For unhashable types (like lists or dicts), convert to a hashable representation\n                if isinstance(arg, list):\n                    key_parts.append(tuple(arg))\n                elif isinstance(arg, dict):\n                    key_parts.append(tuple(sorted(arg.items())))\n                else:\n                    # For other unhashable types, use a string representation\n                    key_parts.append(str(arg))\n        \n        # Process keyword arguments (sorted for consistency)\n        for k in sorted(kwargs.keys()):\n            v = kwargs[k]\n            try:\n                hash(v)\n                key_parts.append((k, v))\n            except TypeError:\n                if isinstance(v, list):\n                    key_parts.append((k, tuple(v)))\n                elif isinstance(v, dict):\n                    key_parts.append((k, tuple(sorted(v.items()))))\n                else:\n                    key_parts.append((k, str(v)))\n        \n        # Create a hashable key from all parts\n        key = hash(tuple(key_parts))\n        \n        # Check if result is already in cache\n        if key in cache:\n            return cache[key]\n        \n        # If not in cache, compute the result\n        result = func(*args, **kwargs)\n        \n        # Store in cache for future use\n        cache[key] = result\n        \n        return result\n    \n    # Add a method to clear the cache\n    wrapper.clear_cache = lambda: cache.clear()\n    \n    # Add a method to get cache info\n    wrapper.cache_info = lambda: {'size': len(cache)}\n    \n    return wrapper\n\n# Example expensive function that would benefit from memoization\n@memoize\ndef compute_relationship_probability(segment_length, relationship_type):\n    \"\"\"Compute the probability of a segment of a given length under a relationship model.\n    \n    Args:\n        segment_length: Length of the segment in cM\n        relationship_type: Type of relationship to model\n        \n    Returns:\n        Probability of the segment under the relationship model\n    \"\"\"\n    # Simulate an expensive computation\n    print(f\"Computing for segment {segment_length:.2f} cM under {relationship_type} relationship...\")\n    \n    # Add artificial delay to simulate a complex calculation\n    time.sleep(0.1)\n    \n    # Different models for different relationships\n    if relationship_type == 'parent-child':\n        return np.exp(-segment_length / 100) * 0.9\n    elif relationship_type == 'full-sibling':\n        return np.exp(-segment_length / 50) * 0.7\n    elif relationship_type == 'half-sibling':\n        return np.exp(-segment_length / 30) * 0.5\n    elif relationship_type == 'first-cousin':\n        return np.exp(-segment_length / 20) * 0.3\n    else:\n        return np.exp(-segment_length / 10) * 0.1\n\n# Test the memoized function\nprint(\"First call (should compute):\")\nprob1 = compute_relationship_probability(15.0, 'parent-child')\nprint(f\"Result: {prob1}\")\n\nprint(\"\\nSecond call with same arguments (should use cache):\")\nprob2 = compute_relationship_probability(15.0, 'parent-child')\nprint(f\"Result: {prob2}\")\n\nprint(\"\\nThird call with different arguments (should compute):\")\nprob3 = compute_relationship_probability(15.0, 'full-sibling')\nprint(f\"Result: {prob3}\")\n\nprint(\"\\nFourth call with first arguments again (should use cache):\")\nprob4 = compute_relationship_probability(15.0, 'parent-child')\nprint(f\"Result: {prob4}\")\n\n# Check cache info\nprint(f\"\\nCache info: {compute_relationship_probability.cache_info()}\")\n\n# Benchmark with and without memoization\ndef benchmark_memoization():\n    \"\"\"Compare performance with and without memoization.\"\"\"\n    # Define test cases\n    segment_lengths = [10.0, 15.0, 20.0, 25.0, 30.0]\n    relationship_types = ['parent-child', 'full-sibling', 'half-sibling', 'first-cousin', 'unrelated']\n    \n    # Create a non-memoized version for comparison\n    def compute_relationship_probability_no_memo(segment_length, relationship_type):\n        # Same function without memoization\n        time.sleep(0.1)  # Artificial delay\n        \n        if relationship_type == 'parent-child':\n            return np.exp(-segment_length / 100) * 0.9\n        elif relationship_type == 'full-sibling':\n            return np.exp(-segment_length / 50) * 0.7\n        elif relationship_type == 'half-sibling':\n            return np.exp(-segment_length / 30) * 0.5\n        elif relationship_type == 'first-cousin':\n            return np.exp(-segment_length / 20) * 0.3\n        else:\n            return np.exp(-segment_length / 10) * 0.1\n    \n    # Benchmark non-memoized version\n    print(\"\\nBenchmarking without memoization:\")\n    start_time = time.time()\n    \n    # Call the function multiple times, including repeated calls\n    for _ in range(3):  # Repeat the whole test set 3 times\n        for length in segment_lengths:\n            for rel_type in relationship_types:\n                compute_relationship_probability_no_memo(length, rel_type)\n    \n    no_memo_time = time.time() - start_time\n    print(f\"Time without memoization: {no_memo_time:.2f} seconds\")\n    \n    # Clear the cache for the memoized version\n    compute_relationship_probability.clear_cache()\n    \n    # Benchmark memoized version\n    print(\"\\nBenchmarking with memoization:\")\n    start_time = time.time()\n    \n    # Call the function multiple times, including repeated calls\n    for _ in range(3):  # Repeat the whole test set 3 times\n        for length in segment_lengths:\n            for rel_type in relationship_types:\n                compute_relationship_probability(length, rel_type)\n    \n    memo_time = time.time() - start_time\n    print(f\"Time with memoization: {memo_time:.2f} seconds\")\n    \n    # Calculate speedup\n    speedup = no_memo_time / memo_time\n    print(f\"Speedup: {speedup:.2f}x\")\n    \n    return {\n        'no_memo_time': no_memo_time,\n        'memo_time': memo_time,\n        'speedup': speedup\n    }\n\n# Run the benchmark\nbenchmark_memoization()",
   "metadata": {},
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "<cell_type>markdown</cell_type>## Part 4: Memory Optimization Techniques\n\n### Theory and Background\n\nMemory optimization is crucial when processing large genetic datasets, as memory constraints often limit scalability before CPU constraints do. Key memory optimization techniques include:\n\n1. **Efficient Data Structures**: Using memory-efficient data structures appropriate for the task\n2. **Streaming Processing**: Processing data in chunks rather than loading everything into memory\n3. **Object Pooling**: Reusing objects instead of creating new ones\n4. **Memory-Mapped Files**: Accessing file content without loading it entirely into memory\n5. **Sparse Representations**: Using data structures that only store non-default values\n6. **Compression**: Storing data in compressed formats\n\nLet's explore how these techniques can be applied to Bonsai v3 when working with large IBD datasets.",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "<cell_type>markdown</cell_type>### Implementation in Bonsai v3\n\nLet's examine memory optimization strategies for storing and processing IBD segments. In Bonsai v3, IBD segments can consume substantial memory, especially with large cohorts.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Example 1: Memory-Efficient IBD Segment Representation\n\n# Original representation (memory-intensive)\nclass IBDSegment:\n    \"\"\"Standard representation of an IBD segment.\"\"\"\n    \n    def __init__(self, id1, id2, chromosome, start_pos, end_pos, start_cm, end_cm, n_snps=None, score=None):\n        \"\"\"Initialize a new IBD segment.\"\"\"\n        self.id1 = id1\n        self.id2 = id2\n        self.chromosome = chromosome\n        self.start_pos = start_pos\n        self.end_pos = end_pos\n        self.start_cm = start_cm\n        self.end_cm = end_cm\n        self.n_snps = n_snps\n        self.score = score\n        \n        # Calculate derived properties\n        self.length_bp = end_pos - start_pos\n        self.length_cm = end_cm - start_cm\n\n# Memory-optimized representation using slots\nclass OptimizedIBDSegment:\n    \"\"\"Memory-efficient representation of an IBD segment using __slots__.\"\"\"\n    \n    __slots__ = ('id1', 'id2', 'chromosome', 'start_pos', 'end_pos', \n                'start_cm', 'end_cm', 'n_snps', 'score', 'length_bp', 'length_cm')\n    \n    def __init__(self, id1, id2, chromosome, start_pos, end_pos, start_cm, end_cm, n_snps=None, score=None):\n        \"\"\"Initialize a new IBD segment.\"\"\"\n        self.id1 = id1\n        self.id2 = id2\n        self.chromosome = chromosome\n        self.start_pos = start_pos\n        self.end_pos = end_pos\n        self.start_cm = start_cm\n        self.end_cm = end_cm\n        self.n_snps = n_snps\n        self.score = score\n        \n        # Calculate derived properties\n        self.length_bp = end_pos - start_pos\n        self.length_cm = end_cm - start_cm\n\n# Example 2: Compact IBD Segment Tuple Representation\ndef create_compact_segment(id1, id2, chromosome, start_pos, end_pos, start_cm, end_cm, n_snps=None, score=None):\n    \"\"\"Create a compact tuple representation of an IBD segment.\"\"\"\n    # Use a namedtuple-like approach, but with just a basic tuple for maximum memory efficiency\n    return (id1, id2, chromosome, start_pos, end_pos, start_cm, end_cm, n_snps, score)\n\ndef get_segment_length_cm(compact_segment):\n    \"\"\"Get the segment length in cM from a compact segment representation.\"\"\"\n    return compact_segment[6] - compact_segment[5]  # end_cm - start_cm\n\n# Example 3: Using numpy structured arrays for bulk storage\ndef create_segment_array(n_segments):\n    \"\"\"Create a numpy structured array for efficient storage of many segments.\"\"\"\n    # Define the structured array dtype\n    segment_dtype = np.dtype([\n        ('id1', np.int32),          # Use integer IDs instead of strings for efficiency\n        ('id2', np.int32),\n        ('chromosome', np.int8),     # Chromosomes 1-23 fit in a byte\n        ('start_pos', np.int32),     # Base positions in bp\n        ('end_pos', np.int32),\n        ('start_cm', np.float32),    # Genetic positions in cM (32-bit float to save memory)\n        ('end_cm', np.float32),\n        ('n_snps', np.int16),        # Number of SNPs in segment\n        ('score', np.float32)        # IBD detection score\n    ])\n    \n    # Create the array\n    segments = np.zeros(n_segments, dtype=segment_dtype)\n    return segments\n\n# Measure memory usage of different representations\ndef compare_segment_memory_usage(n_segments=100000):\n    \"\"\"Compare memory usage of different IBD segment representations.\"\"\"\n    import sys\n    import numpy as np\n    \n    # Generate random segment data\n    ids = np.arange(1000)  # 1000 possible individual IDs\n    \n    # Memory usage results\n    memory_usage = {}\n    \n    # Method 1: Standard class instances (baseline)\n    print(f\"Creating {n_segments} standard IBD segment objects...\")\n    standard_segments = []\n    for i in range(n_segments):\n        id1, id2 = np.random.choice(ids, 2, replace=False)\n        chromosome = np.random.randint(1, 23)\n        start_pos = np.random.randint(1, 200_000_000)\n        end_pos = start_pos + np.random.randint(1000, 5_000_000)\n        start_cm = start_pos / 1_000_000 * 1.5  # Approximate cM position\n        end_cm = end_pos / 1_000_000 * 1.5\n        n_snps = np.random.randint(10, 1000)\n        score = np.random.random()\n        \n        segment = IBDSegment(id1, id2, chromosome, start_pos, end_pos, start_cm, end_cm, n_snps, score)\n        standard_segments.append(segment)\n    \n    # Measure memory usage\n    memory_usage['standard'] = sys.getsizeof(standard_segments) + \\\n                              sum(sys.getsizeof(seg) for seg in standard_segments)\n    \n    # Method 2: Optimized class instances with __slots__\n    print(f\"Creating {n_segments} optimized IBD segment objects with __slots__...\")\n    optimized_segments = []\n    for i in range(n_segments):\n        id1, id2 = np.random.choice(ids, 2, replace=False)\n        chromosome = np.random.randint(1, 23)\n        start_pos = np.random.randint(1, 200_000_000)\n        end_pos = start_pos + np.random.randint(1000, 5_000_000)\n        start_cm = start_pos / 1_000_000 * 1.5\n        end_cm = end_pos / 1_000_000 * 1.5\n        n_snps = np.random.randint(10, 1000)\n        score = np.random.random()\n        \n        segment = OptimizedIBDSegment(id1, id2, chromosome, start_pos, end_pos, start_cm, end_cm, n_snps, score)\n        optimized_segments.append(segment)\n    \n    # Measure memory usage\n    memory_usage['optimized'] = sys.getsizeof(optimized_segments) + \\\n                               sum(sys.getsizeof(seg) for seg in optimized_segments)\n    \n    # Method 3: Tuple representation\n    print(f\"Creating {n_segments} tuple-based IBD segments...\")\n    tuple_segments = []\n    for i in range(n_segments):\n        id1, id2 = np.random.choice(ids, 2, replace=False)\n        chromosome = np.random.randint(1, 23)\n        start_pos = np.random.randint(1, 200_000_000)\n        end_pos = start_pos + np.random.randint(1000, 5_000_000)\n        start_cm = start_pos / 1_000_000 * 1.5\n        end_cm = end_pos / 1_000_000 * 1.5\n        n_snps = np.random.randint(10, 1000)\n        score = np.random.random()\n        \n        segment = create_compact_segment(id1, id2, chromosome, start_pos, end_pos, start_cm, end_cm, n_snps, score)\n        tuple_segments.append(segment)\n    \n    # Measure memory usage\n    memory_usage['tuple'] = sys.getsizeof(tuple_segments) + \\\n                           sum(sys.getsizeof(seg) for seg in tuple_segments)\n    \n    # Method 4: Numpy structured array\n    print(f\"Creating a numpy structured array for {n_segments} IBD segments...\")\n    segment_array = create_segment_array(n_segments)\n    \n    # Fill with random data\n    for i in range(n_segments):\n        id1, id2 = np.random.choice(ids, 2, replace=False)\n        chromosome = np.random.randint(1, 23)\n        start_pos = np.random.randint(1, 200_000_000)\n        end_pos = start_pos + np.random.randint(1000, 5_000_000)\n        start_cm = start_pos / 1_000_000 * 1.5\n        end_cm = end_pos / 1_000_000 * 1.5\n        n_snps = np.random.randint(10, 1000)\n        score = np.random.random()\n        \n        segment_array[i] = (id1, id2, chromosome, start_pos, end_pos, start_cm, end_cm, n_snps, score)\n    \n    # Measure memory usage\n    memory_usage['numpy'] = segment_array.nbytes\n    \n    # Print results\n    print(\"\\nMemory usage comparison:\")\n    for method, memory in memory_usage.items():\n        print(f\"  {method}: {memory/1024/1024:.2f} MB\")\n    \n    # Calculate memory savings\n    baseline = memory_usage['standard']\n    for method, memory in memory_usage.items():\n        if method != 'standard':\n            savings = (baseline - memory) / baseline * 100\n            print(f\"  {method} saves {savings:.2f}% compared to standard\")\n    \n    # Visualize the comparison\n    plt.figure(figsize=(12, 6))\n    methods = list(memory_usage.keys())\n    memory_mb = [memory_usage[m]/1024/1024 for m in methods]\n    \n    colors = ['#3498db', '#2ecc71', '#e74c3c', '#f39c12']\n    bars = plt.bar(methods, memory_mb, color=colors)\n    \n    # Add labels\n    for bar in bars:\n        height = bar.get_height()\n        plt.text(bar.get_x() + bar.get_width()/2., height,\n                f'{height:.2f} MB',\n                ha='center', va='bottom', fontsize=12)\n    \n    plt.xlabel('Representation Method')\n    plt.ylabel('Memory Usage (MB)')\n    plt.title('Memory Usage Comparison for IBD Segment Representations')\n    plt.grid(axis='y', alpha=0.3)\n    plt.tight_layout()\n    plt.show()\n    \n    return memory_usage\n\n# Run the memory usage comparison\nmemory_usage = compare_segment_memory_usage(100000)",
   "metadata": {},
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "<cell_type>markdown</cell_type>### Exercise 4: Implement Chunked Processing for Large Datasets\n\nLarge datasets often can't be loaded entirely into memory. Chunked processing allows us to work with these datasets by processing them in manageable portions.\n\n**Task:** Implement a streaming processor for large IBD segment files that analyzes data without loading the entire file into memory.\n\n**Hint:** Use Python's file handling capabilities to read and process the file line by line.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Exercise 4: Implement a streaming processor for large IBD segment files\n\nclass StreamingIBDProcessor:\n    \"\"\"Process large IBD segment files without loading the entire file into memory.\"\"\"\n    \n    def __init__(self, chunk_size=1000):\n        \"\"\"Initialize the processor.\n        \n        Args:\n            chunk_size: Number of segments to process at once\n        \"\"\"\n        self.chunk_size = chunk_size\n        self.stats = {\n            'total_segments': 0,\n            'total_pairs': 0,\n            'chr_counts': {},\n            'length_stats': {'min': float('inf'), 'max': 0, 'total': 0},\n            'individual_counts': {}\n        }\n    \n    def process_file(self, file_path, callback=None):\n        \"\"\"Process an IBD segment file line by line.\n        \n        Args:\n            file_path: Path to the IBD segment file\n            callback: Optional function to call for each chunk of segments\n            \n        Returns:\n            Dictionary of statistics about the file\n        \"\"\"\n        # TODO: Implement the streaming processor\n        # 1. Open the file and read it line by line\n        # 2. Parse each line into an IBD segment\n        # 3. Process segments in chunks\n        # 4. Update statistics\n        # 5. Call the callback function with each chunk if provided\n        \n        # Implementation\n        current_chunk = []\n        \n        print(f\"Processing {file_path}...\")\n        \n        try:\n            with open(file_path, 'r') as f:\n                # Skip header if present\n                first_line = f.readline().strip()\n                if first_line.startswith('#') or not self._is_segment_line(first_line):\n                    print(\"Skipping header line\")\n                else:\n                    # Process the first line if it's a segment\n                    self._process_segment_line(first_line, current_chunk)\n                \n                # Process the rest of the file\n                for line_num, line in enumerate(f, start=2):\n                    # Skip empty lines and comments\n                    line = line.strip()\n                    if not line or line.startswith('#'):\n                        continue\n                    \n                    # Process this segment\n                    self._process_segment_line(line, current_chunk)\n                    \n                    # If we've reached the chunk size, process the chunk\n                    if len(current_chunk) >= self.chunk_size:\n                        self._process_chunk(current_chunk)\n                        if callback:\n                            callback(current_chunk)\n                        current_chunk = []\n                        \n                        # Print progress every 100,000 segments\n                        if self.stats['total_segments'] % 100000 == 0:\n                            print(f\"  Processed {self.stats['total_segments']} segments...\")\n            \n            # Process any remaining segments\n            if current_chunk:\n                self._process_chunk(current_chunk)\n                if callback:\n                    callback(current_chunk)\n            \n            # Calculate averages\n            if self.stats['total_segments'] > 0:\n                self.stats['avg_length'] = self.stats['length_stats']['total'] / self.stats['total_segments']\n            else:\n                self.stats['avg_length'] = 0\n                \n            print(f\"Completed processing. Found {self.stats['total_segments']} segments across {self.stats['total_pairs']} pairs.\")\n            return self.stats\n            \n        except Exception as e:\n            print(f\"Error processing file: {e}\")\n            return self.stats\n    \n    def _is_segment_line(self, line):\n        \"\"\"Check if a line contains an IBD segment.\"\"\"\n        parts = line.strip().split()\n        return len(parts) >= 6 and all(self._is_numeric(p) for p in parts[2:6])\n    \n    def _is_numeric(self, text):\n        \"\"\"Check if a string represents a number.\"\"\"\n        try:\n            float(text)\n            return True\n        except (ValueError, TypeError):\n            return False\n    \n    def _process_segment_line(self, line, current_chunk):\n        \"\"\"Parse a line into an IBD segment and add it to the current chunk.\"\"\"\n        parts = line.strip().split()\n        \n        # Expected format: id1 id2 chromosome start_pos end_pos genetic_length [additional fields]\n        if len(parts) < 6:\n            # Skip malformed lines\n            return\n        \n        try:\n            id1 = parts[0]\n            id2 = parts[1]\n            chromosome = int(parts[2])\n            start_pos = int(parts[3])\n            end_pos = int(parts[4])\n            genetic_length = float(parts[5])\n            \n            # Create a simple tuple representation for memory efficiency\n            segment = (id1, id2, chromosome, start_pos, end_pos, genetic_length)\n            current_chunk.append(segment)\n            \n        except (ValueError, IndexError) as e:\n            # Skip malformed lines\n            print(f\"Skipping malformed line: {line.strip()} - Error: {e}\")\n    \n    def _process_chunk(self, chunk):\n        \"\"\"Process a chunk of segments.\"\"\"\n        # Update total segments\n        self.stats['total_segments'] += len(chunk)\n        \n        # Process each segment\n        pairs_seen = set()\n        \n        for segment in chunk:\n            id1, id2, chromosome, start_pos, end_pos, genetic_length = segment\n            \n            # Update chromosome counts\n            self.stats['chr_counts'][chromosome] = self.stats['chr_counts'].get(chromosome, 0) + 1\n            \n            # Update length statistics\n            self.stats['length_stats']['min'] = min(self.stats['length_stats']['min'], genetic_length)\n            self.stats['length_stats']['max'] = max(self.stats['length_stats']['max'], genetic_length)\n            self.stats['length_stats']['total'] += genetic_length\n            \n            # Update individual counts\n            self.stats['individual_counts'][id1] = self.stats['individual_counts'].get(id1, 0) + 1\n            self.stats['individual_counts'][id2] = self.stats['individual_counts'].get(id2, 0) + 1\n            \n            # Update pair count (each pair is counted only once)\n            pair = (min(id1, id2), max(id1, id2))\n            if pair not in pairs_seen:\n                pairs_seen.add(pair)\n                self.stats['total_pairs'] += 1\n\n# Create a simple custom callback function for the streaming processor\ndef example_callback(chunk):\n    \"\"\"Example callback function for the streaming processor.\"\"\"\n    # In a real application, this might update a progress bar, save to a database, etc.\n    long_segments = [seg for seg in chunk if seg[5] > 15]  # Filter segments longer than 15 cM\n    if long_segments:\n        print(f\"  Found {len(long_segments)} segments longer than 15 cM in this chunk\")\n        \n# Test with a mock IBD file\ndef create_mock_ibd_file(filename, n_segments=10000):\n    \"\"\"Create a mock IBD segment file for testing.\"\"\"\n    print(f\"Creating mock IBD file with {n_segments} segments: {filename}\")\n    \n    with open(filename, 'w') as f:\n        # Write header\n        f.write(\"# Mock IBD segment file\\n\")\n        f.write(\"id1 id2 chrom start_pos end_pos genetic_length\\n\")\n        \n        # Write segments\n        for i in range(n_segments):\n            id1 = f\"ind_{np.random.randint(1, 100)}\"\n            id2 = f\"ind_{np.random.randint(1, 100)}\"\n            while id2 == id1:\n                id2 = f\"ind_{np.random.randint(1, 100)}\"\n                \n            chromosome = np.random.randint(1, 23)\n            start_pos = np.random.randint(1, 200_000_000)\n            end_pos = start_pos + np.random.randint(1000, 5_000_000)\n            genetic_length = np.random.exponential(10) # Mean 10 cM\n            \n            f.write(f\"{id1} {id2} {chromosome} {start_pos} {end_pos} {genetic_length:.2f}\\n\")\n    \n    print(f\"Mock file created: {filename}\")\n    return filename\n\n# Create a test file\nimport os\nmock_file_path = os.path.join(RESULTS_DIR, \"mock_ibd_segments.txt\")\ncreate_mock_ibd_file(mock_file_path, n_segments=50000)\n\n# Test the streaming processor\nprocessor = StreamingIBDProcessor(chunk_size=5000)\nstats = processor.process_file(mock_file_path, callback=example_callback)\n\n# Display summary statistics\nprint(\"\\nSummary Statistics:\")\nprint(f\"Total Segments: {stats['total_segments']}\")\nprint(f\"Total Pairs: {stats['total_pairs']}\")\nprint(f\"Chromosome Distribution:\")\nfor chr_num in sorted(stats['chr_counts'].keys()):\n    print(f\"  Chr {chr_num}: {stats['chr_counts'][chr_num]} segments\")\nprint(f\"Segment Length Statistics:\")\nprint(f\"  Min: {stats['length_stats']['min']:.2f} cM\")\nprint(f\"  Max: {stats['length_stats']['max']:.2f} cM\")\nprint(f\"  Avg: {stats['avg_length']:.2f} cM\")\nprint(f\"Top 5 individuals by segment count:\")\ntop_individuals = sorted(stats['individual_counts'].items(), key=lambda x: x[1], reverse=True)[:5]\nfor ind, count in top_individuals:\n    print(f\"  {ind}: {count} segments\")\n\n# Visualize some statistics\nplt.figure(figsize=(15, 6))\n\n# Plot 1: Chromosome distribution\nplt.subplot(1, 2, 1)\nchr_nums = sorted(stats['chr_counts'].keys())\nchr_counts = [stats['chr_counts'][chr_num] for chr_num in chr_nums]\nplt.bar(chr_nums, chr_counts)\nplt.xlabel('Chromosome')\nplt.ylabel('Number of Segments')\nplt.title('IBD Segment Distribution by Chromosome')\n\n# Plot 2: Top individuals\nplt.subplot(1, 2, 2)\ntop_n = 10\ntop_individuals = sorted(stats['individual_counts'].items(), key=lambda x: x[1], reverse=True)[:top_n]\ninds = [ind for ind, _ in top_individuals]\ncounts = [count for _, count in top_individuals]\nplt.barh(inds, counts)\nplt.xlabel('Number of Segments')\nplt.ylabel('Individual ID')\nplt.title(f'Top {top_n} Individuals by Segment Count')\n\nplt.tight_layout()\nplt.show()",
   "metadata": {},
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "<cell_type>markdown</cell_type>## Part 5: Precision-Performance Tradeoffs\n\n### Theory and Background\n\nIn genetic genealogy computations, precision and performance often exist in a trade-off relationship. By making intelligent tradeoffs, we can significantly improve performance with minimal impact on accuracy.\n\nKey precision-performance tradeoff strategies include:\n\n1. **Early Termination**: Stopping computations once a sufficient level of confidence is reached\n2. **Approximation Algorithms**: Using faster, approximate methods for computationally intensive operations\n3. **Pruning Low-Information Data**: Focusing on high-quality data and ignoring noisy or ambiguous information\n4. **Adaptive Precision**: Adjusting computational precision based on the specific relationship being analyzed\n5. **Confidence-Weighted Operations**: Allocating more computational resources to higher-confidence predictions\n\nIn Bonsai v3, there are several areas where precision-performance tradeoffs can be intelligently applied without significantly compromising accuracy.",
   "metadata": {},
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}