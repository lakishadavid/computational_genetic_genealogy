<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Lab 18: Optimization Techniques and Performance | Computational Genetic Genealogy</title>
    <link rel="stylesheet" href="../styles.css">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0/css/all.min.css">
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Montserrat:wght@400;600;700&family=Open+Sans:wght@300;400;600&display=swap" rel="stylesheet">
</head>
<body class="textbook-page">
    <header class="textbook-header">
        <div class="container">
            <h1>Computational Genetic Genealogy</h1>
            <p>Optimization Techniques and Performance Enhancements</p>
        </div>
    </header>

    <nav class="textbook-nav">
        <div class="container">
            <a href="../index.html"><i class="fas fa-arrow-left"></i> Back to Main Page</a>
            <a href="contents.html">Table of Contents</a>
            <a href="lab18_optimization_techniques.html" class="active">Lab 18: Optimization Techniques</a>
        </div>
    </nav>

    <main class="container">
        <article class="section lab-content">
            <h2>Lab 18: Optimization Techniques and Performance Enhancements</h2>
            
            <div class="alert alert-info">
                <p><strong>Core Component:</strong> This lab explores the sophisticated optimization techniques used in Bonsai v3 to enable efficient pedigree reconstruction at scale. These optimizations include search space pruning, parallel processing, adaptive parameter selection, and specialized data structures, all of which are essential for handling real-world genetic genealogy datasets with hundreds or thousands of individuals.</p>
            </div>

            <h3>The Challenge of Computational Efficiency</h3>
            
            <div class="concept-section">
                <h4>Why Optimization Matters</h4>
                <p>Pedigree reconstruction from genetic data is inherently combinatorial in nature. The number of possible pedigree configurations grows exponentially with the number of individuals, making naive approaches computationally infeasible for real-world datasets. Bonsai v3 addresses this challenge through a suite of sophisticated optimizations that dramatically reduce computational costs without sacrificing accuracy.</p>
                
                <p>The key optimization challenges in pedigree reconstruction include:</p>
                
                <ol>
                    <li><strong>Combinatorial Explosion:</strong> The number of possible pedigree configurations grows exponentially with the number of individuals</li>
                    <li><strong>Computational Intensity:</strong> Likelihood calculations for each potential pedigree are computationally expensive</li>
                    <li><strong>Memory Requirements:</strong> Naive approaches require storing large numbers of candidate pedigrees</li>
                    <li><strong>Real-time Constraints:</strong> Applications often require results in a reasonable timeframe</li>
                </ol>
                
                <p>Bonsai v3 addresses these challenges through optimization strategies implemented across all levels of the system, from high-level algorithms to low-level implementation details. Let's explore the key techniques that enable Bonsai to scale to large datasets.</p>
            </div>
            
            <div class="concept-section">
                <h4>Search Space Pruning</h4>
                <p>One of the most important optimization strategies in Bonsai v3 is search space pruning â€” intelligently restricting the space of pedigree configurations to explore. This is implemented through several specialized functions that focus computational resources on the most promising regions of the search space.</p>
                
                <p>The <code>prune_search_space</code> function manages this process:</p>
                
                <pre><code>def prune_search_space(
    id_to_up_dct: dict[int, dict[int, dict[int, int]]],
    id_to_shared_ibd: dict[tuple[int, int], list[dict]],
    id_to_info: dict[int, dict],
    pw_ll: Any,
):
    """
    Prune the search space for pedigree reconstruction by identifying
    and focusing on the most promising configurations.
    
    Args:
        id_to_up_dct: Dict mapping IDs to their pedigrees (up-node dictionaries)
        id_to_shared_ibd: Dict mapping ID pairs to their IBD segments
        id_to_info: Dict with demographic information for individuals
        pw_ll: PwLogLike instance for likelihood calculation
        
    Returns:
        pruned_search_space: Data structure defining the pruned search space
    """
    # Step 1: Determine IBD connectivity structure
    id_pair_to_cm = calculate_id_pair_to_cm(id_to_shared_ibd)
    connectivity_graph = build_connectivity_graph(id_pair_to_cm)
    
    # Step 2: Identify clusters of closely related individuals
    clusters = identify_related_clusters(connectivity_graph, min_ibd_threshold=400)
    
    # Step 3: Establish constraints based on demographic information
    constraints = establish_demographic_constraints(id_to_info)
    
    # Step 4: Generate search space partitions
    partitions = generate_search_partitions(clusters, constraints, id_to_up_dct)
    
    # Step 5: Create prioritized search order
    search_order = prioritize_search_order(partitions, id_pair_to_cm, id_to_info)
    
    # Assemble pruned search space representation
    pruned_search_space = {
        'clusters': clusters,
        'constraints': constraints,
        'partitions': partitions,
        'search_order': search_order,
    }
    
    return pruned_search_space</code></pre>
                
                <p>This function implements several key optimization techniques:</p>
                
                <ol>
                    <li><strong>Connectivity-Based Clustering:</strong> Grouping individuals into clusters based on their IBD connectivity, allowing Bonsai to focus on reconstructing each cluster separately before merging them</li>
                    <li><strong>Demographic Constraints:</strong> Using age, sex, and other demographic information to rule out impossible relationships</li>
                    <li><strong>Search Partitioning:</strong> Dividing the search space into partitions that can be explored independently</li>
                    <li><strong>Prioritized Ordering:</strong> Establishing an optimal order for exploring the search space, focusing on the most promising regions first</li>
                </ol>
                
                <p>By intelligently pruning the search space, Bonsai v3 can reduce the number of pedigree configurations to evaluate by orders of magnitude while still finding optimal or near-optimal solutions.</p>
            </div>
            
            <div class="concept-section">
                <h4>Parallel Processing</h4>
                <p>Bonsai v3 takes advantage of modern multi-core processors through parallel processing of independent tasks. This is particularly important for operations that can be naturally parallelized, such as evaluating multiple pedigree configurations or processing different chromosomes independently.</p>
                
                <p>The parallel processing infrastructure is implemented in the <code>parallel_manager.py</code> module:</p>
                
                <pre><code>class ParallelManager:
    """
    Manages parallel execution of tasks in Bonsai v3.
    
    This class provides infrastructure for running multiple independent
    tasks in parallel using a thread pool or process pool.
    """
    
    def __init__(self, max_workers=None, use_processes=False):
        """
        Initialize the parallel manager.
        
        Args:
            max_workers: Maximum number of worker threads/processes
            use_processes: Whether to use processes instead of threads
        """
        self.max_workers = max_workers or min(32, os.cpu_count() + 4)
        self.use_processes = use_processes
        self._executor = None
    
    def execute(self, tasks, callback=None):
        """
        Execute a list of tasks in parallel.
        
        Args:
            tasks: List of (function, args, kwargs) tuples to execute
            callback: Function to call with each result
            
        Returns:
            List of results from all tasks
        """
        executor_cls = concurrent.futures.ProcessPoolExecutor if self.use_processes else concurrent.futures.ThreadPoolExecutor
        
        with executor_cls(max_workers=self.max_workers) as executor:
            # Submit all tasks
            futures = []
            for func, args, kwargs in tasks:
                future = executor.submit(func, *args, **kwargs)
                if callback:
                    future.add_done_callback(lambda f: callback(f.result()))
                futures.append(future)
            
            # Collect results
            results = []
            for future in concurrent.futures.as_completed(futures):
                try:
                    result = future.result()
                    results.append(result)
                except Exception as e:
                    results.append(None)
                    logging.error(f"Task failed with error: {e}")
            
            return results</code></pre>
                
                <p>Bonsai v3 uses this parallel processing infrastructure for several key operations:</p>
                
                <ul>
                    <li><strong>Evaluating Multiple Pedigree Configurations:</strong> When exploring different ways to connect pedigrees or add new individuals, Bonsai can evaluate multiple configurations in parallel</li>
                    <li><strong>Processing Multiple Chromosomes:</strong> IBD segments on different chromosomes can be processed independently</li>
                    <li><strong>Batch Likelihood Calculations:</strong> When calculating likelihoods for multiple relationship configurations, these can be done in parallel</li>
                </ul>
                
                <p>This parallel processing capability allows Bonsai v3 to leverage the full power of modern hardware, dramatically reducing computation time for large datasets.</p>
            </div>

            <h3>Specialized Optimization Techniques</h3>
            
            <div class="concept-section">
                <h4>Adaptive Parameter Selection</h4>
                <p>Bonsai v3 uses adaptive parameter selection to optimize performance based on dataset characteristics. Rather than using fixed parameters for all situations, Bonsai dynamically adjusts its parameters based on factors like dataset size, IBD density, and computational resources available.</p>
                
                <p>The <code>optimize_parameters</code> function implements this approach:</p>
                
                <pre><code>def optimize_parameters(
    id_to_shared_ibd: dict[tuple[int, int], list[dict]],
    id_to_info: dict[int, dict],
    execution_context: dict,
):
    """
    Dynamically optimize algorithm parameters based on dataset characteristics.
    
    Args:
        id_to_shared_ibd: Dict mapping ID pairs to their IBD segments
        id_to_info: Dict with demographic information for individuals
        execution_context: Information about the execution environment
        
    Returns:
        optimized_params: Dict of optimized parameter values
    """
    # Get dataset characteristics
    num_individuals = get_num_individuals(id_to_shared_ibd)
    ibd_density = calculate_ibd_density(id_to_shared_ibd)
    avg_ibd_length = calculate_avg_ibd_length(id_to_shared_ibd)
    
    # Get execution environment information
    available_memory = execution_context.get('available_memory', 8 * 1024 * 1024 * 1024)  # Default 8GB
    available_cores = execution_context.get('available_cores', os.cpu_count())
    
    # Initialize parameters with default values
    params = {
        'max_up': 3,             # Maximum generations to extend upward
        'n_keep': 5,             # Number of top pedigrees to keep
        'ibd_threshold': 20,     # Minimum IBD amount to consider (cM)
        'max_iterations': 100,   # Maximum iterations for optimization
        'batch_size': 10,        # Batch size for parallel processing
        'use_threading': True,   # Whether to use threading
    }
    
    # Adjust max_up based on IBD density and length
    if ibd_density > 0.5 and avg_ibd_length > 1000:
        # Dense IBD with long segments - likely close relatives
        params['max_up'] = 2
    elif ibd_density < 0.1 or avg_ibd_length < 100:
        # Sparse IBD with short segments - likely distant relatives
        params['max_up'] = 4
    
    # Adjust n_keep based on available memory
    if available_memory < 4 * 1024 * 1024 * 1024:  # Less than 4GB
        params['n_keep'] = 3
    elif available_memory > 16 * 1024 * 1024 * 1024:  # More than 16GB
        params['n_keep'] = 10
    
    # Adjust batch_size based on available cores
    params['batch_size'] = min(max(available_cores, 2), 32)
    
    # Adjust ibd_threshold based on dataset size
    if num_individuals > 100:
        params['ibd_threshold'] = 30
    elif num_individuals < 20:
        params['ibd_threshold'] = 10
    
    # Decide whether to use threading or processes
    if available_memory > 8 * 1024 * 1024 * 1024:  # More than 8GB
        params['use_threading'] = False  # Use processes for better parallelism
    
    return params</code></pre>
                
                <p>This adaptive parameter selection allows Bonsai v3 to optimize its behavior for each specific dataset and execution environment. For example:</p>
                
                <ul>
                    <li>With datasets containing mostly close relatives (long IBD segments), Bonsai can use smaller values for <code>max_up</code>, reducing the search space</li>
                    <li>With large datasets on machines with limited memory, Bonsai can reduce <code>n_keep</code> to conserve memory</li>
                    <li>On machines with many CPU cores, Bonsai can increase <code>batch_size</code> to leverage parallel processing</li>
                </ul>
                
                <p>This adaptive approach ensures that Bonsai v3 can efficiently handle a wide range of datasets, from small pedigrees with closely related individuals to large pedigrees with distant relationships.</p>
            </div>
            
            <div class="concept-section">
                <h4>Specialized Data Structures</h4>
                <p>Bonsai v3 uses specialized data structures optimized for the specific requirements of pedigree reconstruction. These data structures are designed to minimize memory usage and computational overhead while still providing efficient access to the information needed for reconstruction.</p>
                
                <p>The <code>CompactIBDStore</code> class is an example of such a specialized data structure:</p>
                
                <pre><code>class CompactIBDStore:
    """
    Memory-efficient storage for IBD segment data.
    
    This class provides a compact representation of IBD segments,
    optimized for the specific access patterns used in Bonsai v3.
    """
    
    def __init__(self, id_to_shared_ibd):
        """
        Initialize the compact IBD store from a standard IBD representation.
        
        Args:
            id_to_shared_ibd: Dict mapping ID pairs to their IBD segments
        """
        # Convert to more efficient representation
        self.pairs = []
        self.segments = []
        self.pair_to_segments = {}
        
        pair_to_idx = {}
        for (id1, id2), segs in id_to_shared_ibd.items():
            pair_idx = len(self.pairs)
            self.pairs.append((id1, id2))
            pair_to_idx[(id1, id2)] = pair_idx
            
            # Store segment indices
            seg_indices = []
            for seg in segs:
                seg_idx = len(self.segments)
                # Store only essential fields
                compact_seg = {
                    'chrom': seg.get('chrom', 0),
                    'start_cm': seg.get('start_cm', 0),
                    'end_cm': seg.get('end_cm', 0),
                    'length_cm': seg.get('length_cm', 0)
                }
                self.segments.append(compact_seg)
                seg_indices.append(seg_idx)
            
            self.pair_to_segments[pair_idx] = seg_indices
    
    def get_shared_segments(self, id1, id2):
        """
        Get the IBD segments shared by two individuals.
        
        Args:
            id1, id2: IDs of the individuals
            
        Returns:
            List of shared IBD segments
        """
        pair = (min(id1, id2), max(id1, id2))
        if pair in self.pairs:
            pair_idx = self.pairs.index(pair)
            seg_indices = self.pair_to_segments.get(pair_idx, [])
            return [self.segments[idx] for idx in seg_indices]
        else:
            return []
    
    def get_total_ibd(self, id1, id2):
        """
        Get the total IBD shared by two individuals.
        
        Args:
            id1, id2: IDs of the individuals
            
        Returns:
            Total shared IBD in centimorgans
        """
        segments = self.get_shared_segments(id1, id2)
        return sum(seg['length_cm'] for seg in segments)</code></pre>
                
                <p>Bonsai v3 uses several such specialized data structures, each optimized for specific operations:</p>
                
                <ul>
                    <li><strong>CompactIBDStore:</strong> Memory-efficient storage for IBD segment data</li>
                    <li><strong>SparseRelationshipMatrix:</strong> Efficient representation of pairwise relationships</li>
                    <li><strong>PedigreeGraph:</strong> Optimized graph representation of pedigree structures</li>
                    <li><strong>LikelihoodCache:</strong> Caching structure for reusing likelihood calculations</li>
                </ul>
                
                <p>These specialized data structures allow Bonsai v3 to minimize memory usage and computational overhead, enabling it to handle much larger datasets than would be possible with general-purpose data structures.</p>
            </div>
            
            <div class="concept-section">
                <h4>Early Termination and Lazy Evaluation</h4>
                <p>Bonsai v3 uses early termination and lazy evaluation strategies to avoid unnecessary computation. These techniques allow Bonsai to quickly eliminate unpromising pedigree configurations without fully evaluating them, leading to significant performance improvements.</p>
                
                <p>The <code>evaluate_pedigree</code> function demonstrates this approach:</p>
                
                <pre><code>def evaluate_pedigree(
    up_dct: dict[int, dict[int, int]],
    id_to_shared_ibd: dict[tuple[int, int], list[dict]],
    id_to_info: dict[int, dict],
    pw_ll: Any,
    early_term_threshold: float = -1000.0,
):
    """
    Evaluate the likelihood of a pedigree with early termination.
    
    Args:
        up_dct: Up-node dictionary representing the pedigree
        id_to_shared_ibd: Dict mapping ID pairs to their IBD segments
        id_to_info: Dict with demographic information for individuals
        pw_ll: PwLogLike instance for likelihood calculation
        early_term_threshold: Threshold for early termination
        
    Returns:
        log_likelihood: Log-likelihood of the pedigree, or None if terminated early
    """
    # Initialize log-likelihood
    log_like = 0.0
    
    # Get all pairs of individuals in the pedigree
    all_ids = list(up_dct.keys())
    all_pairs = [(all_ids[i], all_ids[j]) for i in range(len(all_ids)) for j in range(i+1, len(all_ids))]
    
    # First evaluate high-IBD pairs (more informative)
    high_ibd_pairs = []
    low_ibd_pairs = []
    
    for id1, id2 in all_pairs:
        pair = (min(id1, id2), max(id1, id2))
        if pair in id_to_shared_ibd:
            total_cm = sum(seg.get('length_cm', 0) for seg in id_to_shared_ibd[pair])
            if total_cm > 100:  # High IBD threshold
                high_ibd_pairs.append((id1, id2))
            else:
                low_ibd_pairs.append((id1, id2))
        else:
            low_ibd_pairs.append((id1, id2))
    
    # Evaluate high-IBD pairs first
    for id1, id2 in high_ibd_pairs:
        # Get relationship in the pedigree
        rel_tuple = get_relationship_tuple(up_dct, id1, id2)
        
        # Get IBD segments
        pair = (min(id1, id2), max(id1, id2))
        ibd_segs = id_to_shared_ibd.get(pair, [])
        
        # Calculate likelihood for this pair
        pair_ll = pw_ll.get_ibd_log_like(
            id1=id1,
            id2=id2,
            rel_tuple=rel_tuple,
            ibd_segs=ibd_segs,
        )
        
        # Update total likelihood
        log_like += pair_ll
        
        # Check for early termination
        if log_like < early_term_threshold:
            return None  # Terminate early, pedigree is very unlikely
    
    # Only evaluate low-IBD pairs if we haven't terminated early
    for id1, id2 in low_ibd_pairs:
        # (Similar evaluation as for high-IBD pairs)
        # ...
    
    return log_like</code></pre>
                
                <p>This function implements several key optimization techniques:</p>
                
                <ol>
                    <li><strong>Early Termination:</strong> If the likelihood falls below a threshold during evaluation, the function returns immediately without evaluating the remaining pairs</li>
                    <li><strong>Prioritized Evaluation:</strong> High-IBD pairs are evaluated first, as they are more informative and more likely to trigger early termination if the pedigree is incorrect</li>
                    <li><strong>Lazy Evaluation:</strong> Low-IBD pairs are only evaluated if the pedigree hasn't been rejected based on high-IBD pairs</li>
                </ol>
                
                <p>These techniques allow Bonsai v3 to quickly eliminate unlikely pedigree configurations without fully evaluating them, dramatically reducing computation time for large datasets with many potential configurations.</p>
            </div>

            <h3>Performance Profiling and Bottleneck Elimination</h3>
            
            <div class="concept-section">
                <h4>Identifying Performance Bottlenecks</h4>
                <p>Bonsai v3 includes sophisticated performance profiling capabilities to identify and eliminate bottlenecks. These profiling tools help developers understand where the system is spending time and memory, allowing them to focus optimization efforts on the most critical areas.</p>
                
                <p>The <code>performance_profiler.py</code> module provides these capabilities:</p>
                
                <pre><code>class PerformanceProfiler:
    """
    Profiles the performance of Bonsai v3 components.
    
    This class provides tools for measuring execution time,
    memory usage, and other performance metrics.
    """
    
    def __init__(self):
        """
        Initialize the performance profiler.
        """
        self.timers = {}
        self.memory_samples = {}
        self.function_calls = {}
        self.is_active = False
    
    def start(self):
        """
        Start the profiler.
        """
        self.is_active = True
        self.timers = {}
        self.memory_samples = {}
        self.function_calls = {}
    
    def stop(self):
        """
        Stop the profiler.
        """
        self.is_active = False
    
    @contextlib.contextmanager
    def timer(self, name):
        """
        Context manager for timing a block of code.
        
        Args:
            name: Name of the timer
        """
        if not self.is_active:
            yield
            return
        
        start_time = time.time()
        start_memory = get_memory_usage()
        
        try:
            yield
        finally:
            end_time = time.time()
            end_memory = get_memory_usage()
            
            # Record timing information
            duration = end_time - start_time
            if name not in self.timers:
                self.timers[name] = []
            self.timers[name].append(duration)
            
            # Record memory usage
            memory_delta = end_memory - start_memory
            if name not in self.memory_samples:
                self.memory_samples[name] = []
            self.memory_samples[name].append(memory_delta)
    
    def record_function_call(self, func_name, args_len, kwargs_len):
        """
        Record a function call.
        
        Args:
            func_name: Name of the function
            args_len: Number of positional arguments
            kwargs_len: Number of keyword arguments
        """
        if not self.is_active:
            return
        
        if func_name not in self.function_calls:
            self.function_calls[func_name] = 0
        self.function_calls[func_name] += 1
    
    def get_summary(self):
        """
        Get a summary of the profiling results.
        
        Returns:
            summary: Dict containing profiling summary
        """
        if not self.timers:
            return {"status": "No profiling data available"}
        
        summary = {
            "timers": {},
            "memory": {},
            "function_calls": self.function_calls,
            "hotspots": []
        }
        
        # Summarize timing information
        for name, durations in self.timers.items():
            summary["timers"][name] = {
                "total": sum(durations),
                "calls": len(durations),
                "average": sum(durations) / len(durations),
                "min": min(durations),
                "max": max(durations)
            }
        
        # Summarize memory usage
        for name, deltas in self.memory_samples.items():
            summary["memory"][name] = {
                "total": sum(deltas),
                "calls": len(deltas),
                "average": sum(deltas) / len(deltas),
                "min": min(deltas),
                "max": max(deltas)
            }
        
        # Identify hotspots (operations taking the most time)
        hotspots = sorted(
            [(name, data["total"]) for name, data in summary["timers"].items()],
            key=lambda x: x[1],
            reverse=True
        )
        summary["hotspots"] = hotspots[:10]  # Top 10 hotspots
        
        return summary</code></pre>
                
                <p>This profiling infrastructure allows Bonsai v3 developers to identify and eliminate performance bottlenecks through:</p>
                
                <ul>
                    <li><strong>Time Profiling:</strong> Identifying which operations take the most time</li>
                    <li><strong>Memory Profiling:</strong> Identifying which operations use the most memory</li>
                    <li><strong>Call Profiling:</strong> Identifying which functions are called most frequently</li>
                    <li><strong>Hotspot Analysis:</strong> Focusing optimization efforts on the most critical areas</li>
                </ul>
                
                <p>By systematically identifying and eliminating bottlenecks, Bonsai v3 developers have achieved significant performance improvements over previous versions, enabling the system to handle much larger datasets with reasonable computational resources.</p>
            </div>
            
            <div class="concept-section">
                <h4>Regression Testing</h4>
                <p>To ensure that optimization improvements don't compromise accuracy, Bonsai v3 includes a comprehensive regression testing framework. This framework validates that optimized algorithms produce results that are consistent with reference implementations, allowing developers to confidently improve performance without sacrificing accuracy.</p>
                
                <p>The <code>performance_test.py</code> module implements this approach:</p>
                
                <pre><code>def run_performance_benchmark(
    test_datasets,
    reference_implementation,
    optimized_implementation,
    tolerance=1e-6,
):
    """
    Run a performance benchmark comparing reference and optimized implementations.
    
    Args:
        test_datasets: List of test datasets to use
        reference_implementation: Function implementing the reference algorithm
        optimized_implementation: Function implementing the optimized algorithm
        tolerance: Tolerance for numerical differences in results
        
    Returns:
        benchmark_results: Dict containing benchmark results
    """
    results = {
        "performance_improvement": {},
        "accuracy": {},
        "memory_improvement": {},
        "overall": {}
    }
    
    for dataset_name, dataset in test_datasets.items():
        print(f"Benchmarking {dataset_name}...")
        
        # Run reference implementation
        reference_start_time = time.time()
        reference_start_memory = get_memory_usage()
        reference_result = reference_implementation(dataset)
        reference_end_memory = get_memory_usage()
        reference_end_time = time.time()
        
        reference_time = reference_end_time - reference_start_time
        reference_memory = reference_end_memory - reference_start_memory
        
        # Run optimized implementation
        optimized_start_time = time.time()
        optimized_start_memory = get_memory_usage()
        optimized_result = optimized_implementation(dataset)
        optimized_end_memory = get_memory_usage()
        optimized_end_time = time.time()
        
        optimized_time = optimized_end_time - optimized_start_time
        optimized_memory = optimized_end_memory - optimized_start_memory
        
        # Calculate performance improvement
        time_improvement = (reference_time - optimized_time) / reference_time * 100
        memory_improvement = (reference_memory - optimized_memory) / reference_memory * 100
        
        # Check result accuracy
        is_accurate = check_results_match(reference_result, optimized_result, tolerance)
        
        # Record results
        results["performance_improvement"][dataset_name] = time_improvement
        results["memory_improvement"][dataset_name] = memory_improvement
        results["accuracy"][dataset_name] = is_accurate
        results["overall"][dataset_name] = {
            "reference_time": reference_time,
            "optimized_time": optimized_time,
            "reference_memory": reference_memory,
            "optimized_memory": optimized_memory,
            "time_improvement": time_improvement,
            "memory_improvement": memory_improvement,
            "is_accurate": is_accurate
        }
        
        print(f"  Time improvement: {time_improvement:.2f}%")
        print(f"  Memory improvement: {memory_improvement:.2f}%")
        print(f"  Accurate results: {is_accurate}")
    
    # Calculate overall statistics
    avg_time_improvement = sum(results["performance_improvement"].values()) / len(results["performance_improvement"])
    avg_memory_improvement = sum(results["memory_improvement"].values()) / len(results["memory_improvement"])
    all_accurate = all(results["accuracy"].values())
    
    results["summary"] = {
        "avg_time_improvement": avg_time_improvement,
        "avg_memory_improvement": avg_memory_improvement,
        "all_accurate": all_accurate
    }
    
    print(f"Overall time improvement: {avg_time_improvement:.2f}%")
    print(f"Overall memory improvement: {avg_memory_improvement:.2f}%")
    print(f"All results accurate: {all_accurate}")
    
    return results</code></pre>
                
                <p>This benchmark framework allows Bonsai v3 developers to:</p>
                
                <ul>
                    <li><strong>Measure Performance Improvements:</strong> Quantify time and memory improvements from optimizations</li>
                    <li><strong>Validate Accuracy:</strong> Ensure that optimized implementations produce results consistent with reference implementations</li>
                    <li><strong>Test Multiple Datasets:</strong> Verify that optimizations are effective across a range of dataset types and sizes</li>
                </ul>
                
                <p>By systematically benchmarking and validating optimizations, Bonsai v3 developers can maintain a balance between computational efficiency and accuracy, ensuring that the system can handle large datasets without sacrificing the quality of results.</p>
            </div>

            <div class="alert alert-success">
                <p><strong>Core Component:</strong> The optimization techniques employed in Bonsai v3 are what make large-scale pedigree reconstruction computationally feasible. Through a combination of search space pruning, parallel processing, adaptive parameter selection, specialized data structures, and careful performance profiling, Bonsai v3 can handle datasets orders of magnitude larger than would be possible with naive approaches. These optimizations are not just implementation details but fundamental enabling technologies that make it possible to apply pedigree reconstruction to real-world genetic genealogy datasets.</p>
            </div>
            
            <h3>Comparing Notebook and Bonsai v3</h3>
            
            <p>The Lab18 notebook explores optimization techniques and performance enhancements through simplified implementations and examples. While the notebook provides an educational introduction to the key concepts, the actual Bonsai v3 implementation includes additional sophistication:</p>
            
            <ul>
                <li><strong>Advanced Profiling Infrastructure:</strong> The production code includes more sophisticated profiling capabilities for identifying bottlenecks.</li>
                <li><strong>Extensive Optimization Tuning:</strong> The real implementation includes parameters that have been carefully tuned based on extensive benchmarking.</li>
                <li><strong>Environment-Specific Optimizations:</strong> The production code includes optimizations specific to different hardware and software environments.</li>
                <li><strong>Dynamic Load Balancing:</strong> More sophisticated approaches to balancing computational load across parallel workers.</li>
                <li><strong>Memory Pool Management:</strong> Custom memory management to minimize allocation overhead and fragmentation.</li>
            </ul>
            
            <p>These differences allow the production implementation to achieve maximum performance across a wide range of datasets and execution environments, while the notebook provides a more accessible introduction to the core optimization concepts.</p>

            <h3>Interactive Lab Environment</h3>
            
            <div class="jupyter-integration">
                <p>Click the button below to open the interactive Lab 18 notebook directly in your browser using JupyterLite. No installation required!</p>
                
                <div class="jupyterlite-container">
                    <div class="jupyterlite-info">
                        <h4>Browser-Based Analysis</h4>
                        <p>This link will open the <strong>Lab18_Optimization_Techniques.ipynb</strong> notebook in a browser-based Jupyter environment.</p>
                        <p>You can run the code cells by clicking on them and pressing Shift+Enter.</p>
                        <p><strong>Note:</strong> Your work is automatically saved in your browser's storage. If you clear your browser data, your progress may be lost. Be sure to download your notebook periodically ('File' -> 'Download') to save your work locally.</p>
                        <p><strong>Important:</strong> This notebook will be available when Lab18 is added to the labs_v2 directory.</p>
                    </div>
                    
                    <!-- This is now a link styled as a button -->
                    <a href="https://lakishadavid.github.io/computational_genetic_genealogy/jupyterlite_app/lab/index.html?path=labs_v2/Lab18_Optimization_Techniques.ipynb"
                       class="open-jupyterlite-link"
                       target="_blank"
                       role="button">
                        Open Lab 18 Notebook in JupyterLite
                    </a>
                </div>
            </div>

            <h3>Beyond the Code</h3>
            <p>As you explore optimization techniques and performance enhancements, consider these broader implications:</p>
            <ul>
                <li><strong>Scaling Challenges:</strong> How computational optimizations enable the application of genetic genealogy to increasingly large datasets</li>
                <li><strong>Accuracy vs. Speed Trade-offs:</strong> The balance between computational efficiency and accuracy in real-world applications</li>
                <li><strong>Hardware Limitations:</strong> How optimization strategies must adapt to different hardware environments and constraints</li>
                <li><strong>Algorithm Engineering:</strong> The importance of careful implementation and optimization beyond theoretical algorithmic complexity</li>
                <li><strong>Data-Driven Optimization:</strong> How understanding the structure of genetic genealogy data can inform specific optimization strategies</li>
            </ul>
            <p>These considerations highlight how optimization techniques are not just about making code run faster, but about enabling new applications and insights that would otherwise be computationally infeasible.</p>
            
            <div class="learning-pathway">
                <p>This lab is part of the Bonsai v3 Deep Dive track:</p>
                <div class="pathway-steps">
                    <div class="pathway-step">
                        <h5>Introduction</h5>
                        <p>Lab 01</p>
                    </div>
                    <div class="pathway-step">
                        <h5>Architecture</h5>
                        <p>Lab 02</p>
                    </div>
                    <div class="pathway-step">
                        <h5>IBD Formats</h5>
                        <p>Lab 03</p>
                    </div>
                    <div class="pathway-step">
                        <h5>Statistics</h5>
                        <p>Lab 04</p>
                    </div>
                    <div class="pathway-step">
                        <h5>Models</h5>
                        <p>Lab 05</p>
                    </div>
                    <div class="pathway-step">
                        <h5>Relationships</h5>
                        <p>Lab 06</p>
                    </div>
                    <div class="pathway-step">
                        <h5>PwLogLike</h5>
                        <p>Lab 07</p>
                    </div>
                    <div class="pathway-step">
                        <h5>Age Modeling</h5>
                        <p>Lab 08</p>
                    </div>
                    <div class="pathway-step">
                        <h5>Data Structures</h5>
                        <p>Lab 09</p>
                    </div>
                    <div class="pathway-step">
                        <h5>Up-Node Dict</h5>
                        <p>Lab 10</p>
                    </div>
                    <div class="pathway-step">
                        <h5>Connection Points</h5>
                        <p>Lab 11</p>
                    </div>
                    <div class="pathway-step">
                        <h5>Relationship Assessment</h5>
                        <p>Lab 12</p>
                    </div>
                    <div class="pathway-step">
                        <h5>Small Pedigrees</h5>
                        <p>Lab 13</p>
                    </div>
                    <div class="pathway-step">
                        <h5>Optimizing Pedigrees</h5>
                        <p>Lab 14</p>
                    </div>
                    <div class="pathway-step">
                        <h5>Combine Up Dicts</h5>
                        <p>Lab 15</p>
                    </div>
                    <div class="pathway-step">
                        <h5>Merging Pedigrees</h5>
                        <p>Lab 16</p>
                    </div>
                    <div class="pathway-step">
                        <h5>Incremental Addition</h5>
                        <p>Lab 17</p>
                    </div>
                    <div class="pathway-step active">
                        <h5>Optimization Techniques</h5>
                        <p>Lab 18</p>
                    </div>
                </div>
            </div>

            <div class="lab-navigation">
                <a href="lab17_incremental_addition.html" class="prev-lab">Incremental Addition</a>
                <a href="lab19_caching_mechanisms.html" class="next-lab">Caching Mechanisms</a>
            </div>
        </article>
    </main>

    <footer class="textbook-footer">
        <div class="container">
            <p>&copy; 2025 Dr. LaKisha David, Department of Anthropology, University of Illinois Urbana-Champaign</p>
        </div>
    </footer>
</body>
</html>