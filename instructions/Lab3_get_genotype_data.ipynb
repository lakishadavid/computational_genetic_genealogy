{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z4F5_zqkiJoU"
      },
      "source": [
        "Make sure your kernel (virtual environment) is selected.\n",
        "\n",
        "Run the download_fasta_file.ipynb notebook."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Environment Setup"
      ],
      "metadata": {
        "id": "tyrnlNJ2hiso"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import importlib.util"
      ],
      "metadata": {
        "id": "YZmLfSJqg2uR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "primary_directory = '/content'\n",
        "primary_directory"
      ],
      "metadata": {
        "id": "nduw3nmhg5kV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if primary_directory.startswith('/content'):\n",
        "\n",
        "  from google.colab import drive\n",
        "\n",
        "  drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "LkJZC1Dzg8ZQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define directories\n",
        "if primary_directory.startswith('/content'):\n",
        "    use_directory = \"/content/use\"\n",
        "else:\n",
        "    use_directory = os.path.join(primary_directory, \"use\")\n",
        "\n",
        "results_directory = os.path.join(primary_directory, \"results\")\n",
        "references_directory = os.path.join(primary_directory, \"references\")\n",
        "data_directory = os.path.join(primary_directory, \"data\")\n",
        "\n",
        "# Directories to check\n",
        "directories = [use_directory, results_directory, references_directory, data_directory]\n",
        "\n",
        "# Check if the directories exist and print a message\n",
        "for directory in directories:\n",
        "    if os.path.exists(directory):\n",
        "        print(f\"Directory exists: {directory}\")\n",
        "    else:\n",
        "        os.makedirs(directory)\n",
        "        print(f\"Directory created: {directory}\")"
      ],
      "metadata": {
        "id": "L65GYfvahCmi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to check if a package is installed\n",
        "def check_install_package(package_name, pip_name=None):\n",
        "    package_spec = importlib.util.find_spec(package_name)\n",
        "    if package_spec is None:\n",
        "        print(f\"{package_name} not found. Installing...\")\n",
        "        !pip install {pip_name if pip_name else package_name}\n",
        "    else:\n",
        "        print(f\"{package_name} is already installed.\")\n",
        "\n",
        "check_install_package('pandas', 'pandas')"
      ],
      "metadata": {
        "id": "816gBy1shVYD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Get Fasta file"
      ],
      "metadata": {
        "id": "yfUzoWU7in3l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash -s \"$references_directory\"\n",
        "\n",
        "references_directory=$1\n",
        "\n",
        "# Create the fasta directory if it does not exist\n",
        "mkdir -p ${references_directory}/fasta/\n",
        "\n",
        "# Fasta reference file for GRCh38 from Ensembl\n",
        "# Fasta reference file for GRCh38\n",
        "# from Ensembl at https://www.ensembl.org/\n",
        "# download the fasta reference file for GRCh38, if needed\n",
        "fasta_url=\"http://ftp.ensembl.org/pub/release-109/fasta/homo_sapiens/dna/Homo_sapiens.GRCh38.dna.primary_assembly.fa.gz\"\n",
        "fasta_file=\"${references_directory}/fasta/Homo_sapiens.GRCh38.dna.primary_assembly.fa\"\n",
        "\n",
        "# Check if the fasta file already exists\n",
        "if [ ! -f \"$fasta_file\" ]; then\n",
        "    echo \"FASTA file does not exist. Downloading...\"\n",
        "    wget -O \"${fasta_file}.gz\" $fasta_url && gunzip \"${fasta_file}.gz\"\n",
        "    if [ $? -ne 0 ]; then\n",
        "        echo \"Download or extraction failed.\"\n",
        "    else\n",
        "        echo \"Download and extraction completed successfully.\"\n",
        "    fi\n",
        "else\n",
        "    echo \"FASTA file already exists.\"\n",
        "fi\n",
        "\n",
        "# 25m 3.2s"
      ],
      "metadata": {
        "id": "nXKiry58i080"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Lab"
      ],
      "metadata": {
        "id": "VK2FmhIAikhH"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O0g8Ja3wiJoY"
      },
      "outputs": [],
      "source": [
        "import os # Imports Python's built-in os library, which allows us to interact with the operating system.\n",
        "import glob\n",
        "import shutil\n",
        "import csv\n",
        "import zipfile"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "output_dir = f\"{results_directory}/lineage_output\"\n",
        "if os.path.exists(output_dir):\n",
        "    shutil.rmtree(output_dir)\n",
        "    print(f\"Directory {output_dir} deleted.\")\n",
        "os.makedirs(output_dir)\n",
        "print(f\"Directory {output_dir} created.\")"
      ],
      "metadata": {
        "id": "Ss7zUZ6Q0JEO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "opensnp_data_directory = os.path.join(data_directory, \"opensnp_data\")\n",
        "\n",
        "# Check if the OpenSNP data directory exists\n",
        "if not os.path.exists(opensnp_data_directory):\n",
        "    # Create the OpenSNP data directory if it doesn't exist\n",
        "    os.makedirs(opensnp_data_directory)\n",
        "    print(f\"Created directory: {opensnp_data_directory}\")\n",
        "else:\n",
        "    print(f\"Directory already exists: {opensnp_data_directory}\")"
      ],
      "metadata": {
        "id": "xJNpTi_XlRP3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Manually upload the lab 2 data zip file in your opensnp data directory before running the next cell."
      ],
      "metadata": {
        "id": "zDuWu-9MjjUf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Specify the path to the zip file\n",
        "lab3_data_zip = os.path.join(opensnp_data_directory, \"lab3_data.zip\")\n",
        "\n",
        "# Open the zip file\n",
        "with zipfile.ZipFile(lab3_data_zip, \"r\") as zip_ref:\n",
        "    # Extract all the contents of the zip file to the specified directory\n",
        "    zip_ref.extractall(opensnp_data_directory)\n",
        "\n",
        "# Delete the zip file\n",
        "os.remove(lab3_data_zip)\n",
        "\n",
        "print(\"Zip file extracted and deleted successfully.\")"
      ],
      "metadata": {
        "id": "mhf09KnEkDqa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qe4U6h2DiJoc"
      },
      "outputs": [],
      "source": [
        "file_pattern = os.path.join(opensnp_data_directory, \"*.ancestry.txt\")\n",
        "file_pattern"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U6gFz26niJoc"
      },
      "outputs": [],
      "source": [
        "opensnp_files = glob.glob(file_pattern)\n",
        "opensnp_files"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n0SBKqdBiJod"
      },
      "source": [
        "opensnp_files is a list. Notice that the output above is enclosed in square brackets. This let's you know that it is a list."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ItX8Hy4kiJod"
      },
      "outputs": [],
      "source": [
        "opensnp_files[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bw2yE6cxiJoe"
      },
      "outputs": [],
      "source": [
        "opensnp_files[1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ovNiWcjziJoe"
      },
      "outputs": [],
      "source": [
        "opensnp_files[-1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "apRyUPYkiJoe"
      },
      "outputs": [],
      "source": [
        "opensnp_files[-2]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_LCzNCoLiJoe"
      },
      "outputs": [],
      "source": [
        "total_files = len(glob.glob(file_pattern))\n",
        "total_files"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rboFo4b2iJoe"
      },
      "outputs": [],
      "source": [
        "for file_path in opensnp_files:\n",
        "  print(\"This is the full path:\")\n",
        "  print(file_path)\n",
        "  filename = os.path.basename(file_path)\n",
        "  print(\"This is the file name:\")\n",
        "  print(filename)\n",
        "  print(\"\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q5AoVdQbiJoe"
      },
      "source": [
        "Print the cell below and notice the output for `print(count)`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dJzMLLFViJoe"
      },
      "outputs": [],
      "source": [
        "count = 0\n",
        "for file_path in opensnp_files:\n",
        "  # count =\n",
        "  print(count)\n",
        "  print(\"This is the full path:\")\n",
        "  print(file_path)\n",
        "  filename = os.path.basename(file_path)\n",
        "  print(\"This is the file name:\")\n",
        "  print(filename)\n",
        "  print(\"\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-OnD_PAQiJoe"
      },
      "source": [
        "Correct the cell above so that the counter works. Think about what you need to happen to the count value."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VG6Q_Y2BiJoe"
      },
      "outputs": [],
      "source": [
        "count = 0\n",
        "for file_path in opensnp_files:\n",
        "  count = count + 1\n",
        "  print(count)\n",
        "  print(\"This is the full path:\")\n",
        "  print(file_path)\n",
        "  filename = os.path.basename(file_path)\n",
        "  print(\"This is the file name:\")\n",
        "  print(filename)\n",
        "  print(\"\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qran9-k1iJof"
      },
      "outputs": [],
      "source": [
        "count = 0\n",
        "for file_path in opensnp_files:\n",
        "  count = count + 1\n",
        "  print(count)\n",
        "  print(\"This is the full path:\")\n",
        "  print(file_path)\n",
        "  filename = os.path.basename(file_path)\n",
        "  print(\"This is the file name:\")\n",
        "  print(filename)\n",
        "  file_bits = filename.split(\"_\")\n",
        "  print(\"These are the elements in the filename list:\")\n",
        "  print(file_bits)\n",
        "  username = file_bits[0]\n",
        "  print(\"We can use this to isolate the username.\")\n",
        "  print(username)\n",
        "  print(\"\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-09j1E6UiJof"
      },
      "outputs": [],
      "source": [
        "count = 0\n",
        "len_opensnp_files = len(opensnp_files)\n",
        "for file_path in opensnp_files:\n",
        "    count = count + 1\n",
        "    filename = os.path.basename(file_path)\n",
        "    username = filename.split(\"_\")[0]\n",
        "\n",
        "    print(f\"Processing file {count} in {len_opensnp_files}: {username}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GKccn2LKiJof"
      },
      "outputs": [],
      "source": [
        "!pip install lineage"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lKWIHY5DiJog"
      },
      "outputs": [],
      "source": [
        "from lineage import Lineage\n",
        "\n",
        "# https://snps.readthedocs.io/en/stable/\n",
        "# https://lineage.readthedocs.io/en/stable/\n",
        "\n",
        "# initialize Lineage object\n",
        "l = Lineage(\n",
        "    output_dir = output_dir,\n",
        "    resources_dir = f\"{references_directory}\",\n",
        "    parallelize = True,\n",
        "    processes = 8\n",
        ")\n",
        "\n",
        "# initialize dictionary variables\n",
        "individuals_dict = {}\n",
        "sex_determination = {}\n",
        "# initialize count variable\n",
        "count = 0\n",
        "\n",
        "directory_path = os.path.join(data_directory, \"opensnp_data\")\n",
        "file_pattern = os.path.join(directory_path, \"*.ancestry.txt\")\n",
        "opensnp_files = glob.glob(file_pattern)\n",
        "len_opensnp_files = len(opensnp_files)\n",
        "\n",
        "# Path for the sex determination TSV file\n",
        "sex_determination_file = os.path.join(results_directory, \"opensnp_sex_determination.tsv\")\n",
        "\n",
        "# Create a lineage individual object for each Ancestry file\n",
        "# Loop through file names and create individuals_dict\n",
        "for file_path in opensnp_files:\n",
        "    count = count + 1\n",
        "    filename = os.path.basename(file_path)\n",
        "    username = filename.split(\"_\")[0]\n",
        "\n",
        "    print(f\"Processing file {count} in {len_opensnp_files}: {username}\")\n",
        "\n",
        "    # print(username)\n",
        "    # assign_par_snps (bool) – assign PAR SNPs to the X and Y chromosomes\n",
        "    # with = True, error message: Chromosome PAR not remapped; removing chromosome from SNPs for consistency\n",
        "    # deduplicate_MT_chrom (bool) – deduplicate alleles on MT; see SNPs.heterozygous_MT\n",
        "    # deduplicate_XY_chrom (bool or str) – deduplicate alleles in the non-PAR regions of X and Y for males\n",
        "    # Why message: Chromosome PAR not remapped; removing chromosome from SNPs for consistency\n",
        "    individuals_dict[username] = l.create_individual(username,\n",
        "                                                     file=file_path,\n",
        "                                                     assign_par_snps=True,\n",
        "                                                     deduplicate_MT_chrom=True,\n",
        "                                                     deduplicate_XY_chrom=True)\n",
        "\n",
        "    if individuals_dict[username].build != 38:\n",
        "        individuals_dict[username].remap(38)\n",
        "\n",
        "    individuals_dict[username].sort()\n",
        "    individuals_dict[username].to_tsv(os.path.join(output_dir, f\"{username}.tsv\"))\n",
        "\n",
        "    # Determine sex\n",
        "    # heterozygous_x_snps_threshold (float) – percentage heterozygous X SNPs; above this threshold, Female is determined\n",
        "    # y_snps_not_null_threshold (float) – percentage Y SNPs that are not null; above this threshold, Male is determined\n",
        "    # chrom ({“X”, “Y”}) – use X or Y chromosome SNPs to determine sex\n",
        "    # Returns ‘Male’ or ‘Female’ if detected, else empty str\n",
        "    sex_determination[username] = individuals_dict[username].determine_sex(\n",
        "        heterozygous_x_snps_threshold=0.03,\n",
        "        y_snps_not_null_threshold=0.3,\n",
        "        chrom='X'\n",
        "        )\n",
        "    # print(sex_determination[username])\n",
        "\n",
        "# Save sex determinations to TSV\n",
        "with open(sex_determination_file, 'w', newline='') as file:\n",
        "    writer = csv.writer(file, delimiter='\\t')\n",
        "    writer.writerow(['Username', 'Sex'])\n",
        "    for username, sex in sex_determination.items():\n",
        "        writer.writerow([username, sex])\n",
        "\n",
        "print(\"All files processed.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "udDWTusKiJog"
      },
      "source": [
        "You can ignore the \"Chromosome PAR not remapped; removing chromosome from SNPs for consistency\" note for now."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WaVXT1QXiJoh"
      },
      "source": [
        "## Install bcftools and htslib"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash -s \"$primary_directory\" \"$use_directory\"\n",
        "\n",
        "primary_directory=$1\n",
        "use_directory=$2\n",
        "\n",
        "# Install dependencies\n",
        "sudo apt-get update\n",
        "sudo apt-get install -y tabix\n",
        "sudo apt-get install -y bcftools\n",
        "\n",
        "cd $use_directory\n",
        "git clone --recurse-submodules https://github.com/samtools/htslib.git\n",
        "git clone https://github.com/samtools/bcftools.git\n",
        "\n",
        "cd ${use_directory}/bcftools\n",
        "make\n",
        "export BCFTOOLS_PLUGINS=${use_directory}/bcftools/plugins\n",
        "\n",
        "cd $primary_directory"
      ],
      "metadata": {
        "id": "EkVoFdLjBR9x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Install gawk"
      ],
      "metadata": {
        "id": "JksU7lDrAgZ_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!sudo apt-get update\n",
        "!sudo apt-get install gawk"
      ],
      "metadata": {
        "id": "uadPUuxK_Ck3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Smz7xRlviJoh"
      },
      "outputs": [],
      "source": [
        "%%bash -s \"$results_directory\" \"$references_directory\"\n",
        "\n",
        "results_directory=$1\n",
        "references_directory=$2\n",
        "\n",
        "# Create the merged sample VCF file\n",
        "\n",
        "# First, convert the files from AncestryDNA tsv format to the 23andMe tsv format\n",
        "# Then, bcftools converts the data file from 23andMe tsv to vcf\n",
        "# Then, each vcf file is indexed\n",
        "# Finally, get a list of the individual vcf files and merge them into a single vcf file\n",
        "# The bcftools stats are from the final MergedSample.vcf file\n",
        "\n",
        "for file in ${results_directory}/lineage_output/*.tsv\n",
        "do\n",
        "    echo \"converting to vcf.gz: \" $file\n",
        "    # Create a new file with the modified format\n",
        "    new_file=\"${file%.tsv}_modified.tsv\"\n",
        "    gawk -F'\\t' '{ print $1\"\\t\"$2\"\\t\"$3\"\\t\"$4$5; }' $file > $new_file\n",
        "    bcftools convert -c ID,CHROM,POS,AA -s $(basename $file .tsv) \\\n",
        "            --haploid2diploid \\\n",
        "            -f ${references_directory}/fasta/Homo_sapiens.GRCh38.dna.primary_assembly.fa \\\n",
        "            --tsv2vcf $new_file \\\n",
        "            -Oz -o $(dirname $file)/$(basename $file .tsv).vcf.gz\n",
        "\n",
        "    echo \"indexing vcf file\" $(dirname $file)/$(basename $file .tsv).vcf.gz\n",
        "    bcftools index $(dirname $file)/$(basename $file .tsv).vcf.gz\n",
        "done"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q9va6cD1iJoi"
      },
      "source": [
        "### Understanding the output\n",
        "\n",
        "```\n",
        "Rows total: \t668742\n",
        "Rows skipped: \t7514\n",
        "Sites written: \t661228\n",
        "Missing GTs: \t12176\n",
        "Hom RR: \t349401\n",
        "Het RA: \t177931\n",
        "Hom AA: \t121589\n",
        "Het AA: \t131\n",
        "```\n",
        "\n",
        "\n",
        "\n",
        "`Rows total: 668742`\n",
        "\n",
        "Total number of variant records processed in the TSV file.\n",
        "\n",
        "`Rows skipped: 7514`\n",
        "\n",
        "The number of records that were skipped, potentially due to formatting issues or not meeting certain criteria for conversion.\n",
        "\n",
        "`Sites written: 661228`\n",
        "\n",
        "Number of variant sites successfully converted and written to the VCF file.\n",
        "\n",
        "`Missing GTs: 12176`\n",
        "\n",
        "GTs stands for Genotypes. This count indicates that there were 12,176 instances where the genotype information was missing.\n",
        "\n",
        "<br>\n",
        "In the following, `R` stands for reference and `A` stands for alternative.\n",
        "\n",
        "`Hom RR: 349401`\n",
        "\n",
        "Number of homozygous reference genotypes. In these cases, both alleles at a particular site match the reference genome.\n",
        "\n",
        "`Het RA: 177931`\n",
        "\n",
        "Number of heterozygous genotypes where one allele is the reference allele and the other is an alternative allele.\n",
        "\n",
        "`Hom AA: 121589`\n",
        "\n",
        "Number of homozygous alternative genotypes. Here, both alleles are the alternative variant, differing from the reference genome.\n",
        "\n",
        "`Het AA: 131`\n",
        "\n",
        "The count of heterozygous genotypes where both alleles are different alternative alleles (neither matches the reference)."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create a VCF file"
      ],
      "metadata": {
        "id": "GcgwJ0Hs5yGt"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yyOGpebtiJoi"
      },
      "outputs": [],
      "source": [
        "%%bash -s \"$results_directory\" \"$references_directory\"\n",
        "\n",
        "results_directory=$1\n",
        "references_directory=$2\n",
        "\n",
        "find ${results_directory}/lineage_output -type f -name \"*.vcf.gz\" > ${results_directory}/file_list.txt\n",
        "bcftools merge -o ${results_directory}/MergedSamples.vcf --file-list ${results_directory}/file_list.txt\n",
        "bcftools stats -s - ${results_directory}/MergedSamples.vcf > ${results_directory}/MergedSamples_step0_stats.vchk\n",
        "rm ${results_directory}/file_list.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1dwIKD7HiJoi"
      },
      "outputs": [],
      "source": [
        "# very basic parser for the MergedSamples stats file\n",
        "\n",
        "# Path to the stats file\n",
        "file_path = os.path.join(results_directory, \"MergedSamples_step0_stats.vchk\")\n",
        "\n",
        "# Function to parse the file\n",
        "def parse_summary_numbers(file_path):\n",
        "    sn_data = {}\n",
        "    capture = False\n",
        "    with open(file_path, 'r') as file:\n",
        "        for line in file:\n",
        "            if line.startswith('SN'):\n",
        "                print(line)\n",
        "\n",
        "    return\n",
        "\n",
        "# Call the function and process the results\n",
        "stats_data = parse_summary_numbers(file_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rxan2agCiJoj"
      },
      "source": [
        "## Quality Controls"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YdOj2AhkiJoj"
      },
      "outputs": [],
      "source": [
        "%%bash -s \"$use_directory\"\n",
        "\n",
        "# Define the directory and file URL\n",
        "use_directory=$1\n",
        "echo $use_directory\n",
        "\n",
        "plink2_file=\"https://s3.amazonaws.com/plink2-assets/alpha5/plink2_linux_x86_64_20240105.zip\"\n",
        "plink2_zip=$(basename \"$plink2_file\")\n",
        "\n",
        "# Check if the plink2 file already exists\n",
        "if [ ! -f \"${use_directory}/plink2\" ]; then\n",
        "    echo \"Downloading plink2...\"\n",
        "    # Download plink2\n",
        "    wget ${plink2_file} -P ${use_directory}\n",
        "    # Unzip the downloaded file\n",
        "    (cd \"${use_directory}\" && unzip \"${plink2_zip}\" && rm \"${plink2_zip}\")\n",
        "fi\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jxURdnyuiJok"
      },
      "outputs": [],
      "source": [
        "%%bash -s \"$results_directory\" \"$references_directory\" \"$use_directory\"\n",
        "\n",
        "results_directory=$1\n",
        "references_directory=$2\n",
        "use_directory=$3\n",
        "sample_file=MergedSamples\n",
        "\n",
        "# Quality Control\n",
        "\n",
        "# Perform qualty control on the MergedSample.vcf file using plink2\n",
        "\n",
        "# Quality control:\n",
        "# --autosome keep only autosomal SNPs\n",
        "# --rm-dup remove duplicate SNPs, keeping the first occurance\n",
        "# --vcf-half-call treat half-calls as missing\n",
        "# --snps-only outside of {'A', 'C', 'G', 'T', 'a', 'c', 'g', 't', <missing code>} are excluded\n",
        "# --min-alleles 2 and --max-alleles 2 keep SNPs where there are only 2 alleles\n",
        "# Split the MergedSamples_qc.vcf file by chromosome\n",
        "\n",
        "\n",
        "\n",
        "cd ${use_directory}\n",
        "\n",
        "./plink2 \\\n",
        "  --vcf ${results_directory}/${sample_file}.vcf \\\n",
        "  --autosome \\\n",
        "  --snps-only 'just-acgt' \\\n",
        "  --make-pgen \\\n",
        "  --out ${results_directory}/${sample_file}_step1\n",
        "\n",
        "# These comments are from a different run. Shown here for illustrative purposes only.\n",
        "# started with 816509 variants\n",
        "# --vcf: 783106 variants scanned (33403 skipped).\n",
        "# --vcf: 720k variants converted.\n",
        "# 783102 out of 783106 variants loaded\n",
        "# 783102 variants remaining after main filters.\n",
        "###################################################\n",
        "\n",
        "cd ${use_directory}\n",
        "\n",
        "./plink2 \\\n",
        "  --pfile ${results_directory}/${sample_file}_step1 \\\n",
        "  --rm-dup force-first \\\n",
        "  --min-alleles 2 \\\n",
        "  --max-alleles 2 \\\n",
        "  --make-pgen \\\n",
        "  --out ${results_directory}/${sample_file}_step2\n",
        "\n",
        "# These comments are from a different run. Shown here for illustrative purposes only.\n",
        "# 616190 out of 783102 variants loaded\n",
        "# --rm-dup: 5 duplicated IDs, 5 variants removed.\n",
        "# 616185 variants remaining after main filters.\n",
        "# #####################################################\n",
        "\n",
        "cd ${use_directory}\n",
        "\n",
        "./plink2 \\\n",
        "  --pfile ${results_directory}/${sample_file}_step2 \\\n",
        "  --geno 0.01 \\\n",
        "  --maf 0.01 \\\n",
        "  --sort-vars \\\n",
        "  --make-pgen \\\n",
        "  --out ${results_directory}/${sample_file}_step3\n",
        "\n",
        "# --mind 0.01 for sample missingness\n",
        "# #############################################################"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zhQzCWypiJok"
      },
      "outputs": [],
      "source": [
        "%%bash -s \"$results_directory\" \"$references_directory\" \"$use_directory\"\n",
        "\n",
        "# split by chromosome\n",
        "\n",
        "results_directory=$1\n",
        "references_directory=$2\n",
        "use_directory=$3\n",
        "sample_file=MergedSamples\n",
        "\n",
        "for chromosome in {1..22}\n",
        "do\n",
        "  cd ${use_directory}\n",
        "\n",
        "  ./plink2 \\\n",
        "    --pfile ${results_directory}/${sample_file}_step3 \\\n",
        "    --chr ${chromosome} \\\n",
        "    --export vcf \\\n",
        "    --out ${results_directory}/${sample_file}_qcfinished_chr${chromosome}\n",
        "\n",
        "  bgzip ${results_directory}/${sample_file}_qcfinished_chr${chromosome}.vcf\n",
        "  bcftools index -f ${results_directory}/${sample_file}_qcfinished_chr${chromosome}.vcf.gz\n",
        "done"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Zip and download your results directory"
      ],
      "metadata": {
        "id": "w4x-D8DlVlAo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "If you're using Google Colab, you'll need to download your results directory so that you can use it in subsequent labs. Run the following cell to create a zip file of your results directory. You can then download the zip file."
      ],
      "metadata": {
        "id": "Hp4pm-awWKbs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import zipfile\n",
        "\n",
        "def zip_directory(directory_path, zip_path):\n",
        "    with zipfile.ZipFile(zip_path, 'w', zipfile.ZIP_DEFLATED) as zipf:\n",
        "        for root, dirs, files in os.walk(directory_path):\n",
        "            for file in files:\n",
        "                zipf.write(os.path.join(root, file),\n",
        "                           os.path.relpath(os.path.join(root, file),\n",
        "                                           os.path.join(directory_path, '..')))\n",
        "\n",
        "# Specify the directory to be zipped\n",
        "directory_to_zip = \"/content/results\"\n",
        "\n",
        "# Specify the path where the zip file will be saved\n",
        "zip_file_path = \"/content/results.zip\"\n",
        "\n",
        "if use_directory == '/content/use':\n",
        "  # Create the zip file\n",
        "  zip_directory(directory_to_zip, zip_file_path)"
      ],
      "metadata": {
        "id": "AjCx58woVK9K"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}