{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from collections import Counter\n",
    "import logging\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import zipfile\n",
    "import urllib3\n",
    "from urllib3.util import Retry\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load your environment variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_comp_gen_dir():\n",
    "    \"\"\"Find the computational_genetic_genealogy directory by searching up from current directory.\"\"\"\n",
    "    current = Path.cwd()\n",
    "    \n",
    "    # Search up through parent directories\n",
    "    while current != current.parent:\n",
    "        # Check if target directory exists in current path\n",
    "        target = current / 'computational_genetic_genealogy'\n",
    "        if target.is_dir():\n",
    "            return target\n",
    "        # Move up one directory\n",
    "        current = current.parent\n",
    "    \n",
    "    raise FileNotFoundError(\"Could not find computational_genetic_genealogy directory\")\n",
    "\n",
    "def load_env_file():\n",
    "    \"\"\"Find and load the .env file from the computational_genetic_genealogy directory.\"\"\"\n",
    "    try:\n",
    "        # Find the computational_genetic_genealogy directory\n",
    "        comp_gen_dir = find_comp_gen_dir()\n",
    "        \n",
    "        # Look for .env file\n",
    "        env_path = comp_gen_dir / '.env'\n",
    "        if not env_path.exists():\n",
    "            print(f\"Warning: No .env file found in {comp_gen_dir}\")\n",
    "            return None\n",
    "        \n",
    "        # Load the .env file\n",
    "        load_dotenv(env_path, override=True)\n",
    "        print(f\"Loaded environment variables from: {env_path}\")\n",
    "        return env_path\n",
    "        \n",
    "    except FileNotFoundError as e:\n",
    "        print(f\"Error: {e}\")\n",
    "        return None\n",
    "\n",
    "# Use the function\n",
    "env_path = load_env_file()\n",
    "\n",
    "working_directory = os.getenv('PROJECT_WORKING_DIR', default=None)\n",
    "data_directory = os.getenv('PROJECT_DATA_DIR', default=None)\n",
    "references_directory = os.getenv('PROJECT_REFERENCES_DIR', default=None)\n",
    "results_directory = os.getenv('PROJECT_RESULTS_DIR', default=None)\n",
    "utils_directory = os.getenv('PROJECT_UTILS_DIR', default=None)\n",
    "\n",
    "os.environ[\"WORKING_DIRECTORY\"] = working_directory\n",
    "os.environ[\"DATA_DIRECTORY\"] = data_directory\n",
    "os.environ[\"REFERENCES_DIRECTORY\"] = references_directory\n",
    "os.environ[\"RESULTS_DIRECTORY\"] = results_directory\n",
    "os.environ[\"UTILS_DIRECTORY\"] = utils_directory\n",
    "\n",
    "print(f\"Working Directory: {working_directory}\")\n",
    "print(f\"Data Directory: {data_directory}\")\n",
    "print(f\"References Directory: {references_directory}\")\n",
    "print(f\"Results Directory: {results_directory}\")\n",
    "print(f\"Utils Directory: {utils_directory}\")\n",
    "\n",
    "os.chdir(working_directory)\n",
    "print(f\"The current directory is {os.getcwd()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let the environment know where bcftools is located."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"BCFTOOLS_PLUGINS\"] = \"/usr/lib/x86_64-linux-gnu/bcftools\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def configure_logging(log_filename, log_file_debug_level=\"INFO\", console_debug_level=\"INFO\"):\n",
    "    \"\"\"\n",
    "    Configure logging for both file and console handlers.\n",
    "\n",
    "    Args:\n",
    "        log_filename (str): Path to the log file where logs will be written.\n",
    "        log_file_debug_level (str): Logging level for the file handler.\n",
    "        console_debug_level (str): Logging level for the console handler.\n",
    "    \"\"\"\n",
    "    # Create a root logger\n",
    "    logger = logging.getLogger()\n",
    "    logger.setLevel(logging.DEBUG)  # Capture all messages at the root level\n",
    "\n",
    "    # Convert level names to numeric levels\n",
    "    file_level = getattr(logging, log_file_debug_level.upper(), logging.INFO)\n",
    "    console_level = getattr(logging, console_debug_level.upper(), logging.INFO)\n",
    "\n",
    "    # File handler: Logs messages at file_level and above to the file\n",
    "    file_handler = logging.FileHandler(log_filename)\n",
    "    file_handler.setLevel(file_level)\n",
    "    file_formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')\n",
    "    file_handler.setFormatter(file_formatter)\n",
    "\n",
    "    # Console handler: Logs messages at console_level and above to the console\n",
    "    console_handler = logging.StreamHandler(sys.stdout)\n",
    "    console_handler.setLevel(console_level)\n",
    "    console_formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')\n",
    "    console_handler.setFormatter(console_formatter)\n",
    "\n",
    "    # Add handlers to the root logger\n",
    "    logger.addHandler(file_handler)\n",
    "    logger.addHandler(console_handler)\n",
    "    \n",
    "def clear_logger():\n",
    "    \"\"\"Remove all handlers from the root logger.\"\"\"\n",
    "    logger = logging.getLogger()\n",
    "    for handler in logger.handlers[:]:\n",
    "        logger.removeHandler(handler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def configure_logging(log_filename, log_file_debug_level=\"INFO\", console_debug_level=\"INFO\"):\n",
    "    \"\"\"\n",
    "    Configure logging for both file and console handlers.\n",
    "\n",
    "    Args:\n",
    "        log_filename (str): Path to the log file where logs will be written.\n",
    "        log_file_debug_level (str): Logging level for the file handler.\n",
    "        console_debug_level (str): Logging level for the console handler.\n",
    "    \"\"\"\n",
    "    # Create a root logger\n",
    "    logger = logging.getLogger()\n",
    "    logger.setLevel(logging.DEBUG)  # Capture all messages at the root level\n",
    "\n",
    "    # Convert level names to numeric levels\n",
    "    file_level = getattr(logging, log_file_debug_level.upper(), logging.INFO)\n",
    "    console_level = getattr(logging, console_debug_level.upper(), logging.INFO)\n",
    "\n",
    "    # File handler: Logs messages at file_level and above to the file\n",
    "    file_handler = logging.FileHandler(log_filename)\n",
    "    file_handler.setLevel(file_level)\n",
    "    file_formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')\n",
    "    file_handler.setFormatter(file_formatter)\n",
    "\n",
    "    # Console handler: Logs messages at console_level and above to the console\n",
    "    console_handler = logging.StreamHandler(sys.stdout)\n",
    "    console_handler.setLevel(console_level)\n",
    "    console_formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')\n",
    "    console_handler.setFormatter(console_formatter)\n",
    "\n",
    "    # Add handlers to the root logger\n",
    "    logger.addHandler(file_handler)\n",
    "    logger.addHandler(console_handler)\n",
    "    \n",
    "def clear_logger():\n",
    "    \"\"\"Remove all handlers from the root logger.\"\"\"\n",
    "    logger = logging.getLogger()\n",
    "    for handler in logger.handlers[:]:\n",
    "        logger.removeHandler(handler)\n",
    "        \n",
    "log_filename = os.path.join(results_directory, \"lab3_log.txt\")\n",
    "print(f\"The Lab 3 log file is located at {log_filename}.\")\n",
    "\n",
    "# Ensure the results_directory exists\n",
    "if not os.path.exists(results_directory):\n",
    "    os.makedirs(results_directory)\n",
    "\n",
    "# Check if the file exists; if not, create it\n",
    "if not os.path.exists(log_filename):\n",
    "    with open(log_filename, 'w') as file:\n",
    "        pass  # The file is now created.\n",
    "    \n",
    "clear_logger() # Clear the logger before reconfiguring it\n",
    "configure_logging(log_filename, log_file_debug_level=\"INFO\", console_debug_level=\"INFO\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clear_logger() # Clear the logger before reconfiguring it\n",
    "configure_logging(log_filename, log_file_debug_level=\"INFO\", console_debug_level=\"INFO\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Genetic Maps\n",
    "\n",
    "Genetic maps, also known as recombination maps, are essential tools that illustrate the relative positions of genetic markers (such as single nucleotide polymorphisms, or SNPs) along a chromosome. Unlike physical maps that measure distances in base pairs, genetic maps measure distances in centiMorgans (cM), where one centiMorgan represents a 1% probability of recombination between markers during meiosis.\n",
    "\n",
    "## Key Components of Genetic Maps\n",
    "\n",
    "- **Markers:**  \n",
    "  Identifiable DNA sequences used as reference points on the genome.\n",
    "\n",
    "- **Recombination Frequency:**  \n",
    "  The probability of a recombination event occurring between markers, which informs the genetic distances.\n",
    "\n",
    "- **Map Distance:**  \n",
    "  Expressed in centiMorgans (cM), reflecting the likelihood of recombination rather than the physical distance.\n",
    "\n",
    "## BEAGLE's Genetic Map\n",
    "\n",
    "BEAGLE is a widely used software package for phasing, genotype imputation, and identity-by-descent (IBD) analysis. Its performance is closely tied to the use of high-resolution genetic maps. Here are some distinctive features of BEAGLE's genetic map:\n",
    "\n",
    "- **High Marker Density:**  \n",
    "  The genetic maps provided with BEAGLE include a dense array of markers. This density allows for the precise capture of fine-scale recombination events, which in turn improves the accuracy of haplotype phasing and genotype imputation.\n",
    "\n",
    "- **Species and Population Specificity:**  \n",
    "  The maps are often developed from extensive pedigree or population studies. For human genetic studies, they are constructed based on large-scale recombination data, ensuring relevance to the population under study.\n",
    "\n",
    "- **Integration with Statistical Models:**  \n",
    "  BEAGLE utilizes these maps within its statistical algorithms to model recombination events effectively. This integration is crucial for accurately inferring missing genotypes and detecting IBD segments.\n",
    "\n",
    "- **Enhanced Analysis Accuracy:**  \n",
    "  The detailed recombination information in BEAGLE's genetic maps allows for better adjustment for linkage disequilibrium and recombination rates, ultimately leading to more robust downstream genetic analyses.\n",
    "\n",
    "## Benefits of Using BEAGLE's Genetic Map\n",
    "\n",
    "- **Improved Phasing Accuracy:**  \n",
    "  The high-resolution data facilitates precise haplotype reconstruction, reducing errors in phase determination.\n",
    "\n",
    "- **Robust Genotype Imputation:**  \n",
    "  Detailed recombination rate data enhances the accuracy of imputing missing genotypes, ensuring more reliable datasets.\n",
    "\n",
    "- **Streamlined Analysis Workflow:**  \n",
    "  The genetic map is specifically tailored to integrate seamlessly with BEAGLE’s algorithms, thereby optimizing the overall analysis process.\n",
    "\n",
    "## References\n",
    "\n",
    "1. Browning, B. L., & Browning, S. R. (2007). *Rapid and Accurate Haplotype Phasing and Missing-Data Inference for Whole-Genome Association Studies by Use of Localized Haplotype Clustering*. [American Journal of Human Genetics](https://www.cell.com/AJHG/fulltext/S0002-9297(07)63882-8)\n",
    "2. Browning, B. L., Zhou, Y., & Browning, S. R. (2018). *A One-Penny Imputed Genome from Next-Generation Reference Panels*. [American Journal of Human Genetics](https://pubmed.ncbi.nlm.nih.gov/30100085/)\n",
    "3. [BEAGLE Documentation](https://faculty.washington.edu/browning/beagle/beagle.html)\n",
    "4. [NHGRI Glossary: Genetic Map](https://www.genome.gov/genetics-glossary/Genetic-Map)\n",
    "5. Li, Y., Willer, C., Sanna, S., & Abecasis, G. (2009). *Genotype Imputation*. [Annual Review of Genomics and Human Genetics](https://www.annualreviews.org/content/journals/10.1146/annurev.genom.9.081307.164242)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download Genetic Maps (Beagle's plink version)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define download helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_session_with_retries():\n",
    "    \"\"\"Create a requests session with retry strategy\"\"\"\n",
    "    retry_strategy = Retry(\n",
    "        total=5,\n",
    "        backoff_factor=0.5,\n",
    "        status_forcelist=[500, 502, 503, 504]\n",
    "    )\n",
    "    http = urllib3.PoolManager(retries=retry_strategy)\n",
    "    return http\n",
    "\n",
    "session = create_session_with_retries()\n",
    "\n",
    "def download_with_progress(url, output_path):\n",
    "    \"\"\"Download file with progress tracking\"\"\"\n",
    "    response = session.request('GET', url, preload_content=False)\n",
    "\n",
    "    if response.status != 200:\n",
    "        raise Exception(f\"HTTP error occurred: {response.status} {response.reason}\")\n",
    "    \n",
    "    total_size = int(response.headers.get('content-length', 0))\n",
    "    block_size = 8192\n",
    "    progress_increment = max(1, total_size // 50) if total_size > 0 else block_size\n",
    "    \n",
    "    with open(output_path, 'wb') as f:\n",
    "        downloaded = 0\n",
    "        last_print = 0\n",
    "        while True:\n",
    "            chunk = response.read(block_size)\n",
    "            if not chunk:\n",
    "                break\n",
    "                \n",
    "            f.write(chunk)\n",
    "            downloaded += len(chunk)\n",
    "            if total_size > 0 and downloaded - last_print >= progress_increment:\n",
    "                last_print = downloaded\n",
    "                progress = (downloaded / total_size) * 100\n",
    "                logging.info(f\"Download progress: {progress:.1f}%\")\n",
    "                \n",
    "    response.release_conn()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download the genetic map files from Beagle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "⚠️ Long-Running Operation: Download genetic maps for PLINK\n",
    "\n",
    "The next cell downloads genetic maps for multiple chromosomes. This operation:\n",
    "\n",
    "- Downloads approximately 120 MB of genetic map data\n",
    "- May take 5-10 minutes depending on your internet connection\n",
    "- Creates a set of genetic map files required for IBD detection\n",
    "\n",
    "If you already have these files in your references directory, this step will verify and skip files that already exist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up output directories\n",
    "genetic_maps_directory = os.path.join(references_directory, \"genetic_maps\")\n",
    "os.makedirs(genetic_maps_directory, exist_ok=True)\n",
    "\n",
    "beagle_genetic_maps = os.path.join(genetic_maps_directory, \"beagle_genetic_maps\")\n",
    "os.makedirs(beagle_genetic_maps, exist_ok=True)\n",
    "\n",
    "# Download BEAGLE genetic maps\n",
    "!poetry run python -m scripts_support.genetic_maps_download --data-source BEAGLE --assembly GRCh38"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### New Resource\n",
    "\n",
    "Take a look at your `genetic_maps` directory. You should see the `beagle_genetic_maps` directory. Within `beagle_genetic_maps`, you should see your genetic map files, one for each chromosome. The naming convention is `plink.chr{chromosome_number}.GRCh38.map`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VCF Quality Control and Processing Pipeline\n",
    "\n",
    "This script implements a comprehensive quality control (QC) and processing pipeline for merged VCF files, designed specifically for downstream genetic analyses (e.g., genetic genealogy). The pipeline integrates several tools (e.g., PLINK2, bcftools, Beagle) to perform quality control, filtering, and conversion of VCF files into other formats."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore the Code Chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Validate the Merged VCF file as input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you are returning back to this point after running subsequent labs, you need the results from the ped-sim notebook. Alternatively (e.g, if you don't have those results), run the following cell to copy prepared results from the instructor's run of the ped-sim notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy required files from class_data to results directory\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "# List of files to copy\n",
    "files_to_copy = [\n",
    "    \"ped_sim_run2-everyone.fam\",\n",
    "    \"ped_sim_run2.seg\",\n",
    "    \"ped_sim_run2.seg_dict.txt\",\n",
    "    \"pedigree.fam\"\n",
    "]\n",
    "\n",
    "# Copy each file\n",
    "for file in files_to_copy:\n",
    "    source = os.path.join(data_directory, \"class_data\", file)\n",
    "    destination = os.path.join(results_directory, file)\n",
    "    shutil.copy2(source, destination)\n",
    "    print(f\"Copied {file} to {results_directory}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Select your VCF file**\n",
    "\n",
    "In the next cell, uncomment the file you want to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the VCF file and directory to use\n",
    "# Uncomment the VCF file you want to use:\n",
    "\n",
    "# vcf_file = os.path.join(results_directory, \"merged_sample_autosomes_unphased.vcf.gz\")\n",
    "# vcf_directory = os.path.join(results_directory, \"real_data_autosomes\")\n",
    "\n",
    "vcf_file = os.path.join(data_directory, \"class_data\", \"merged_opensnps_data.vcf.gz\")\n",
    "vcf_directory = os.path.join(data_directory, \"class_data\", \"merged_opensnps_data_autosomes\")\n",
    "\n",
    "# vcf_file = os.path.join(results_directory, \"ped_sim_run2.vcf.gz\")\n",
    "# vcf_directory = os.path.join(results_directory, \"ped_sim_run2_autosomes\")\n",
    "\n",
    "# Check if files and directories exist\n",
    "if not os.path.exists(vcf_file):\n",
    "    print(f\"⚠️ Warning: The VCF file does not exist at {vcf_file}\")\n",
    "    print(\"Please check the path or run the previous labs to generate this file.\")\n",
    "    \n",
    "# Ensure the output directory exists\n",
    "if not os.path.exists(vcf_directory):\n",
    "    print(f\"Creating output directory: {vcf_directory}\")\n",
    "    os.makedirs(vcf_directory, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "\n",
    "def validate_merged_vcf(vcf_path):\n",
    "    \"\"\"Validate merged VCF and extract available chromosomes.\"\"\"\n",
    "    if not os.path.exists(vcf_path):\n",
    "        logging.error(f\"VCF file does not exist: {vcf_path}\")\n",
    "        return 0, 0, [], []\n",
    "        \n",
    "    try:\n",
    "        cmd_counts = [\"bcftools\", \"plugin\", \"counts\", vcf_path]\n",
    "        result_counts = subprocess.run(cmd_counts, capture_output=True, text=True, check=True)\n",
    "        logging.info(f\"Plugin 'counts' validation output for {vcf_path}:\\n{result_counts.stdout}\")\n",
    "        if result_counts.stderr:\n",
    "            logging.info(f\"Plugin 'counts' validation errors:\\n{result_counts.stderr}\")\n",
    "\n",
    "        num_samples = 0\n",
    "        for line in result_counts.stdout.splitlines():\n",
    "            if line.startswith(\"Number of samples:\"):\n",
    "                parts = line.split(\":\")\n",
    "                if len(parts) == 2:\n",
    "                    num_samples = int(parts[1].strip())\n",
    "        if not num_samples:\n",
    "            logging.error(f\"No sample count found in VCF file: {vcf_path}\")\n",
    "\n",
    "        num_snps = 0\n",
    "        for line in result_counts.stdout.splitlines():\n",
    "            if line.startswith(\"Number of SNPs:\"):\n",
    "                parts = line.split(\":\")\n",
    "                if len(parts) == 2:\n",
    "                    num_snps = int(parts[1].strip())\n",
    "        if not num_snps:\n",
    "            logging.error(f\"No sample count found in VCF file: {vcf_path}\")\n",
    "\n",
    "\n",
    "        logging.info(\"Extracting list of chromosomes from the VCF header.\")\n",
    "        cmd_chrom_contig = f\"bcftools view -h {vcf_path} | grep '^##contig' | cut -d'=' -f3 | cut -d',' -f1\"\n",
    "        result_chrom_contig = subprocess.run(cmd_chrom_contig, shell=True, capture_output=True, text=True, check=True)\n",
    "        chromosomes_contig = result_chrom_contig.stdout.splitlines()\n",
    "        if not chromosomes_contig:\n",
    "            logging.warning(f\"No chromosomes found in VCF file contig headers: {vcf_path}\")\n",
    "        else:\n",
    "            logging.debug(f\"Chromosomes found in VCF file header: {', '.join(chromosomes_contig)}\")\n",
    "\n",
    "\n",
    "        logging.info(\"Extracting a list of chromosomes from the CHROM column..\")\n",
    "        cmd_chrom_field = f\"bcftools query -f '%CHROM\\n' {vcf_path} | sort -u\"\n",
    "        result_chrom_field = subprocess.run(cmd_chrom_field, shell=True, capture_output=True, text=True, check=True)\n",
    "        chromosomes_field = result_chrom_field.stdout.splitlines()\n",
    "        if not chromosomes_field:\n",
    "            logging.error(f\"No chromosomes found in VCF file in the CHROM field: {vcf_path}\")\n",
    "        else:\n",
    "            logging.debug(f\"Chromosomes found in VCF file in the CHROM field: {', '.join(chromosomes_field)}\")\n",
    "\n",
    "\n",
    "        if chromosomes_contig and chromosomes_field and chromosomes_contig != chromosomes_field:\n",
    "            logging.warning(\"Mismatch between chromosomes in contig and field headers.\")\n",
    "            logging.warning(f\"Contig chromosomes: {chromosomes_contig}\")\n",
    "            logging.warning(f\"Field chromosomes: {chromosomes_field}\")\n",
    "\n",
    "\n",
    "        logging.info(\"Extracting sample IDs from the VCF file.\")\n",
    "        cmd_sample_list = [\"bcftools\", \"query\", \"-l\", vcf_path]\n",
    "        result_sample_list = subprocess.run(cmd_sample_list, capture_output=True, text=True, check=True)\n",
    "        sample_ids = result_sample_list.stdout.splitlines()\n",
    "\n",
    "        if not sample_ids:\n",
    "            logging.error(f\"No sample IDs found in VCF file: {vcf_path}\")\n",
    "        else:\n",
    "            logging.debug(f\"Sample IDs found in VCF file: {', '.join(sample_ids)}\")\n",
    "\n",
    "        return num_samples, num_snps, chromosomes_field, sample_ids\n",
    "        \n",
    "    except subprocess.CalledProcessError as e:\n",
    "        logging.error(f\"Error validating VCF file {vcf_path}: {e}\")\n",
    "        logging.error(f\"Command output: {e.stdout}\")\n",
    "        logging.error(f\"Command error: {e.stderr}\")\n",
    "        return 0, 0, [], []\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Unexpected error validating VCF file {vcf_path}: {e}\")\n",
    "        return 0, 0, [], []\n",
    "\n",
    "\n",
    "# VCF created in Lab3 Get Raw DNA Profile\n",
    "if os.path.exists(vcf_file):\n",
    "    num_samples, num_snps, chromosomes, sample_ids = validate_merged_vcf(vcf_file)\n",
    "    print(f\"VCF summary: {num_samples} samples, {num_snps} SNPs\")\n",
    "    print(f\"Chromosomes: {chromosomes}\")\n",
    "    print(f\"Sample IDs: {', '.join(sample_ids) if len(sample_ids) < 10 else f'{len(sample_ids)} samples'}\")\n",
    "else:\n",
    "    print(f\"⚠️ Cannot validate VCF file: {vcf_file} does not exist\")\n",
    "    print(\"Please check the file path or run the previous labs to generate this file.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** The ERROR between the Contig chromosomes and Field chromosomes are okay for now. Try to see why there is an error here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prepare the Supplemental Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_sex_determination(determined_sex_file, failed_sex):\n",
    "    \"\"\"Parse the sex determination log and create a mapping of user IDs to sexes.\"\"\"\n",
    "    sex_mapping = {}\n",
    "    with open(determined_sex_file, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "\n",
    "    for line in lines:\n",
    "        line = line.strip()\n",
    "        if not line:  # Skip empty lines\n",
    "            continue\n",
    "\n",
    "        user_id, sex = line.split(\"\\t\")\n",
    "        sex_mapping[user_id] = \"1\" if sex == \"Male\" else \"2\"\n",
    "\n",
    "    with open(failed_sex, 'r') as p:\n",
    "        lines = p.readlines()\n",
    "\n",
    "    for line in lines:\n",
    "        line = line.strip()\n",
    "        if not line:\n",
    "            continue\n",
    "\n",
    "        user_id, sex = line.split(\"\\t\")\n",
    "        sex_mapping[user_id] = \"0\" # Unknown sex\n",
    "\n",
    "    # Count occurrences of each sex code\n",
    "    counts = Counter(sex_mapping.values())\n",
    "\n",
    "    # Print results\n",
    "    logging.info(f\"Count of SEX=0 (Unknown): {counts['0']}\")\n",
    "    logging.info(f\"Count of SEX=1 (Male): {counts['1']}\")\n",
    "    logging.info(f\"Count of SEX=2 (Female): {counts['2']}\")\n",
    "\n",
    "    return sex_mapping\n",
    "\n",
    "def write_sex_files(sex_mapping, sample_ids, psam_file_all, psam_file_Y, sex_update_file):\n",
    "    \"\"\"Write both PLINK2-compatible .psam files and sex update file.\"\"\"\n",
    "    \n",
    "    # Reorder sex_mapping based on sample_ids\n",
    "    ordered_sex_mapping = {sample_id: sex_mapping.get(sample_id, \"0\") for sample_id in sample_ids}\n",
    "    \n",
    "    # Write standard .psam file for all chromosomes\n",
    "    with open(psam_file_all, 'w') as f:\n",
    "        f.write(\"#FID\\tIID\\tSEX\\n\")  # Header for .psam file\n",
    "        for user_id, sex_code in ordered_sex_mapping.items():\n",
    "            if sex_code == \"0\":\n",
    "                continue  # Exclude unknown sexes\n",
    "            f.write(f\"{user_id}\\t{user_id}\\t{sex_code}\\n\")\n",
    "    \n",
    "    # Write .psam file for Y chromosome (males only)\n",
    "    with open(psam_file_Y, 'w') as f:\n",
    "        f.write(\"#FID\\tIID\\tSEX\\n\")\n",
    "        for user_id, sex_code in ordered_sex_mapping.items():\n",
    "            if sex_code != \"1\":\n",
    "                continue  # Exclude non-males\n",
    "            f.write(f\"{user_id}\\t{user_id}\\t{sex_code}\\n\")\n",
    "    \n",
    "    # Write sex update file for PLINK2 --update-sex\n",
    "    with open(sex_update_file, 'w') as f:\n",
    "        f.write(\"#IID\\tSEX\\n\")  # PLINK2 format for sex update\n",
    "        for user_id, sex_code in ordered_sex_mapping.items():\n",
    "            if sex_code == \"0\":\n",
    "                continue  # Exclude unknown sexes\n",
    "            f.write(f\"{user_id}\\t{sex_code}\\n\")\n",
    "            \n",
    "\n",
    "determined_sex_file = f\"{data_directory}/class_data/determined_sex.txt\"\n",
    "failed_sex = f\"{data_directory}/class_data/failed_sex.txt\"\n",
    "\n",
    "sex_mapping = parse_sex_determination(determined_sex_file, failed_sex)\n",
    "base_name = os.path.splitext(determined_sex_file)[0]\n",
    "psam_file_all = f\"{base_name}_all.psam\"\n",
    "psam_file_Y = f\"{base_name}_Y.psam\"\n",
    "sex_update_file = f\"{base_name}_update_sex.txt\"\n",
    "write_sex_files(sex_mapping, sample_ids, psam_file_all, psam_file_Y, sex_update_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "⚠️ Long-Running Operation: Perform quality control on VCF files\n",
    "\n",
    "The next cell performs extensive quality control operations on the VCF files, including:\n",
    "\n",
    "- Filtering SNPs based on various criteria\n",
    "- Removing duplicate variants\n",
    "- Filtering on minor allele frequency\n",
    "- Sorting and indexing variants\n",
    "\n",
    "This process runs on **all chromosomes** and may take **15-30 minutes** to complete depending on your system performance. You'll see progress updates for each chromosome as it processes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash -s \"$vcf_file\" \"$vcf_directory\"\n",
    "\n",
    "vcf_file=\"$1\"\n",
    "vcf_directory=\"$2\"\n",
    "\n",
    "# Extract the file prefix (removing .vcf.gz extension)\n",
    "output_prefix=\"${vcf_file%.vcf.gz}\"\n",
    "base_name=$(basename \"$output_prefix\")\n",
    "bgzip -d \"$vcf_file\"\n",
    "bgzip \"${output_prefix}.vcf\"\n",
    "\n",
    "unphased_samples_directory=\"${vcf_directory}/unphased_samples\"\n",
    "mkdir -p \"$unphased_samples_directory\"\n",
    "\n",
    "# Define input and output files\n",
    "input_vcf=\"${vcf_file}\"\n",
    "temp_prefix=\"${results_directory}/temp\"\n",
    "bcftools index -t -f \"$input_vcf\"\n",
    "\n",
    "for chromosome in {1..22}; do\n",
    "    echo \"Processing chromosome ${chromosome}...\"\n",
    "    \n",
    "    output_vcf=\"${vcf_directory}/unphased_samples/${base_name}_qcfinished_chr${chromosome}.vcf.gz\"\n",
    "    \n",
    "    # Extended QC pipeline:\n",
    "    # 1. Select autosomal chromosome\n",
    "    # 2. Keep only biallelic SNPs\n",
    "    # -m2 keeps only variants with at least 2 alleles\n",
    "    # -M2 keeps only variants with at most 2 alleles\n",
    "    # could add: -i 'strlen(REF)=1 && strlen(ALT)=1' | \\\n",
    "    # 3. Remove exact duplicate variants\n",
    "    # 4. Filter on MAF and missing data\n",
    "    # 5. Sort variants\n",
    "    bcftools view \"$input_vcf\" \\\n",
    "        --regions \"${chromosome}\" \\\n",
    "        --types snps \\\n",
    "        -m2 -M2 \\\n",
    "        -i 'strlen(REF)=1 && strlen(ALT)=1' | \\\n",
    "    bcftools norm --rm-dup exact | \\\n",
    "    bcftools view \\\n",
    "        -q 0.05:minor \\\n",
    "        -i 'F_MISSING < 0.05' | \\\n",
    "    bcftools sort -Oz -o \"$output_vcf\"\n",
    "    \n",
    "    # Index the final VCF with force flag\n",
    "    bcftools index -f \"$output_vcf\"\n",
    "    \n",
    "    # Report number of variants\n",
    "    echo \"Number of variants in chromosome ${chromosome} after QC:\"\n",
    "    bcftools index -n \"$output_vcf\"\n",
    "    echo\n",
    "done"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Phase the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash -s \"$vcf_file\" \"$vcf_directory\"\n",
    "\n",
    "vcf_file=\"$1\"\n",
    "vcf_directory=\"$2\"\n",
    "echo \"vcf_file: $vcf_file\"\n",
    "echo \"vcf_directory: $vcf_directory\"\n",
    "\n",
    "# Extract the file prefix (removing .vcf.gz extension)\n",
    "output_prefix=\"${vcf_file%.vcf.gz}\"\n",
    "# Get base name of the VCF file\n",
    "base_name=$(basename \"$output_prefix\")\n",
    "echo \"base_name: ${base_name}\"\n",
    "\n",
    "unphased_directory=\"${vcf_directory}/unphased_samples\"\n",
    "phased_directory=\"${vcf_directory}/phased_samples\"\n",
    "beagle=\"${UTILS_DIRECTORY}/beagle.17Dec24.224.jar\"\n",
    "\n",
    "# Create the phased directory if it does not exist\n",
    "mkdir -p \"$phased_directory\"\n",
    "\n",
    "# Phase chromosomes using Beagle\n",
    "for chr in {1..22}; do\n",
    "    echo \"Processing chromosome ${chr}\"\n",
    "\n",
    "    INPUT_VCF=\"${unphased_directory}/${base_name}_qcfinished_chr${chr}.vcf.gz\"\n",
    "    REF_VCF=\"${REFERENCES_DIRECTORY}/onethousandgenomes_genotype/onethousandgenomes_genotyped_phased.chr${chr}.vcf.gz\"\n",
    "    MAP_FILE=\"${REFERENCES_DIRECTORY}/genetic_maps/beagle_genetic_maps/plink.chr${chr}.GRCh38.map\"\n",
    "    OUTPUT_PREFIX=\"${phased_directory}/${base_name}_phased_chr${chr}_temp\"\n",
    "    PHASED_VCF=\"${OUTPUT_PREFIX}.vcf.gz\"\n",
    "    TEMP_VCF=\"${phased_directory}/temp_chr${chr}.vcf.gz\"\n",
    "    SORTED_VCF=\"${phased_directory}/${base_name}_phased_chr${chr}.vcf.gz\"\n",
    "\n",
    "    # Check if input VCF exists\n",
    "    if [ ! -f \"${INPUT_VCF}\" ]; then\n",
    "        echo \"Input VCF file not found for chromosome ${chr}. Skipping.\"\n",
    "        echo \"${INPUT_VCF}\"\n",
    "        continue\n",
    "    fi\n",
    "\n",
    "    # Run Beagle phasing\n",
    "    if [ -f \"${REF_VCF}\" ]; then\n",
    "        echo \"Running Beagle with reference panel for chromosome ${chr}\"\n",
    "        java -jar ${beagle} \\\n",
    "            gt=\"${INPUT_VCF}\" \\\n",
    "            ref=\"${REF_VCF}\" \\\n",
    "            map=\"${MAP_FILE}\" \\\n",
    "            out=\"${OUTPUT_PREFIX}\" || {\n",
    "                echo \"Beagle failed for chromosome ${chr}. Skipping.\"\n",
    "                continue\n",
    "            }\n",
    "    else\n",
    "        echo \"Running Beagle without reference panel for chromosome ${chr}\"\n",
    "        java -jar ${beagle} \\\n",
    "            gt=\"${INPUT_VCF}\" \\\n",
    "            map=\"${MAP_FILE}\" \\\n",
    "            out=\"${OUTPUT_PREFIX}\" || {\n",
    "                echo \"Beagle failed for chromosome ${chr}. Skipping.\"\n",
    "                continue\n",
    "            }\n",
    "    fi\n",
    "\n",
    "    if [ ! -f \"${PHASED_VCF}\" ]; then\n",
    "        echo \"Phasing failed for chromosome ${chr}. Output file not found. Skipping.\"\n",
    "        continue\n",
    "    fi\n",
    "\n",
    "    # Index the file\n",
    "    tabix -f -p vcf \"${PHASED_VCF}\"\n",
    "    \n",
    "    # Add INFO field definition and sort\n",
    "    echo \"Sorting VCF for chromosome $CHR\"\n",
    "    bcftools annotate --header-lines <(echo '##INFO=<ID=END,Number=1,Type=Integer,Description=\"End position of the variant\">') \"${PHASED_VCF}\" | \\\n",
    "    bcftools sort -Oz -o \"${SORTED_VCF}\" || {\n",
    "        echo \"Sorting failed for chromosome $CHR\"\n",
    "        continue\n",
    "    }\n",
    "\n",
    "    # Index the sorted file\n",
    "    tabix -f -p vcf \"${SORTED_VCF}\"\n",
    "    \n",
    "    # If the sorted vcf and index exists, remove phased vcf and index\n",
    "    if [ -f \"${SORTED_VCF}\" ] && [ -f \"${SORTED_VCF}.tbi\" ]; then\n",
    "        rm -f \"${PHASED_VCF}\"\n",
    "        rm -f \"${PHASED_VCF}.tbi\"\n",
    "        rm -f \"${PHASED_VCF}.log\"\n",
    "    fi\n",
    "done"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's generate some stats on our files to manually inspect them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash -s \"$vcf_file\" \"$vcf_directory\"\n",
    "\n",
    "vcf_file=\"$1\"\n",
    "vcf_directory=\"$2\"\n",
    "echo \"vcf_file: $vcf_file\"\n",
    "echo \"vcf_directory: $vcf_directory\"\n",
    "\n",
    "# Extract the file prefix (removing .vcf.gz extension)\n",
    "output_prefix=\"${vcf_file%.vcf.gz}\"\n",
    "# Get base name of the VCF file\n",
    "base_name=$(basename \"$output_prefix\")\n",
    "echo \"base_name: ${base_name}\"\n",
    "\n",
    "phased_directory=\"${vcf_directory}/phased_samples\"\n",
    "\n",
    "# Generate stats for each chromosome\n",
    "for chr in {1..22}; do\n",
    "    PHASED_VCF=\"${phased_directory}/${base_name}_phased_chr${chr}.vcf.gz\"\n",
    "    echo \"PHASED_VCF: ${PHASED_VCF}\"\n",
    "    if [ -f \"$PHASED_VCF\" ]; then\n",
    "        STATS_OUTPUT=\"${phased_directory}/${base_name}_phased_chr${chr}_stats.vchk\"\n",
    "        bcftools stats -s - \"$PHASED_VCF\" > \"$STATS_OUTPUT\"\n",
    "        echo \"Stats generated for chromosome $chr. See: $STATS_OUTPUT\"\n",
    "    else\n",
    "        echo \"Phased VCF not found for chromosome $chr. Skipping stats generation.\"\n",
    "    fi\n",
    "done"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Concat the by-chromosome files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash -s \"$vcf_file\" \"$vcf_directory\"\n",
    "\n",
    "vcf_file=\"$1\"\n",
    "vcf_directory=\"$2\"\n",
    "echo \"vcf_file: $vcf_file\"\n",
    "echo \"vcf_directory: $vcf_directory\"\n",
    "\n",
    "# Extract the file prefix (removing .vcf.gz extension)\n",
    "output_prefix=\"${vcf_file%.vcf.gz}\"\n",
    "# Get base name of the VCF file\n",
    "base_name=$(basename \"$output_prefix\")\n",
    "echo \"base_name: ${base_name}\"\n",
    "\n",
    "phased_directory=\"${vcf_directory}/phased_samples\"\n",
    "\n",
    "merged_vcf=\"${RESULTS_DIRECTORY}/${base_name}_autosomes.vcf.gz\"\n",
    "\n",
    "# List of sorted VCFs\n",
    "vcf_list=()\n",
    "for chr in {1..22}; do\n",
    "    SORTED_VCF=\"${phased_directory}/${base_name}_phased_chr${chr}.vcf.gz\"\n",
    "    if [ -f \"$SORTED_VCF\" ]; then\n",
    "        vcf_list+=(\"$SORTED_VCF\")\n",
    "    else\n",
    "        echo \"Missing sorted VCF for chromosome ${chr}, skipping.\"\n",
    "        echo \"Missing $SORTED_VCF\"\n",
    "    fi\n",
    "done\n",
    "\n",
    "# Merge all VCFs\n",
    "if [ ${#vcf_list[@]} -gt 0 ]; then\n",
    "    echo \"Merging phased VCFs into a single autosomal file...\"\n",
    "    bcftools concat -Oz -o \"${merged_vcf}\" \"${vcf_list[@]}\" || { echo \"Merging failed.\"; exit 1; }\n",
    "\n",
    "    # Index the merged VCF\n",
    "    tabix -f -p vcf \"${merged_vcf}\"\n",
    "    echo \"Merged VCF created at ${merged_vcf}\"\n",
    "else\n",
    "    echo \"No VCFs available for merging.\"\n",
    "fi\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
