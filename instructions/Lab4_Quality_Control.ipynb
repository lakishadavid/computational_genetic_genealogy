{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from collections import Counter\n",
    "import logging\n",
    "import sys\n",
    "\n",
    "# Load environment variables from .env file\n",
    "from dotenv import load_dotenv\n",
    "notebook_dir = os.getcwd()\n",
    "project_root = os.path.dirname(notebook_dir)\n",
    "env_path = os.path.join(project_root, '.env')\n",
    "load_dotenv(env_path, override=True)\n",
    "\n",
    "print(env_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "working_directory = os.getenv('PROJECT_WORKING_DIR', default=None)\n",
    "data_directory = os.getenv('PROJECT_DATA_DIR', default=None)\n",
    "references_directory = os.getenv('PROJECT_REFERENCES_DIR', default=None)\n",
    "results_directory = os.getenv('PROJECT_RESULTS_DIR', default=None)\n",
    "utils_directory = os.getenv('PROJECT_UTILS_DIR', default=None)\n",
    "\n",
    "print(f\"Working Directory: {working_directory}\")\n",
    "print(f\"Data Directory: {data_directory}\")\n",
    "print(f\"References Directory: {references_directory}\")\n",
    "print(f\"Results Directory: {results_directory}\")\n",
    "print(f\"Utils Directory: {utils_directory}\")\n",
    "\n",
    "os.chdir(working_directory)\n",
    "print(f\"The current directory is {os.getcwd()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def configure_logging(log_filename, log_file_debug_level=\"INFO\", console_debug_level=\"INFO\"):\n",
    "    \"\"\"\n",
    "    Configure logging for both file and console handlers.\n",
    "\n",
    "    Args:\n",
    "        log_filename (str): Path to the log file where logs will be written.\n",
    "        log_file_debug_level (str): Logging level for the file handler.\n",
    "        console_debug_level (str): Logging level for the console handler.\n",
    "    \"\"\"\n",
    "    # Create a root logger\n",
    "    logger = logging.getLogger()\n",
    "    logger.setLevel(logging.DEBUG)  # Capture all messages at the root level\n",
    "\n",
    "    # Convert level names to numeric levels\n",
    "    file_level = getattr(logging, log_file_debug_level.upper(), logging.INFO)\n",
    "    console_level = getattr(logging, console_debug_level.upper(), logging.INFO)\n",
    "\n",
    "    # File handler: Logs messages at file_level and above to the file\n",
    "    file_handler = logging.FileHandler(log_filename)\n",
    "    file_handler.setLevel(file_level)\n",
    "    file_formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')\n",
    "    file_handler.setFormatter(file_formatter)\n",
    "\n",
    "    # Console handler: Logs messages at console_level and above to the console\n",
    "    console_handler = logging.StreamHandler(sys.stdout)\n",
    "    console_handler.setLevel(console_level)\n",
    "    console_formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')\n",
    "    console_handler.setFormatter(console_formatter)\n",
    "\n",
    "    # Add handlers to the root logger\n",
    "    logger.addHandler(file_handler)\n",
    "    logger.addHandler(console_handler)\n",
    "    \n",
    "def clear_logger():\n",
    "    \"\"\"Remove all handlers from the root logger.\"\"\"\n",
    "    logger = logging.getLogger()\n",
    "    for handler in logger.handlers[:]:\n",
    "        logger.removeHandler(handler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_filename = os.path.join(results_directory, \"lab4_log.txt\")\n",
    "print(f\"The Lab 4 log file is located at {log_filename}.\")\n",
    "\n",
    "# Ensure the results_directory exists\n",
    "if not os.path.exists(results_directory):\n",
    "    os.makedirs(results_directory)\n",
    "\n",
    "# Check if the file exists; if not, create it\n",
    "if not os.path.exists(log_filename):\n",
    "    with open(log_filename, 'w') as file:\n",
    "        pass  # The file is now created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "clear_logger() # Clear the logger before reconfiguring it\n",
    "configure_logging(log_filename, log_file_debug_level=\"INFO\", console_debug_level=\"INFO\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Genetic Maps\n",
    "\n",
    "Genetic maps, also known as recombination maps, are essential tools that illustrate the relative positions of genetic markers (such as single nucleotide polymorphisms, or SNPs) along a chromosome. Unlike physical maps that measure distances in base pairs, genetic maps measure distances in centiMorgans (cM), where one centiMorgan represents a 1% probability of recombination between markers during meiosis.\n",
    "\n",
    "## Key Components of Genetic Maps\n",
    "\n",
    "- **Markers:**  \n",
    "  Identifiable DNA sequences used as reference points on the genome.\n",
    "\n",
    "- **Recombination Frequency:**  \n",
    "  The probability of a recombination event occurring between markers, which informs the genetic distances.\n",
    "\n",
    "- **Map Distance:**  \n",
    "  Expressed in centiMorgans (cM), reflecting the likelihood of recombination rather than the physical distance.\n",
    "\n",
    "## BEAGLE's Genetic Map\n",
    "\n",
    "BEAGLE is a widely used software package for phasing, genotype imputation, and identity-by-descent (IBD) analysis. Its performance is closely tied to the use of high-resolution genetic maps. Here are some distinctive features of BEAGLE's genetic map:\n",
    "\n",
    "- **High Marker Density:**  \n",
    "  The genetic maps provided with BEAGLE include a dense array of markers. This density allows for the precise capture of fine-scale recombination events, which in turn improves the accuracy of haplotype phasing and genotype imputation.\n",
    "\n",
    "- **Species and Population Specificity:**  \n",
    "  The maps are often developed from extensive pedigree or population studies. For human genetic studies, they are constructed based on large-scale recombination data, ensuring relevance to the population under study.\n",
    "\n",
    "- **Integration with Statistical Models:**  \n",
    "  BEAGLE utilizes these maps within its statistical algorithms to model recombination events effectively. This integration is crucial for accurately inferring missing genotypes and detecting IBD segments.\n",
    "\n",
    "- **Enhanced Analysis Accuracy:**  \n",
    "  The detailed recombination information in BEAGLE's genetic maps allows for better adjustment for linkage disequilibrium and recombination rates, ultimately leading to more robust downstream genetic analyses.\n",
    "\n",
    "## Benefits of Using BEAGLE's Genetic Map\n",
    "\n",
    "- **Improved Phasing Accuracy:**  \n",
    "  The high-resolution data facilitates precise haplotype reconstruction, reducing errors in phase determination.\n",
    "\n",
    "- **Robust Genotype Imputation:**  \n",
    "  Detailed recombination rate data enhances the accuracy of imputing missing genotypes, ensuring more reliable datasets.\n",
    "\n",
    "- **Streamlined Analysis Workflow:**  \n",
    "  The genetic map is specifically tailored to integrate seamlessly with BEAGLEâ€™s algorithms, thereby optimizing the overall analysis process.\n",
    "\n",
    "## References\n",
    "\n",
    "1. Browning, B. L., & Browning, S. R. (2007). *Rapid and Accurate Haplotype Phasing and Missing-Data Inference for Whole-Genome Association Studies by Use of Localized Haplotype Clustering*. [American Journal of Human Genetics](https://www.cell.com/AJHG/fulltext/S0002-9297(07)63882-8)\n",
    "2. Browning, B. L., Zhou, Y., & Browning, S. R. (2018). *A One-Penny Imputed Genome from Next-Generation Reference Panels*. [American Journal of Human Genetics](https://pubmed.ncbi.nlm.nih.gov/30100085/)\n",
    "3. [BEAGLE Documentation](https://faculty.washington.edu/browning/beagle/beagle.html)\n",
    "4. [NHGRI Glossary: Genetic Map](https://www.genome.gov/genetics-glossary/Genetic-Map)\n",
    "5. Li, Y., Willer, C., Sanna, S., & Abecasis, G. (2009). *Genotype Imputation*. [Annual Review of Genomics and Human Genetics](https://www.annualreviews.org/content/journals/10.1146/annurev.genom.9.081307.164242)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash -s \"$data_directory\"\n",
    "\n",
    "data_directory=\"$1\"\n",
    "\n",
    "# For Beagle data:\n",
    "poetry run python -m scripts_support.genetic_maps_download --data-source BEAGLE --assembly GRCh38"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Quality Control"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This pipeline was designed such that running the following code block takes your VCF data as input, filters the data based on set metrics, and phases the data, and outputs the phased data by chromosome in the results directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash -s \"$data_directory\"\n",
    "\n",
    "data_directory=\"$1\"\n",
    "\n",
    "poetry run python -m scripts_work.quality_control_vcf \\\n",
    "    --vcf_file ${data_directory}/merged_opensnps_data.vcf.gz \\\n",
    "    --determined_sex_file ${data_directory}/class_data/determined_sex.txt \\\n",
    "    --failed_sex ${data_directory}/class_data/failed_sex.txt \\\n",
    "    --geno 0.05 --maf 0.05"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's it. You're now ready to run the IBD detection algorithm. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What happened?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, let's pause a moment to see what happened here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VCF Quality Control and Processing Pipeline\n",
    "\n",
    "This script implements a comprehensive quality control (QC) and processing pipeline for merged VCF files, designed specifically for downstream genetic analyses (e.g., genetic genealogy). The pipeline integrates several tools (e.g., PLINK2, bcftools, Beagle) to perform quality control, filtering, and conversion of VCF files into other formats.\n",
    "\n",
    "---\n",
    "\n",
    "## Overview\n",
    "\n",
    "The pipeline performs the following major steps:\n",
    "\n",
    "1. **Quality Control Filtering:**  \n",
    "   Applies a series of default QC filters to the VCF file:\n",
    "   - **Autosomal Filtering:** Only autosomal SNPs are retained (excludes X and Y chromosomes to simplify analysis).\n",
    "   - **Duplicate Removal:** Duplicated SNPs are removed (keeps the first occurrence).\n",
    "   - **VCF Half-Call Handling:** Half-calls are treated as missing.\n",
    "   - **SNP Filtering:** Keeps only SNPs with nucleotide calls in {A, C, G, T} (ignoring case).\n",
    "   - **Biallelic SNPs:** Filters for SNPs with exactly two alleles.\n",
    "   - **Genotype Missingness (`--geno`):** Excludes SNPs with a missingness rate exceeding the specified threshold.\n",
    "   - **Minor Allele Frequency (`--maf`):** Excludes SNPs with a frequency lower than the specified threshold.\n",
    "\n",
    "2. **Validation and Dependency Checks:**  \n",
    "   - Validates the merged VCF file by counting samples, SNPs, and verifying chromosome headers.\n",
    "   - Checks that required tools (e.g., `bcftools`, `bgzip`, `plink2`) are installed and attempts to install missing components like the Beagle JAR dynamically.\n",
    "\n",
    "3. **Sex Determination Processing:**  \n",
    "   - Parses input files containing sex determination results.\n",
    "   - Writes PLINK2-compatible sex files (including `.psam` and sex update files) for further analysis.\n",
    "\n",
    "4. **Data Conversion and Filtering:**  \n",
    "   - **Step 1:** Converts the VCF file into PLINK format using `plink2` with QC parameters.\n",
    "   - **Step 2:** Filters the PLINK data by genotype missingness and minor allele frequency.\n",
    "   - **Step 3:** Splits the dataset by chromosome and exports each chromosomeâ€™s data as a VCF file.  \n",
    "     This step also includes filtering for biallelic variants, compressing, and indexing the output VCF files.\n",
    "\n",
    "5. **Processing Special Chromosomes:**  \n",
    "   - Dedicated routines handle chromosomes X, Y, and MT (mitochondrial) separately, as they may require additional processing (e.g., PAR splitting for chromosome X).\n",
    "\n",
    "6. **Phasing:**  \n",
    "   - After QC and filtering, the script calls an external shell script (`phase_chromosomes.sh`) to phase chromosomes using Beagle.  \n",
    "   - The script passes an input file prefix (derived from the processed VCF file) along with directory paths and the Beagle JAR file for further phasing operations.\n",
    "\n",
    "7. **Logging and Error Handling:**  \n",
    "   - A robust logging mechanism is set up to capture both console and file outputs, enabling detailed tracking of the pipelineâ€™s progress and errors.\n",
    "   - The script employs exception handling to catch failures at different steps, ensuring the pipeline stops if critical issues are detected.\n",
    "\n",
    "---\n",
    "\n",
    "## Key Functions and Their Roles\n",
    "\n",
    "- **`configure_logging()`**  \n",
    "  Sets up file and console logging with configurable debug levels.\n",
    "\n",
    "- **`parse_arguments()`**  \n",
    "  Parses command-line arguments to customize parameters such as the input VCF file, QC thresholds, and sex determination files.\n",
    "\n",
    "- **`check_dependencies()`**  \n",
    "  Verifies that external tools (`bcftools`, `bgzip`, `plink2`) are available. It dynamically locates (or installs) the Beagle JAR file if it is not found.\n",
    "\n",
    "- **`validate_merged_vcf()` and `validate_vcf()`**  \n",
    "  Validate the input VCF file by:\n",
    "  - Counting the number of samples and SNPs.\n",
    "  - Extracting and comparing chromosome lists from the VCF header and the CHROM field.\n",
    "  - Extracting sample IDs.\n",
    "\n",
    "- **`parse_sex_determination()` and `write_sex_files()`**  \n",
    "  Process sex determination outputs to generate files compatible with PLINK2, ensuring that downstream analyses correctly interpret the sex of each sample.\n",
    "\n",
    "- **Conversion and Filtering Steps (`step_1_convert_vcf_to_plink()`, `step_2_filter_genotype_and_maf()`, `step_3_split_by_chromosome()`)**  \n",
    "  Convert VCF data to PLINK format and apply additional QC filters. The pipeline then splits the data by chromosome and generates per-chromosome VCF outputs.\n",
    "\n",
    "- **Special Chromosome Processing (`step_process_X()`, `step_process_Y()`, `step_process_MT()`)**  \n",
    "  Handle conversion, filtering, and exporting for chromosomes X, Y, and MT. Each function is tailored to address the unique challenges associated with these chromosomes (e.g., sex update for chromosome X).\n",
    "\n",
    "- **`run_command()`**  \n",
    "  A helper function to execute shell commands, suppressing or capturing output as necessary, with error handling.\n",
    "\n",
    "- **`main()`**  \n",
    "  The main entry point that ties all the steps together. It orchestrates:\n",
    "  - Validation of the VCF file.\n",
    "  - QC and filtering processes.\n",
    "  - Dependency checks and installation steps.\n",
    "  - Execution of the phasing script.\n",
    "\n",
    "---\n",
    "\n",
    "## Usage Example\n",
    "\n",
    "To run the script using Poetry, one might call:\n",
    "\n",
    "```bash\n",
    "poetry run python -m scripts_work.quality_control_vcf \\\n",
    "    --vcf_file data/open_snps_data/opensnps.vcf.gz \\\n",
    "    --determined_sex_file data/open_snps_data/determined_sex.txt \\\n",
    "    --failed_sex data/open_snps_data/failed_sex.txt \\\n",
    "    --geno 0.05 --maf 0.05\n",
    "```\n",
    "\n",
    "This command initiates the QC pipeline with specified thresholds for genotype missingness (--geno) and minor allele frequency (--maf), along with input files for VCF and sex determination.\n",
    "\n",
    "## Conclusion\n",
    "This pipeline is designed to provide a robust, modular framework for processing VCF files in genetic studies. By integrating multiple tools and configurable QC parameters, it balances stringency and flexibility, ensuring a high-quality dataset for downstream analyses in genetic genealogy and population genetics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "\n",
    "def validate_merged_vcf(vcf_path):\n",
    "    \"\"\"Validate merged VCF and extract available chromosomes.\"\"\"\n",
    "    cmd_counts = [\"bcftools\", \"plugin\", \"counts\", vcf_path]\n",
    "    result_counts = subprocess.run(cmd_counts, capture_output=True, text=True, check=True)\n",
    "    logging.info(f\"Plugin 'counts' validation output for {vcf_path}:\\n{result_counts.stdout}\")\n",
    "    if result_counts.stderr:\n",
    "        logging.info(f\"Plugin 'counts' validation errors:\\n{result_counts.stderr}\")\n",
    "\n",
    "    num_samples = 0\n",
    "    for line in result_counts.stdout.splitlines():\n",
    "        if line.startswith(\"Number of samples:\"):\n",
    "            parts = line.split(\":\")\n",
    "            if len(parts) == 2:\n",
    "                num_samples = int(parts[1].strip())\n",
    "    if not num_samples:\n",
    "        logging.error(f\"No sample count found in VCF file: {vcf_path}\")\n",
    "\n",
    "    num_snps = 0\n",
    "    for line in result_counts.stdout.splitlines():\n",
    "        if line.startswith(\"Number of SNPs:\"):\n",
    "            parts = line.split(\":\")\n",
    "            if len(parts) == 2:\n",
    "                num_snps = int(parts[1].strip())\n",
    "    if not num_snps:\n",
    "        logging.error(f\"No sample count found in VCF file: {vcf_path}\")\n",
    "\n",
    "\n",
    "    logging.info(\"Extracting list of chromosomes from the VCF header.\")\n",
    "    cmd_chrom_contig = f\"bcftools view -h {vcf_path} | grep '^##contig' | cut -d'=' -f3 | cut -d',' -f1\"\n",
    "    result_chrom_contig = subprocess.run(cmd_chrom_contig, shell=True, capture_output=True, text=True, check=True)\n",
    "    chromosomes_contig = result_chrom_contig.stdout.splitlines()\n",
    "    if not chromosomes_contig:\n",
    "        logging.error(f\"No chromosomes found in VCF file: {vcf_path}\")\n",
    "    else:\n",
    "        logging.debug(f\"Chromosomes found in VCF file header: {', '.join(chromosomes_contig)}\")\n",
    "\n",
    "\n",
    "    logging.info(\"Extracting a list of chromosomes from the CHROM column..\")\n",
    "    cmd_chrom_field = f\"bcftools query -f '%CHROM\\n' {vcf_path} | sort -u\"\n",
    "    result_chrom_field = subprocess.run(cmd_chrom_field, shell=True, capture_output=True, text=True, check=True)\n",
    "    chromosomes_field = result_chrom_field.stdout.splitlines()\n",
    "    if not chromosomes_field:\n",
    "        logging.error(f\"No chromosomes found in VCF file in the CHROM field: {vcf_path}\")\n",
    "    else:\n",
    "        logging.debug(f\"Chromosomes found in VCF file in the CHROM field: {', '.join(chromosomes_field)}\")\n",
    "\n",
    "\n",
    "    if chromosomes_contig != chromosomes_field:\n",
    "        logging.error(\"Mismatch between chromosomes in contig and field headers.\")\n",
    "        logging.error(f\"Contig chromosomes: {chromosomes_contig}\")\n",
    "        logging.error(f\"Field chromosomes: {chromosomes_field}\")\n",
    "\n",
    "\n",
    "    logging.info(\"Extracting sample IDs from the VCF file.\")\n",
    "    cmd_sample_list = [\"bcftools\", \"query\", \"-l\", vcf_path]\n",
    "    result_sample_list = subprocess.run(cmd_sample_list, capture_output=True, text=True, check=True)\n",
    "    sample_ids = result_sample_list.stdout.splitlines()\n",
    "\n",
    "    if not sample_ids:\n",
    "        logging.error(f\"No sample IDs found in VCF file: {vcf_path}\")\n",
    "    else:\n",
    "        logging.debug(f\"Sample IDs found in VCF file: {', '.join(sample_ids)}\")\n",
    "\n",
    "    return num_samples, num_snps, chromosomes_field, sample_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pick up from lab 3\n",
    "vcf_path = f\"{data_directory}/merged_opensnps_data.vcf.gz\"\n",
    "num_samples, num_snps, chromosomes, sample_ids = validate_merged_vcf(vcf_path)\n",
    "print(num_samples, num_snps, chromosomes, sample_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_sex_determination(determined_sex_file, failed_sex):\n",
    "    \"\"\"Parse the sex determination log and create a mapping of user IDs to sexes.\"\"\"\n",
    "    sex_mapping = {}\n",
    "    with open(determined_sex_file, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "\n",
    "    for line in lines:\n",
    "        line = line.strip()\n",
    "        if not line:  # Skip empty lines\n",
    "            continue\n",
    "\n",
    "        user_id, sex = line.split(\"\\t\")\n",
    "        sex_mapping[user_id] = \"1\" if sex == \"Male\" else \"2\"\n",
    "\n",
    "    with open(failed_sex, 'r') as p:\n",
    "        lines = p.readlines()\n",
    "\n",
    "    for line in lines:\n",
    "        line = line.strip()\n",
    "        if not line:\n",
    "            continue\n",
    "\n",
    "        user_id, sex = line.split(\"\\t\")\n",
    "        sex_mapping[user_id] = \"0\" # Unknown sex\n",
    "\n",
    "    # Count occurrences of each sex code\n",
    "    counts = Counter(sex_mapping.values())\n",
    "\n",
    "    # Print results\n",
    "    logging.info(f\"Count of SEX=0 (Unknown): {counts['0']}\")\n",
    "    logging.info(f\"Count of SEX=1 (Male): {counts['1']}\")\n",
    "    logging.info(f\"Count of SEX=2 (Female): {counts['2']}\")\n",
    "\n",
    "    return sex_mapping\n",
    "\n",
    "def write_sex_files(sex_mapping, sample_ids, psam_file_all, psam_file_Y, sex_update_file):\n",
    "    \"\"\"Write both PLINK2-compatible .psam files and sex update file.\"\"\"\n",
    "    \n",
    "    # Reorder sex_mapping based on sample_ids\n",
    "    ordered_sex_mapping = {sample_id: sex_mapping.get(sample_id, \"0\") for sample_id in sample_ids}\n",
    "    \n",
    "    # Write standard .psam file for all chromosomes\n",
    "    with open(psam_file_all, 'w') as f:\n",
    "        f.write(\"#FID\\tIID\\tSEX\\n\")  # Header for .psam file\n",
    "        for user_id, sex_code in ordered_sex_mapping.items():\n",
    "            if sex_code == \"0\":\n",
    "                continue  # Exclude unknown sexes\n",
    "            f.write(f\"{user_id}\\t{user_id}\\t{sex_code}\\n\")\n",
    "    \n",
    "    # Write .psam file for Y chromosome (males only)\n",
    "    with open(psam_file_Y, 'w') as f:\n",
    "        f.write(\"#FID\\tIID\\tSEX\\n\")\n",
    "        for user_id, sex_code in ordered_sex_mapping.items():\n",
    "            if sex_code != \"1\":\n",
    "                continue  # Exclude non-males\n",
    "            f.write(f\"{user_id}\\t{user_id}\\t{sex_code}\\n\")\n",
    "    \n",
    "    # Write sex update file for PLINK2 --update-sex\n",
    "    with open(sex_update_file, 'w') as f:\n",
    "        f.write(\"#IID\\tSEX\\n\")  # PLINK2 format for sex update\n",
    "        for user_id, sex_code in ordered_sex_mapping.items():\n",
    "            if sex_code == \"0\":\n",
    "                continue  # Exclude unknown sexes\n",
    "            f.write(f\"{user_id}\\t{sex_code}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "determined_sex_file = f\"{data_directory}/class_data/determined_sex.txt\"\n",
    "failed_sex = f\"{data_directory}/class_data/failed_sex.txt\"\n",
    "\n",
    "sex_mapping = parse_sex_determination(determined_sex_file, failed_sex)\n",
    "base_name = os.path.splitext(determined_sex_file)[0]\n",
    "psam_file_all = f\"{base_name}_all.psam\"\n",
    "psam_file_Y = f\"{base_name}_Y.psam\"\n",
    "sex_update_file = f\"{base_name}_update_sex.txt\"\n",
    "write_sex_files(sex_mapping, sample_ids, psam_file_all, psam_file_Y, sex_update_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash -s \"$data_directory\" \"$utils_directory\" \"$results_directory\"\n",
    "\n",
    "data_directory=\"$1\"\n",
    "utils_directory=\"$2\"\n",
    "results_directory=\"$3\"\n",
    "\n",
    "#The first step of the pipeline involves parsing command-line arguments and applying default quality control parameters to the merged VCF file. These filters include:\n",
    "\n",
    "# - **Autosomal Filtering:** Only autosomal SNPs are retained.\n",
    "# - **Duplicate Removal:** Duplicate SNPs are removed (keeping the first occurrence).\n",
    "# - **VCF Half-Call Handling:** Half-calls are treated as missing.\n",
    "# - **SNP Filtering:** Only SNPs with nucleotide types {A, C, G, T} are kept.\n",
    "# - **Biallelic SNPs:** Only SNPs with exactly two alleles are retained.\n",
    "# - **Genotype Missingness (`--geno`):** Excludes SNPs with a missingness rate above a threshold.\n",
    "# - **Minor Allele Frequency (`--maf`):** Excludes SNPs with a frequency below a threshold.\n",
    "\n",
    "# Change to the utils directory if necessary\n",
    "cd \"${utils_directory}\"\n",
    "\n",
    "\n",
    "plink2 --vcf ${data_directory}/merged_opensnps_data.vcf.gz \\\n",
    "  --autosome \\\n",
    "  --snps-only just-acgt \\\n",
    "  --rm-dup exclude-all \\\n",
    "  --min-alleles 2 \\\n",
    "  --max-alleles 2 \\\n",
    "  --make-pgen \\\n",
    "  --out ${results_directory}/opensnps_autosomes_step1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash -s \"$data_directory\" \"$utils_directory\" \"$results_directory\"\n",
    "\n",
    "data_directory=\"$1\"\n",
    "utils_directory=\"$2\"\n",
    "results_directory=\"$3\"\n",
    "\n",
    "# Change to the utils directory if necessary\n",
    "cd \"${utils_directory}\"\n",
    "\n",
    "echo \"Filtering by genotype missingness (geno=${geno}) and minor allele frequency (maf=${maf})...\"\n",
    "plink2 --pfile ${results_directory}/opensnps_autosomes_step1 \\\n",
    "  --geno .05 \\\n",
    "  --maf .05 \\\n",
    "  --sort-vars \\\n",
    "  --make-pgen \\\n",
    "  --out ${results_directory}/opensnps_autosomes_step2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash -s \"$data_directory\" \"$utils_directory\" \"$results_directory\"\n",
    "\n",
    "data_directory=\"$1\"\n",
    "utils_directory=\"$2\"\n",
    "results_directory=\"$3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash -s \"$data_directory\" \"$utils_directory\" \"$results_directory\"\n",
    "\n",
    "data_directory=\"$1\"\n",
    "utils_directory=\"$2\"\n",
    "results_directory=\"$3\"\n",
    "\n",
    "# Derive the sample base name from the merged VCF file name (e.g., \"opensnps\")\n",
    "sample_file=$(basename \"${data_directory}/merged_opensnps_data.vcf.gz\" | cut -d. -f1)\n",
    "\n",
    "# The input prefix is the output from step 2\n",
    "input_prefix=\"${results_directory}/opensnps_autosomes_step2\"\n",
    "\n",
    "echo \"Splitting by chromosome and exporting as VCF...\"\n",
    "\n",
    "# Loop over autosomal chromosomes 1 through 22\n",
    "for chromosome in {1..22}; do\n",
    "    echo \"Processing chromosome ${chromosome}...\"\n",
    "    \n",
    "    # Define the output prefix for the current chromosome\n",
    "    output_prefix=\"${results_directory}/${sample_file}_qc_chr${chromosome}\"\n",
    "    \n",
    "    # Export the chromosome-specific data as a VCF using plink2\n",
    "    plink2 --pfile \"$input_prefix\" \\\n",
    "           --chr \"${chromosome}\" \\\n",
    "           --export vcf \\\n",
    "           --out \"$output_prefix\"\n",
    "    \n",
    "    # Define the input and output for bcftools processing\n",
    "    bcftools_input=\"${output_prefix}.vcf\"\n",
    "    bcftools_output=\"${results_directory}/${sample_file}_qcfinished_chr${chromosome}.vcf.gz\"\n",
    "    \n",
    "    echo \"Filtering for biallelic variants on chromosome ${chromosome}...\"\n",
    "    # Filter for biallelic variants and compress the VCF using bcftools\n",
    "    bcftools view -m2 -M2 -Oz -o \"$bcftools_output\" \"$bcftools_input\"\n",
    "    \n",
    "    echo \"Indexing the filtered VCF for chromosome ${chromosome}...\"\n",
    "    # Index the compressed VCF file\n",
    "    bcftools index \"$bcftools_output\"\n",
    "done\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bagg-analysis-_9iskiIm-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
