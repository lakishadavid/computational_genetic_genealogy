{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from collections import Counter\n",
    "import logging\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import subprocess\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from IPython.display import display, HTML\n",
    "import IPython\n",
    "import pandas as pd\n",
    "import boto3\n",
    "\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_comp_gen_dir():\n",
    "    \"\"\"Find the computational_genetic_genealogy directory by searching up from current directory.\"\"\"\n",
    "    current = Path.cwd()\n",
    "    \n",
    "    # Search up through parent directories\n",
    "    while current != current.parent:\n",
    "        # Check if target directory exists in current path\n",
    "        target = current / 'computational_genetic_genealogy'\n",
    "        if target.is_dir():\n",
    "            return target\n",
    "        # Move up one directory\n",
    "        current = current.parent\n",
    "    \n",
    "    raise FileNotFoundError(\"Could not find computational_genetic_genealogy directory\")\n",
    "\n",
    "def load_env_file():\n",
    "    \"\"\"Find and load the .env file from the computational_genetic_genealogy directory.\"\"\"\n",
    "    try:\n",
    "        # Find the computational_genetic_genealogy directory\n",
    "        comp_gen_dir = find_comp_gen_dir()\n",
    "        \n",
    "        # Look for .env file\n",
    "        env_path = comp_gen_dir / '.env'\n",
    "        if not env_path.exists():\n",
    "            print(f\"Warning: No .env file found in {comp_gen_dir}\")\n",
    "            return None\n",
    "        \n",
    "        # Load the .env file\n",
    "        load_dotenv(env_path, override=True)\n",
    "        print(f\"Loaded environment variables from: {env_path}\")\n",
    "        return env_path\n",
    "        \n",
    "    except FileNotFoundError as e:\n",
    "        print(f\"Error: {e}\")\n",
    "        return None\n",
    "\n",
    "# Use the function\n",
    "env_path = load_env_file()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "working_directory = os.getenv('PROJECT_WORKING_DIR', default=None)\n",
    "data_directory = os.getenv('PROJECT_DATA_DIR', default=None)\n",
    "references_directory = os.getenv('PROJECT_REFERENCES_DIR', default=None)\n",
    "results_directory = os.getenv('PROJECT_RESULTS_DIR', default=None)\n",
    "utils_directory = os.getenv('PROJECT_UTILS_DIR', default=None)\n",
    "\n",
    "print(f\"Working Directory: {working_directory}\")\n",
    "print(f\"Data Directory: {data_directory}\")\n",
    "print(f\"References Directory: {references_directory}\")\n",
    "print(f\"Results Directory: {results_directory}\")\n",
    "print(f\"Utils Directory: {utils_directory}\")\n",
    "\n",
    "os.chdir(working_directory)\n",
    "print(f\"The current directory is {os.getcwd()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def configure_logging(log_filename, log_file_debug_level=\"INFO\", console_debug_level=\"INFO\"):\n",
    "    \"\"\"\n",
    "    Configure logging for both file and console handlers.\n",
    "\n",
    "    Args:\n",
    "        log_filename (str): Path to the log file where logs will be written.\n",
    "        log_file_debug_level (str): Logging level for the file handler.\n",
    "        console_debug_level (str): Logging level for the console handler.\n",
    "    \"\"\"\n",
    "    # Create a root logger\n",
    "    logger = logging.getLogger()\n",
    "    logger.setLevel(logging.DEBUG)  # Capture all messages at the root level\n",
    "\n",
    "    # Convert level names to numeric levels\n",
    "    file_level = getattr(logging, log_file_debug_level.upper(), logging.INFO)\n",
    "    console_level = getattr(logging, console_debug_level.upper(), logging.INFO)\n",
    "\n",
    "    # File handler: Logs messages at file_level and above to the file\n",
    "    file_handler = logging.FileHandler(log_filename)\n",
    "    file_handler.setLevel(file_level)\n",
    "    file_formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')\n",
    "    file_handler.setFormatter(file_formatter)\n",
    "\n",
    "    # Console handler: Logs messages at console_level and above to the console\n",
    "    console_handler = logging.StreamHandler(sys.stdout)\n",
    "    console_handler.setLevel(console_level)\n",
    "    console_formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')\n",
    "    console_handler.setFormatter(console_formatter)\n",
    "\n",
    "    # Add handlers to the root logger\n",
    "    logger.addHandler(file_handler)\n",
    "    logger.addHandler(console_handler)\n",
    "    \n",
    "def clear_logger():\n",
    "    \"\"\"Remove all handlers from the root logger.\"\"\"\n",
    "    logger = logging.getLogger()\n",
    "    for handler in logger.handlers[:]:\n",
    "        logger.removeHandler(handler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_filename = os.path.join(results_directory, \"lab5.log\")\n",
    "print(f\"The Lab 5 log file is located at {log_filename}.\")\n",
    "\n",
    "# Ensure the results_directory exists\n",
    "if not os.path.exists(results_directory):\n",
    "    os.makedirs(results_directory)\n",
    "\n",
    "# Check if the file exists; if not, create it\n",
    "if not os.path.exists(log_filename):\n",
    "    with open(log_filename, 'w') as file:\n",
    "        pass  # The file is now created.\n",
    "    \n",
    "clear_logger() # Clear the logger before reconfiguring it\n",
    "configure_logging(log_filename, log_file_debug_level=\"INFO\", console_debug_level=\"INFO\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get The Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸ“¥ How to Download OpenSNP Data\n",
    "\n",
    "There are **two ways** to download OpenSNP data. You only need to select **one** method. **The preferred method is using `boto3`.**\n",
    "\n",
    "---\n",
    "\n",
    "## **âœ… Option 1: Download Using `boto3` (Preferred)**\n",
    "With `boto3` installed, you can use it to efficiently download files from the OpenSNP public S3 bucket. This method is recommended for bulk downloading and better reliability. However, how to setup boto3 will not be covered until later in the semester. For now, use option 2.\n",
    "\n",
    "---\n",
    "\n",
    "## **âœ… Option 2: Download Using `requests` (No AWS Setup Required)**\n",
    "You can use the `requests` library to download the files directly from a public S3 URLs. This method is easier to use but may be slower for large downloads.\n",
    "\n",
    "---\n",
    "\n",
    "## **ðŸ“Œ Manually Downloading Files**\n",
    "If you prefer to manually download individual files:\n",
    "1. Open your web browser and go to:\n",
    "   ```\n",
    "   https://opensnpdata.s3.us-east-2.amazonaws.com/[FILENAME]\n",
    "   ```\n",
    "   Replace `[FILENAME]` with the exact filename from `opensnp_file_list.txt`.\n",
    "\n",
    "2. Example:\n",
    "\n",
    "   https://opensnpdata.s3.us-east-2.amazonaws.com/user1001_file496_yearofbirth_unknown_sex_unknown.ancestry.txt\n",
    "\n",
    "3. **Right-click â†’ Save As...** to download the file.\n",
    "\n",
    "4. Move the file to:\n",
    "   ```\n",
    "   data_directory/class_data/raw_dna_profiles/\n",
    "   ```\n",
    "\n",
    "---\n",
    "\n",
    "ðŸ“Œ **Choose one method that works best for you.** If unsure, use **boto3** for better performance.\n",
    "\n",
    "ðŸš€ Happy downloading! ðŸš€"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Option 1: boto3 version**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define constants\n",
    "BUCKET_NAME = \"opensnpdata\"\n",
    "SAVE_DIR = f\"{data_directory}/class_data/raw_dna_profiles\"\n",
    "\n",
    "# Ensure the save directory exists\n",
    "os.makedirs(SAVE_DIR, exist_ok=True)\n",
    "\n",
    "# Initialize an anonymous S3 client\n",
    "s3 = boto3.client(\"s3\", config=boto3.session.Config(signature_version=\"s3v4\"))\n",
    "\n",
    "def count_files_in_bucket():\n",
    "    \"\"\"Count the total number of files in the OpenSNP bucket.\"\"\"\n",
    "    response = s3.list_objects_v2(Bucket=BUCKET_NAME)\n",
    "    if \"Contents\" in response:\n",
    "        return len(response[\"Contents\"])\n",
    "    return 0\n",
    "\n",
    "num_files = count_files_in_bucket()\n",
    "print(f\"Total files in bucket: {num_files}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_files(limit=None):\n",
    "    \"\"\"\n",
    "    Download a specified number of files (or all files) from the OpenSNP bucket.\n",
    "\n",
    "    Parameters:\n",
    "        limit (int or None): Number of files to download. If None, downloads all files.\n",
    "    \"\"\"\n",
    "    response = s3.list_objects_v2(Bucket=BUCKET_NAME)\n",
    "    \n",
    "    if \"Contents\" not in response:\n",
    "        print(\"No files found in the OpenSNP bucket.\")\n",
    "        return\n",
    "    \n",
    "    files = response[\"Contents\"]\n",
    "    \n",
    "    # Apply limit if specified\n",
    "    if limit is not None:\n",
    "        files = files[:limit]\n",
    "\n",
    "    print(f\"Downloading {len(files)} files...\")\n",
    "\n",
    "    for obj in files:\n",
    "        file_key = obj[\"Key\"]\n",
    "        local_path = os.path.join(SAVE_DIR, os.path.basename(file_key))\n",
    "\n",
    "        print(f\"Downloading: {file_key} -> {local_path}\")\n",
    "        s3.download_file(BUCKET_NAME, file_key, local_path)\n",
    "\n",
    "    print(\"Download completed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download a certain number of files (e.g., first 5)\n",
    "download_files(limit=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download all files\n",
    "download_files(limit=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Option 2: non boto3 version**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from tqdm.notebook import tqdm  # Changed to notebook version of tqdm\n",
    "from tqdm.auto import tqdm as tqdm_auto  # For auto-detection of environment\n",
    "import signal\n",
    "import sys\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Optional\n",
    "import logging\n",
    "\n",
    "@dataclass\n",
    "class DownloadConfig:\n",
    "    bucket_url: str\n",
    "    save_dir: str\n",
    "    file_list_path: str\n",
    "    max_workers: int = 5\n",
    "    chunk_size: int = 8192\n",
    "\n",
    "class ParallelDownloader:\n",
    "    def __init__(self, config: DownloadConfig):\n",
    "        self.config = config\n",
    "        self.interrupted = False\n",
    "        self.failed_downloads = []\n",
    "        self.setup_logging()\n",
    "        self.setup_signal_handlers()\n",
    "        \n",
    "    def setup_logging(self):\n",
    "        logging.basicConfig(\n",
    "            level=logging.INFO,\n",
    "            format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "            handlers=[\n",
    "                logging.FileHandler('download_log.txt'),\n",
    "                logging.StreamHandler(sys.stdout)\n",
    "            ]\n",
    "        )\n",
    "        self.logger = logging.getLogger(__name__)\n",
    "\n",
    "    def setup_signal_handlers(self):\n",
    "        signal.signal(signal.SIGINT, self.handle_interrupt)\n",
    "        signal.signal(signal.SIGTERM, self.handle_interrupt)\n",
    "\n",
    "    def handle_interrupt(self, signum, frame):\n",
    "        self.logger.warning(\"Received interrupt signal. Finishing current downloads...\")\n",
    "        self.interrupted = True\n",
    "\n",
    "    def get_file_list(self, start: Optional[int] = None, end: Optional[int] = None) -> List[str]:\n",
    "        \"\"\"Read and optionally slice the file list.\"\"\"\n",
    "        try:\n",
    "            with open(self.config.file_list_path, \"r\") as f:\n",
    "                file_list = [line.strip() for line in f.readlines()]\n",
    "            return file_list[start:end] if start is not None else file_list\n",
    "        except FileNotFoundError:\n",
    "            self.logger.error(f\"File list not found: {self.config.file_list_path}\")\n",
    "            raise\n",
    "\n",
    "    def download_file(self, filename: str, overall_pbar) -> bool:\n",
    "        \"\"\"Download a single file with progress tracking.\"\"\"\n",
    "        if self.interrupted:\n",
    "            return False\n",
    "\n",
    "        file_url = f\"{self.config.bucket_url}/{filename}\"\n",
    "        local_path = os.path.join(self.config.save_dir, os.path.basename(filename))\n",
    "\n",
    "        # Skip if file exists and has content\n",
    "        if os.path.exists(local_path) and os.path.getsize(local_path) > 0:\n",
    "            self.logger.info(f\"Skipping existing file: {filename}\")\n",
    "            overall_pbar.update(1)\n",
    "            return True\n",
    "\n",
    "        try:\n",
    "            response = requests.get(file_url, stream=True)\n",
    "            response.raise_for_status()\n",
    "\n",
    "            total_size = int(response.headers.get('content-length', 0))\n",
    "            \n",
    "            with open(local_path, \"wb\") as file:\n",
    "                if total_size == 0:\n",
    "                    file.write(response.content)\n",
    "                else:\n",
    "                    downloaded = 0\n",
    "                    for chunk in response.iter_content(chunk_size=self.config.chunk_size):\n",
    "                        if self.interrupted:\n",
    "                            return False\n",
    "                        if chunk:\n",
    "                            file.write(chunk)\n",
    "                            downloaded += len(chunk)\n",
    "                            \n",
    "            overall_pbar.update(1)\n",
    "            return True\n",
    "\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Failed to download {filename}: {str(e)}\")\n",
    "            self.failed_downloads.append((filename, str(e)))\n",
    "            if os.path.exists(local_path):\n",
    "                os.remove(local_path)\n",
    "            overall_pbar.update(1)\n",
    "            return False\n",
    "\n",
    "    def download_files(self, start: Optional[int] = None, \n",
    "                      end: Optional[int] = None, \n",
    "                      max_retries: int = 3) -> None:\n",
    "        \"\"\"\n",
    "        Download files in parallel with retry mechanism.\n",
    "        \n",
    "        Args:\n",
    "            start: Optional starting index for file range\n",
    "            end: Optional ending index for file range\n",
    "            max_retries: Maximum number of retry attempts for failed downloads\n",
    "        \"\"\"\n",
    "        os.makedirs(self.config.save_dir, exist_ok=True)\n",
    "        \n",
    "        file_list = self.get_file_list(start, end)\n",
    "        total_files = len(file_list)\n",
    "        \n",
    "        self.logger.info(f\"Starting download of {total_files} files...\")\n",
    "        \n",
    "        for attempt in range(max_retries + 1):\n",
    "            if not file_list:\n",
    "                break\n",
    "                \n",
    "            with ThreadPoolExecutor(max_workers=self.config.max_workers) as executor:\n",
    "                # Create the progress bar\n",
    "                with tqdm_auto(total=len(file_list), \n",
    "                             desc=\"Downloading files\", \n",
    "                             unit=\"file\") as pbar:\n",
    "                    \n",
    "                    # Submit all downloads\n",
    "                    future_to_file = {\n",
    "                        executor.submit(self.download_file, filename, pbar): filename\n",
    "                        for filename in file_list\n",
    "                    }\n",
    "                    \n",
    "                    successful_downloads = []\n",
    "                    \n",
    "                    # Process completed downloads\n",
    "                    for future in as_completed(future_to_file):\n",
    "                        filename = future_to_file[future]\n",
    "                        try:\n",
    "                            if future.result():\n",
    "                                successful_downloads.append(filename)\n",
    "                        except Exception as e:\n",
    "                            self.logger.error(f\"Error downloading {filename}: {str(e)}\")\n",
    "                        \n",
    "                        if self.interrupted:\n",
    "                            self.logger.warning(\"Download interrupted by user.\")\n",
    "                            return\n",
    "\n",
    "                # Remove successful downloads from the list\n",
    "                file_list = [f for f in file_list if f not in successful_downloads]\n",
    "                \n",
    "                if file_list and attempt < max_retries:\n",
    "                    self.logger.info(f\"Retrying {len(file_list)} failed downloads... \"\n",
    "                                   f\"(Attempt {attempt + 2}/{max_retries + 1})\")\n",
    "        \n",
    "        if self.failed_downloads:\n",
    "            self.logger.error(\"Failed downloads:\")\n",
    "            for filename, error in self.failed_downloads:\n",
    "                self.logger.error(f\"  {filename}: {error}\")\n",
    "            \n",
    "        self.logger.info(f\"Download completed. \"\n",
    "                        f\"Successfully downloaded: {total_files - len(self.failed_downloads)}/{total_files}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create config in one cell\n",
    "config = DownloadConfig(\n",
    "    bucket_url=\"https://opensnpdata.s3.us-east-2.amazonaws.com\",\n",
    "    save_dir=f\"{data_directory}/class_data/raw_dna_profiles\",\n",
    "    file_list_path=f\"{data_directory}/class_data/opensnp_file_list.txt\",\n",
    "    max_workers=5\n",
    ")\n",
    "\n",
    "# Create downloader instance\n",
    "downloader = ParallelDownloader(config)\n",
    "\n",
    "\n",
    "# You can run this cell multiple times if needed\n",
    "downloader.download_files(start=5, end=15)  # Start with just 10 files as a test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download all files\n",
    "downloader.download_files()\n",
    "\n",
    "# There are 117 files. It took mine 3 minutes and 33 seconds to download."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Genetic Map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use your existing Beagle genetic maps to create the genetic maps for IBIS. If you already have these genetic maps, you do not need to rerun these cells to download the genetic maps again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_ibis_map():\n",
    "    beagle_map_dir = os.path.join(references_directory, \"genetic_maps/beagle_genetic_maps\")\n",
    "    ibis_map_dir = os.path.join(references_directory, \"genetic_maps/ibis_genetic_maps\")\n",
    "    os.makedirs(ibis_map_dir, exist_ok=True)\n",
    "    \n",
    "    for map_file in os.listdir(beagle_map_dir):\n",
    "        if map_file.endswith(\".map\"):\n",
    "            beagle_map_filename = os.path.join(beagle_map_dir, map_file)\n",
    "            ibis_map_filename = os.path.join(ibis_map_dir, map_file)\n",
    "            print(f\"Processing {beagle_map_filename} to create IBIS map...\")\n",
    "            \n",
    "            # For IBIS maps, we need: CHR POSITION GENETIC_POSITION [RATE]\n",
    "            # From Beagle maps which are: CHR . GENETIC_POSITION PHYSICAL_POSITION\n",
    "            # Move the genetic position to column 3 (not 4)\n",
    "            command = f\"awk '{{print $1, $4, $3, 0}}' {beagle_map_filename} > {ibis_map_filename}\"\n",
    "            \n",
    "            subprocess.run(command, shell=True, check=True)\n",
    "    print(\"All Beagle genetic maps converted to IBIS format.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up output directories\n",
    "genetic_maps_dir = os.path.join(references_directory, \"genetic_maps\")\n",
    "os.makedirs(genetic_maps_dir, exist_ok=True)\n",
    "\n",
    "ibis_genetic_maps = os.path.join(genetic_maps_dir, \"ibis_genetic_maps\")\n",
    "os.makedirs(ibis_genetic_maps, exist_ok=True)\n",
    "\n",
    "assembly = \"GRCh38\"\n",
    "preprocess_ibis_map()\n",
    "\n",
    "# # Alternative source\n",
    "# plink2_genetic_map_url=\"https://alkesgroup.broadinstitute.org/Eagle/downloads/tables/genetic_map_hg38_withX.txt.gz\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Visual Inspection**\n",
    "\n",
    "The above code should have created a set of genetic map files in the format that IBIS use. Look in your `references/genetic_mpas` directory and check for the `ibis_genetic_maps` subdirectory and individual by chromosome files within `ibis_genetic_maps`. Open the chromosome 1 file of both `ibis_genetic_maps` and `beagle_genetic_maps`. Compare them, visually. How are they similar? How are they different?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ðŸ›‘ **STOP**\n",
    "\n",
    "If you're using the newly downloaded data, you need to run Lab3 and Lab4 to process that data first. \n",
    "\n",
    "The following cells starts with the files that are in your `results/phased_samples` directory (which is where Lab4 ends). \n",
    "\n",
    "For now, as a demonstraction of the rest of this code, you can manually move the files out of `class_data/phased_samples` and place them in `results/phased_samples`. These files are from my run of all the files through Labs 3 and 4. Make sure you delete these from `results/phased_samples` and process the files you downloaded later to complete the lab."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Concatenate phased VCF files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash -s \"$data_directory\" \"$utils_directory\" \"$results_directory\"\n",
    "\n",
    "data_directory=\"$1\"\n",
    "utils_directory=\"$2\"\n",
    "results_directory=\"$3\"\n",
    "\n",
    "# Define the directory containing phased VCF files\n",
    "phased_samples_dir=\"${results_directory}/phased_samples\"\n",
    "\n",
    "# Concatenate phased VCF files\n",
    "echo \"Creating list of phased VCF files...\"\n",
    "PHASED_FILE_LIST=\"${phased_samples_dir}/phased_file_list_sample.txt\"\n",
    "\n",
    "# Empty the file list if it already exists\n",
    "> \"$PHASED_FILE_LIST\"\n",
    "\n",
    "for CHR in {1..22}; do\n",
    "    PHASED_VCF=\"${phased_samples_dir}/merged_opensnps_phased_chr${CHR}.vcf.gz\"\n",
    "    if [ -f \"$PHASED_VCF\" ]; then\n",
    "        echo \"$PHASED_VCF\" >> \"$PHASED_FILE_LIST\"\n",
    "    else\n",
    "        echo \"Phased VCF missing for chromosome $CHR\"\n",
    "    fi\n",
    "done\n",
    "\n",
    "CONCATENATED_VCF=\"${phased_samples_dir}/merged_opensnps_autosomes.vcf\"\n",
    "SORTED_VCF=\"${phased_samples_dir}/merged_opensnps_autosomes_sorted.vcf.gz\"\n",
    "STATS_OUTPUT=\"${phased_samples_dir}/merged_opensnps_autosomes_sorted_stats.vchk\"\n",
    "\n",
    "# Concatenate VCF files\n",
    "bcftools concat -o \"$CONCATENATED_VCF\" --file-list \"$PHASED_FILE_LIST\"\n",
    "\n",
    "if [ -f \"$CONCATENATED_VCF\" ]; then\n",
    "    # Sort and compress the concatenated VCF\n",
    "    bcftools sort -Oz -o \"$SORTED_VCF\" \"$CONCATENATED_VCF\"\n",
    "\n",
    "    # Index the sorted VCF\n",
    "    bcftools index --tbi -f \"$SORTED_VCF\"\n",
    "\n",
    "    # Generate stats\n",
    "    bcftools stats -s - \"$SORTED_VCF\" > \"$STATS_OUTPUT\"\n",
    "\n",
    "    rm -f \"${results_directory}/merged_opensnps_autosomes_step1*\"\n",
    "    rm -f \"${results_directory}/merged_opensnps_autosomes_step2*\"\n",
    "\n",
    "    echo \"Phasing, cleanup, and concatenation completed successfully.\"\n",
    "\n",
    "    # Remove individual phased VCF files\n",
    "    echo \"Removing individual phased VCF files...\"\n",
    "    for CHR in {1..22}; do\n",
    "        PHASED_VCF=\"${phased_samples_dir}/merged_opensnps_phased_chr${CHR}.vcf.gz\"\n",
    "        if [ -f \"${PHASED_VCF}\" ]; then\n",
    "            rm -f \"${PHASED_VCF}\"\n",
    "            rm -f \"${PHASED_VCF}.tbi\"\n",
    "            rm -f \"${phased_samples_dir}/merged_opensnps_phased_chr${CHR}.log\"\n",
    "            rm -f \"${phased_samples_dir}/merged_opensnps_phased_chr${CHR}_stats.vchk\"\n",
    "            echo \"Removed $PHASED_VCF and its index.\"\n",
    "        fi\n",
    "    done\n",
    "    rm -f \"${phased_samples_dir}/merged_opensnps_autosomes.vcf\"\n",
    "else\n",
    "    echo \"Concatenated VCF file missing. Pipeline aborted.\"\n",
    "    exit 1\n",
    "fi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Change the format of the data files from VCF to BED."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash -s \"$data_directory\" \"$utils_directory\" \"$results_directory\"\n",
    "\n",
    "data_directory=\"$1\"\n",
    "utils_directory=\"$2\"\n",
    "results_directory=\"$3\"\n",
    "\n",
    "# Define\n",
    "phased_samples_dir=\"${results_directory}/phased_samples\"\n",
    "vcf_file=\"${phased_samples_dir}/merged_opensnps_autosomes_sorted.vcf.gz\"\n",
    "\n",
    "# Ensure the PLINK2 executable exists\n",
    "if [[ ! -f \"${utils_directory}/plink2\" ]]; then\n",
    "    echo \"Error: PLINK2 executable not found: ${utils_directory}/plink2\" >&2\n",
    "    exit 1\n",
    "fi\n",
    "\n",
    "# Ensure the phased samples directory exists\n",
    "if [[ ! -d \"${phased_samples_dir}\" ]]; then\n",
    "    echo \"Error: Phased samples directory not found: ${phased_samples_dir}\" >&2\n",
    "    exit 1\n",
    "fi\n",
    "\n",
    "# Check if the file exists\n",
    "if [[ ! -f \"$vcf_file\" ]]; then\n",
    "    echo \"No matching VCF file found in $phased_samples_dir\" >&2\n",
    "    exit 1\n",
    "fi\n",
    "\n",
    "# Extract the file prefix (removing .vcf.gz extension)\n",
    "output_prefix=\"${vcf_file%.vcf.gz}\"\n",
    "\n",
    "# Convert the VCF file to PLINK format\n",
    "${utils_directory}/plink2 --vcf \"$vcf_file\" --autosome --make-bed --out \"$output_prefix\"\n",
    "\n",
    "# Check exit status\n",
    "if [[ $? -eq 0 ]]; then\n",
    "    echo \"PLINK2 successfully processed: ${vcf_file}\"\n",
    "else\n",
    "    echo \"Error processing ${vcf_file}\" >&2\n",
    "fi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explanation:\n",
    "- `data_directory`, `utils_directory`, and `results_directory` are passed as arguments and assigned.\n",
    "- The script verifies that plink2 exists and that the phased_samples_dir is a valid directory.\n",
    "- It loops over files matching opensnps_phased_*.vcf.gz, checking if they exist before processing.\n",
    "- Uses PLINK2 to convert each .vcf.gz file to PLINK binary format (.bed, .bim, .fam).\n",
    "- Handles errors and prints appropriate messages."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add Genetic Map to Bim File\n",
    "(as per the IBIS developer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!head -10 /home/lakishadavid/computational_genetic_genealogy/results/phased_samples/merged_opensnps_autosomes_sorted.bim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash -s \"$results_directory\" \"$references_directory\" \"$utils_directory\"\n",
    "\n",
    "results_directory=\"$1\"\n",
    "references_directory=\"$2\"\n",
    "utils_directory=\"$3\"\n",
    "\n",
    "python scripts_support/add_genetic_map.py \\\n",
    "  /home/lakishadavid/computational_genetic_genealogy/results/phased_samples/merged_opensnps_autosomes_sorted.bim \\\n",
    "  /home/lakishadavid/computational_genetic_genealogy/references/genetic_maps/ibis_genetic_maps \\\n",
    "  /home/lakishadavid/computational_genetic_genealogy/results/phased_samples/merged_opensnps_autosomes_sorted.bim.gm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!head -10 /home/lakishadavid/computational_genetic_genealogy/results/phased_samples/merged_opensnps_autosomes_sorted.bim.gm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explanation:\n",
    "- The script assigns arguments to data_directory, references_directory, and utils_directory.\n",
    "- It verifies the existence of the add-map-plink.pl script.\n",
    "- It checks for .bim files in data_directory, ensuring at least one exists.\n",
    "- Extracts the chromosome number from the .bim filename.\n",
    "- Determines the corresponding genetic map file.\n",
    "- If the necessary files exist, it runs the Perl script to append the genetic map.\n",
    "- The new .bim file is saved with a _gm.bim suffix.\n",
    "- Errors are handled with messages and exit codes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the IBD Detection Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash -s \"$data_directory\" \"$results_directory\" \"$utils_directory\"\n",
    "\n",
    "data_directory=\"$1\"\n",
    "results_directory=\"$2\"\n",
    "utils_directory=\"$3\"\n",
    "\n",
    "# Define the IBIS executable path\n",
    "ibis=\"${utils_directory}/ibis/ibis\"\n",
    "\n",
    "# Ensure the IBIS executable exists\n",
    "if [[ ! -f \"${ibis}\" ]]; then\n",
    "    echo \"Error: IBIS executable not found: ${ibis}\" >&2\n",
    "fi\n",
    "\n",
    "bed_file=\"${results_directory}/phased_samples/merged_opensnps_autosomes_sorted.bed\"\n",
    "bim_file=\"${results_directory}/phased_samples/merged_opensnps_autosomes_sorted.bim.gm\"\n",
    "fam_file=\"${results_directory}/phased_samples/merged_opensnps_autosomes_sorted.fam\"\n",
    "${ibis} ${bed_file} ${bim_file} ${fam_file} -ibd2 -min_l 7 -mt 500 -er .004 \\\n",
    "    -o \"${results_directory}/merged_opensnps_autosomes_ibis\" \\\n",
    "    -printCoef -noFamID\n",
    "\n",
    "# Check exit status\n",
    "if [[ $? -eq 0 ]]; then\n",
    "    echo \"IBIS analysis completed successfully.\"\n",
    "else\n",
    "    echo \"Error running IBIS analysis.\" >&2\n",
    "fi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IBIS Output File Descriptions\n",
    "\n",
    "## IBIS\n",
    "This file contains detailed information about identity-by-descent (IBD) segments shared between pairs of individuals.\n",
    "\n",
    "### Columns:\n",
    "- **sample1, sample2**: IDs of the two individuals being compared for shared genetic segments.\n",
    "- **chrom**: Chromosome number where the IBD segment is located.\n",
    "- **phys_start_pos, phys_end_pos**: Start and end positions of the IBD segment in base pairs (physical positions).\n",
    "- **IBD_type**: Type of IBD segment (e.g., IBD1 for sharing one parental haplotype or IBD2 for sharing both parental haplotypes).\n",
    "- **genetic_start_pos, genetic_end_pos**: Start and end positions of the segment in genetic map units (centiMorgans).\n",
    "- **genetic_seg_length**: Length of the IBD segment in centiMorgans (genetic distance).\n",
    "- **marker_count**: Number of genetic markers (SNPs) within the segment.\n",
    "- **error_count**: Total number of mismatches or genotyping errors detected in the segment.\n",
    "- **error_density**: Average error rate per marker in the segment (error_count divided by marker_count).\n",
    "\n",
    "---\n",
    "\n",
    "## Coef\n",
    "This file provides information about pairwise kinship coefficients and degrees of relatedness.\n",
    "\n",
    "### Columns:\n",
    "- **sample1, sample2**: IDs of the two individuals being compared.\n",
    "- **kinship_coefficient**: A measure of genetic similarity between the individuals, ranging from 0 (no relation) to higher values for close relatives.\n",
    "- **IBD2_fraction**: Proportion of the genome where both parental haplotypes are shared between the individuals.\n",
    "- **segment_count**: Total number of IBD segments identified between the individuals.\n",
    "- **degree_of_relatedness**: Classification of the relationship based on kinship (e.g., siblings, cousins).\n",
    "\n",
    "---\n",
    "\n",
    "## IBD2\n",
    "Represents segments where two individuals share both parental haplotypes.  \n",
    "IBD2 is particularly useful in identifying siblings or individuals with close familial ties, as these segments indicate inheritance from both sides of the family.\n",
    "\n",
    "---\n",
    "\n",
    "## HBD (Runs of Homozygosity)\n",
    "Indicates segments where an individual has matching haplotypes on both chromosomes, likely due to inheritance from a common ancestor.  \n",
    "This is a measure of inbreeding or autozygosity (when an individual inherits identical haplotypes from both parents).\n",
    "\n",
    "### Columns:\n",
    "- **sample_id**: ID of the individual being analyzed for HBD segments.\n",
    "- **chrom**: Chromosome number where the HBD segment is located.\n",
    "- **phys_start_pos, phys_end_pos**: Start and end positions of the HBD segment in base pairs.\n",
    "- **HBD_type**: Type or classification of the HBD segment.\n",
    "- **genetic_start_pos, genetic_end_pos**: Start and end positions of the segment in genetic map units (centiMorgans).\n",
    "- **genetic_seg_length**: Length of the HBD segment in centiMorgans.\n",
    "- **marker_count**: Number of genetic markers (SNPs) in the segment.\n",
    "- **error_count**: Total number of mismatches or genotyping errors detected in the segment.\n",
    "- **error_density**: Average error rate per marker in the segment.\n",
    "\n",
    "---\n",
    "\n",
    "## Incoef\n",
    "Provides inbreeding coefficients for individuals, based on HBD analysis.\n",
    "\n",
    "### Columns:\n",
    "- **sample_id**: ID of the individual being analyzed.\n",
    "- **inbreeding_coefficient**: A measure of inbreeding for the individual, reflecting the proportion of the genome covered by HBD segments.\n",
    "- **segment_count**: Total number of HBD segments identified in the individual's genome."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explore The Coefficients Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def explore_coefficients(\n",
    "    results_directory, \n",
    "    filename=\"merged_opensnps_autosomes_ibis.coef\", \n",
    "    focus_on_related=True, \n",
    "    save_plots=True, \n",
    "    show_plots=True,\n",
    "    output_subdir=\"segments\"\n",
    "    ):\n",
    "    \"\"\"\n",
    "    Reads and explores the coefficients file from the results directory.\n",
    "    Includes handling for missing values and options to focus on related individuals.\n",
    "    \n",
    "    Parameters:\n",
    "        results_directory (str): Directory containing the result files.\n",
    "        filename (str): Filename of the coefficients file.\n",
    "        focus_on_related (bool): If True, focuses analysis on related individuals (Degree > 0).\n",
    "        save_plots (bool): If True, saves plots to the specified output directory.\n",
    "        output_dir (str): Directory to save plots.\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: Processed coefficients DataFrame for further analysis.\n",
    "    \"\"\"\n",
    "        \n",
    "    # Ensure output directory exists\n",
    "    output_dir = os.path.join(results_directory, output_subdir)\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    # Step 1: Read the coefficients file\n",
    "    file_path = os.path.join(results_directory, filename)\n",
    "    coefficients = pd.read_csv(file_path, sep=\"\\t\", low_memory=False)\n",
    "\n",
    "    # Save both full and filtered data if focus_on_related is True\n",
    "    full_data = coefficients.copy()\n",
    "    filtered_data = None\n",
    "\n",
    "    if focus_on_related:\n",
    "        print(\"\\nFocusing on related individuals (Degree > 0).\")\n",
    "        filtered_data = full_data[full_data['Degree'] > 0]\n",
    "        print(f\"Filtered DataFrame Info (Degree > 0):\")\n",
    "        filtered_data.info()\n",
    "        print(\"\\n=== Descriptive Statistics (Filtered) ===\")\n",
    "        print(filtered_data.describe())\n",
    "        print(\"\\n\")\n",
    "        filtered_file_path = os.path.join(output_dir, \"filtered_coefficients.csv\")\n",
    "        filtered_data.to_csv(filtered_file_path, index=False)\n",
    "        print(f\"Filtered coefficients saved to: {filtered_file_path}\")\n",
    "\n",
    "    # Save and print the full data\n",
    "    print(\"\\nFull DataFrame Info:\")\n",
    "    full_data.info()\n",
    "    print(\"\\n=== Descriptive Statistics (Full) ===\")\n",
    "    print(full_data.describe())\n",
    "    print(\"\\n\")\n",
    "    full_file_path = os.path.join(output_dir, \"full_coefficients.csv\")\n",
    "    full_data.to_csv(full_file_path, index=False)\n",
    "    print(f\"Full coefficients saved to: {full_file_path}\")\n",
    "\n",
    "    # Analyze both datasets\n",
    "    datasets = {\"Full\": full_data, \"Filtered\": filtered_data} if focus_on_related else {\"Full\": full_data}\n",
    "\n",
    "    for name, data in datasets.items():\n",
    "        if data is not None:\n",
    "            print(f\"\\n=== Analyzing {name} Data ===\")\n",
    "            \n",
    "            # Counts by Degree\n",
    "            degree_grouped_counts = data['Degree'].value_counts().sort_index()\n",
    "            degree_grouped_counts_df = degree_grouped_counts.reset_index(name='Count')\n",
    "            degree_grouped_counts_df.columns = ['Degree', 'Count']\n",
    "            print(f\"=== Counts by Degree ({name}) ===\")\n",
    "            print(degree_grouped_counts_df)\n",
    "            \n",
    "            # Save HTML table\n",
    "            html_table = degree_grouped_counts_df.to_html(index=False)\n",
    "            html_file_path = os.path.join(output_dir, f\"{name.lower()}_degree_counts.html\")\n",
    "            with open(html_file_path, \"w\") as f:\n",
    "                f.write(html_table)\n",
    "            print(f\"HTML table for {name} data saved to: {html_file_path}\")\n",
    "\n",
    "            # Display in Jupyter if available\n",
    "            if hasattr(IPython, 'get_ipython') and IPython.get_ipython() is not None:\n",
    "                display(HTML(html_table))\n",
    "\n",
    "            # Visualizations\n",
    "            def save_or_show_plot(fig, filename):\n",
    "                if save_plots:\n",
    "                    fig.savefig(os.path.join(output_dir, f\"{name.lower()}_{filename}\"))\n",
    "                if show_plots:\n",
    "                    plt.show()\n",
    "                plt.close(fig)\n",
    "\n",
    "            # Degree distribution\n",
    "            fig, ax = plt.subplots(figsize=(8, 5))\n",
    "            sns.histplot(data['Degree'], bins=10, kde=False, ax=ax)\n",
    "            ax.set_title(f'Degree Distribution ({name})')\n",
    "            ax.set_xlabel('Degree')\n",
    "            ax.set_ylabel('Frequency')\n",
    "            save_or_show_plot(fig, \"degree_distribution.png\")\n",
    "\n",
    "            # Other plots\n",
    "            if 'Kinship_Coefficient' in data.columns:\n",
    "                fig, ax = plt.subplots(figsize=(8, 5))\n",
    "                sns.histplot(data['Kinship_Coefficient'], bins=30, kde=True, ax=ax)\n",
    "                ax.set_title(f'Kinship Coefficient Distribution ({name})')\n",
    "                ax.set_xlabel('Kinship Coefficient')\n",
    "                ax.set_ylabel('Frequency')\n",
    "                save_or_show_plot(fig, \"kinship_coefficient_distribution.png\")\n",
    "\n",
    "            if 'IBD2_Fraction' in data.columns:\n",
    "                fig, ax = plt.subplots(figsize=(8, 5))\n",
    "                sns.histplot(data['IBD2_Fraction'], bins=30, kde=True, ax=ax)\n",
    "                ax.set_title(f'IBD2 Fraction Distribution ({name})')\n",
    "                ax.set_xlabel('IBD2 Fraction')\n",
    "                ax.set_ylabel('Frequency')\n",
    "                save_or_show_plot(fig, \"ibd2_fraction_distribution.png\")\n",
    "\n",
    "            if all(col in data.columns for col in ['Kinship_Coefficient', 'IBD2_Fraction']):\n",
    "                fig, ax = plt.subplots(figsize=(8, 5))\n",
    "                sns.scatterplot(\n",
    "                    data=data,\n",
    "                    x='Kinship_Coefficient',\n",
    "                    y='IBD2_Fraction',\n",
    "                    hue='Degree', palette='viridis', ax=ax\n",
    "                )\n",
    "                ax.set_title(f'Kinship vs. IBD2 Fraction ({name})')\n",
    "                ax.set_xlabel('Kinship Coefficient')\n",
    "                ax.set_ylabel('IBD2 Fraction')\n",
    "                plt.legend(title='Degree')\n",
    "                save_or_show_plot(fig, \"kinship_vs_ibd2_fraction.png\")\n",
    "\n",
    "            # Correlation matrix\n",
    "            numeric_cols = ['Kinship_Coefficient', 'IBD2_Fraction', 'Segment_Count']\n",
    "            existing_cols = [col for col in numeric_cols if col in data.columns]\n",
    "            if existing_cols:\n",
    "                fig, ax = plt.subplots(figsize=(6, 5))\n",
    "                corr = data[existing_cols].corr()\n",
    "                sns.heatmap(corr, annot=True, cmap='Blues', square=True, ax=ax)\n",
    "                ax.set_title(f'Correlation Matrix ({name})')\n",
    "                save_or_show_plot(fig, \"correlation_matrix.png\")\n",
    "\n",
    "    print(\"\\nAnalysis completed.\")\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explore_coefficients(\n",
    "    results_directory, \n",
    "    filename=\"merged_opensnps_autosomes_ibis.coef\", \n",
    "    focus_on_related=True, \n",
    "    save_plots=True,\n",
    "    show_plots=True,\n",
    "    output_subdir=\"segments\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### New Results\n",
    "\n",
    "Look in your `results/segments` directory. You should see several new files. The image files are the ones that with the `.png` extension. Look at the 5 image files that start wtih `filtered_`. What do you think these mean?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explore The Segments Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seg_file = os.path.join(results_directory, \"merged_opensnps_autosomes_ibis.seg\")\n",
    "\n",
    "seg_data_temp = pd.read_csv(seg_file, sep=\"\\t\", header=None)\n",
    "seg_data_temp.columns = [\n",
    "    \"sample1\", \"sample2\", \"chrom\", \n",
    "    \"phys_start_pos\", \"phys_end_pos\", \n",
    "    \"IBD_type\", \"genetic_start_pos\", \n",
    "    \"genetic_end_pos\", \"genetic_seg_length\", \n",
    "    \"marker_count\", \"error_count\", \"error_density\"\n",
    "    ]\n",
    "seg_data = seg_data_temp.sort_values(\n",
    "    by=[\"chrom\", \"phys_start_pos\", \"phys_end_pos\", \"IBD_type\"],\n",
    "    ascending=[True, True, True, True]\n",
    ")\n",
    "\n",
    "output_file = os.path.join(results_directory, \"merged_opensnps_autosomes_ibis.csv\")\n",
    "seg_data.to_csv(output_file, sep=\"\\t\", index=False, header=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice the new file extension of `.csv`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create some data visualizations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def explore_segments_ibis(\n",
    "        results_directory, \n",
    "        filename=\"merged_opensnps_autosomes_ibis.seg\",\n",
    "        min_length=7, \n",
    "        min_markers=436, \n",
    "        max_error_density=0.004,\n",
    "        save_plots=True,\n",
    "        show_plots=True, \n",
    "        output_subdir=\"segments\"\n",
    "):\n",
    "    \"\"\"\n",
    "    Explores and optionally filters the segments DataFrame.\n",
    "    \n",
    "    Parameters:\n",
    "        results_directory (str): Directory containing the segments file.\n",
    "        filename (str): Filename of the segments file.\n",
    "        min_length (float): Minimum genetic length threshold for filtering.\n",
    "        min_markers (int): Minimum marker count threshold for filtering.\n",
    "        max_error_density (float): Maximum error density threshold for filtering.\n",
    "        filter_segments_enabled (bool): If True, apply filtering to the segments.\n",
    "        save_plots (bool): If True, save plots to the specified directory.\n",
    "        output_dir (str): Directory to save outputs and plots.\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: The segments DataFrame (filtered or unfiltered based on input).\n",
    "    \"\"\"\n",
    "    # Ensure output directory exists\n",
    "    output_dir = os.path.join(results_directory, output_subdir)\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    # Step 1: Read the segments file\n",
    "    file_path = os.path.join(results_directory, filename)\n",
    "    segments = pd.read_csv(file_path, sep=\"\\t\", header=None)\n",
    "    segments.columns = [\n",
    "        \"id1\", \"id2\", \"chromosome\", \"physical_position_start\", \n",
    "        \"physical_position_end\", \"IBD_type\", \"genetic_position_start\", \n",
    "        \"genetic_position_end\", \"genetic_length\", \"marker_count\", \n",
    "        \"error_count\", \"error_density\"\n",
    "    ]\n",
    "\n",
    "    # Ensure numeric columns are properly parsed\n",
    "    numeric_columns = [\"genetic_length\", \"marker_count\", \"error_density\", \"chromosome\"]\n",
    "    for col in numeric_columns:\n",
    "        if col in segments.columns:\n",
    "            segments[col] = pd.to_numeric(segments[col], errors='coerce')\n",
    "\n",
    "    # Drop rows with NaN values in numeric columns\n",
    "    nan_rows = segments[segments[numeric_columns].isnull().any(axis=1)]\n",
    "    if not nan_rows.empty:\n",
    "        nan_file_path = os.path.join(output_dir, \"nan_segments_ibis.csv\")\n",
    "        nan_rows.to_csv(nan_file_path, sep=\"\\t\", index=False)\n",
    "        print(f\"Rows with NaN values saved to: {nan_file_path}\")\n",
    "    segments = segments.dropna(subset=numeric_columns).reset_index(drop=True)\n",
    "\n",
    "    # Step 2: Basic info and descriptive statistics\n",
    "    print(\"=== Segments DataFrame Info ===\")\n",
    "    segments.info()\n",
    "    print(\"\\n=== Descriptive Statistics ===\")\n",
    "    print(segments[['genetic_length', 'marker_count', 'error_density']].describe())\n",
    "    print(\"\\n\")\n",
    "\n",
    "    # Save the unfiltered data\n",
    "    unfiltered_file_path = os.path.join(output_dir, \"unfiltered_segments_ibis.csv\")\n",
    "    segments.to_csv(unfiltered_file_path, sep=\"\\t\", index=False)\n",
    "    print(f\"Unfiltered segments saved to: {unfiltered_file_path}\")\n",
    "    print()\n",
    "\n",
    "    filtered_segments = segments[\n",
    "        (segments['genetic_length'] >= min_length) &\n",
    "        (segments['marker_count'] >= min_markers) &\n",
    "        (segments['error_density'] <= max_error_density)\n",
    "    ].copy()\n",
    "    \n",
    "    print(\"=== Filtered Segments Info ===\")\n",
    "    filtered_segments.info()\n",
    "    print(\"\\n=== Descriptive Statistics (Filtered) ===\")\n",
    "    print(filtered_segments[['genetic_length', 'marker_count', 'error_density']].describe())\n",
    "    print(\"\\n\")\n",
    "    \n",
    "    # Save filtered segments to a new file\n",
    "    filtered_filename = \"filtered_segments_ibis.csv\"\n",
    "    filtered_file_path = os.path.join(output_dir, filtered_filename)\n",
    "    filtered_segments.to_csv(filtered_file_path, sep=\"\\t\", index=False)\n",
    "    print(f\"Filtered segments saved to: {filtered_file_path}\")\n",
    "\n",
    "    print(f\"\\nSummary:\")\n",
    "    print(f\"Total segments: {len(segments)}\")\n",
    "    print(f\"Filtered segments: {len(filtered_segments)}\")\n",
    "    if not nan_rows.empty:\n",
    "        print(f\"Rows with NaN values: {len(nan_rows)} (saved to: {nan_file_path})\")\n",
    "\n",
    "\n",
    "    # Step 4: Visualizations\n",
    "    def save_or_show_plot(fig, filename):\n",
    "        if save_plots:\n",
    "            fig.savefig(os.path.join(output_dir, filename))\n",
    "        if show_plots:\n",
    "            plt.show()\n",
    "        plt.close(fig)\n",
    "\n",
    "    def plot_distribution(data, column, title, xlabel, ylabel, filename, bins=30, kde=True):\n",
    "        fig, ax = plt.subplots(figsize=(8, 5))\n",
    "        sns.histplot(data[column], bins=bins, kde=kde, ax=ax)\n",
    "        ax.set_title(title)\n",
    "        ax.set_xlabel(xlabel)\n",
    "        ax.set_ylabel(ylabel)\n",
    "        save_or_show_plot(fig, filename)\n",
    "\n",
    "    # Visualize genetic_length distribution\n",
    "    plot_distribution(\n",
    "        segments, \"genetic_length\", \"Distribution of Genetic Length\", \n",
    "        \"Genetic Length (cM)\", \"Frequency\", \"genetic_length_distribution_unfiltered.png\"\n",
    "    )\n",
    "\n",
    "    plot_distribution(\n",
    "        filtered_segments, \"genetic_length\", \"Distribution of Genetic Length (Filtered)\", \n",
    "        \"Genetic Length (cM)\", \"Frequency\", \"genetic_length_distribution_filtered.png\"\n",
    "    )\n",
    "\n",
    "    # Visualize marker_count distribution\n",
    "    plot_distribution(\n",
    "        segments, \"marker_count\", \"Distribution of Marker Count\", \n",
    "        \"Marker Count\", \"Frequency\", \"marker_count_distribution_unfiltered.png\"\n",
    "    )\n",
    "    plot_distribution(\n",
    "        filtered_segments, \"marker_count\", \"Distribution of Marker Count (Filtered)\", \n",
    "        \"Marker Count\", \"Frequency\", \"marker_count_distribution_filtered.png\"\n",
    "    )\n",
    "\n",
    "    # Boxplot of genetic_length by chromosome\n",
    "    def plot_boxplot(data, x_col, y_col, title, xlabel, ylabel, filename):\n",
    "        fig, ax = plt.subplots(figsize=(10, 6))\n",
    "        sns.boxplot(x=x_col, y=y_col, data=data, ax=ax)\n",
    "        ax.set_title(title)\n",
    "        ax.set_xlabel(xlabel)\n",
    "        ax.set_ylabel(ylabel)\n",
    "        plt.xticks(rotation=45)\n",
    "        plt.tight_layout()\n",
    "        save_or_show_plot(fig, filename)\n",
    "\n",
    "    plot_boxplot(\n",
    "        segments, \"chromosome\", \"genetic_length\", \n",
    "        \"Distribution of Genetic Length by Chromosome\", \n",
    "        \"Chromosome\", \"Genetic Length (cM)\", \"genetic_length_by_chromosome_unfiltered.png\"\n",
    "    )\n",
    "    plot_boxplot(\n",
    "        filtered_segments, \"chromosome\", \"genetic_length\", \n",
    "        \"Distribution of Genetic Length by Chromosome (Filtered)\", \n",
    "        \"Chromosome\", \"Genetic Length (cM)\", \"genetic_length_by_chromosome_filtered.png\"\n",
    "    )\n",
    "\n",
    "    print(\"\\nAnalysis completed.\")\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explore_segments_ibis(\n",
    "        results_directory, \n",
    "        filename=\"merged_opensnps_autosomes_ibis.seg\",\n",
    "        min_length=7, \n",
    "        min_markers=436, \n",
    "        max_error_density=0.004,\n",
    "        save_plots=True,\n",
    "        show_plots=True,\n",
    "        output_subdir=\"segments\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The plots were saved in your `results/segments` directory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take a look at the plots. What information are they communicating? Some are meaningful. Some less so. What other information do you think should be communicated?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Continue to explore the segment data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell takes the segment data output from IBIS IBD dectection algorithm and processes it using `pandas`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = os.path.join(results_directory, \"merged_opensnps_autosomes_ibis.seg\")\n",
    "segments = pd.read_csv(file_path, sep=\"\\t\", header=None)\n",
    "segments.columns = [\n",
    "    \"id1\", \"id2\", \"chromosome\", \"physical_position_start\", \n",
    "    \"physical_position_end\", \"IBD_type\", \"genetic_position_start\", \n",
    "    \"genetic_position_end\", \"genetic_length\", \"marker_count\", \n",
    "    \"error_count\", \"error_density\"\n",
    "]\n",
    "\n",
    "# Ensure numeric columns are properly parsed\n",
    "numeric_columns = [\"genetic_length\", \"marker_count\", \"error_density\", \"chromosome\"]\n",
    "for col in numeric_columns:\n",
    "    if col in segments.columns:\n",
    "        segments[col] = pd.to_numeric(segments[col], errors='coerce')\n",
    "\n",
    "segments = segments.dropna(subset=numeric_columns).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next cell allows you to look at the first five rows. Notice that the data now has column headers. The IBIS output file does not come with column headers. We have to look at the developers' GitHub page or other documenation to determine the names of the columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "segments.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "segments.head() # You can enter a number greater than 5 to view more rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "segments[[\"genetic_length\", \"marker_count\", \"error_count\", \"error_density\"]].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter segments on min_length=7, min_markers=436, max_error_density=0.004,\n",
    "\n",
    "filtered_segments = segments[\n",
    "    (segments[\"genetic_length\"] >= 7) & \n",
    "    (segments[\"marker_count\"] >= 436) & \n",
    "    (segments[\"error_density\"] <= 0.004)\n",
    "].copy()\n",
    "\n",
    "filtered_segments[[\"genetic_length\", \"marker_count\", \"error_count\", \"error_density\"]].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_segments_20cM = filtered_segments[filtered_segments[\"genetic_length\"] >= 20].copy()\n",
    "filtered_segments_20cM[[\"genetic_length\", \"marker_count\", \"error_count\", \"error_density\"]].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's aggregate the data by pairs instead of looking at it by segment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# First ensure id1 and id2 are consistently ordered\n",
    "filtered_segments[[\"id1\", \"id2\"]] = filtered_segments.apply(\n",
    "    lambda row: pd.Series((row[\"id1\"], row[\"id2\"])) if row[\"id1\"] < row[\"id2\"] \n",
    "    else pd.Series((row[\"id2\"], row[\"id1\"])), axis=1\n",
    ")\n",
    "\n",
    "filtered_segments.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pair_counts = filtered_segments.groupby([\"id1\", \"id2\"]).size().reset_index(name=\"pair_count\")\n",
    "pair_count_distribution = pair_counts[\"pair_count\"].value_counts().reset_index()\n",
    "pair_count_distribution.columns = [\"Number of Segments\", \"Number of Pairs\"]\n",
    "pair_count_distribution = pair_count_distribution.reset_index(drop=True)\n",
    "display(pair_count_distribution.style.hide(axis=\"index\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_segments[filtered_segments[\"IBD_type\"] == \"IBD1\"].shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_segments[filtered_segments[\"IBD_type\"] == \"IBD2\"].shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_segments[filtered_segments[\"id1\"] == filtered_segments[\"id2\"]].shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by id pairs and calculate all metrics at once\n",
    "aggregated_segments = filtered_segments.groupby([\"id1\", \"id2\"]).agg(\n",
    "    total_genetic_length=(\"genetic_length\", \"sum\"),\n",
    "    num_segments=(\"genetic_length\", \"count\"),\n",
    "    largest_segment=(\"genetic_length\", \"max\")\n",
    ").reset_index()\n",
    "\n",
    "# Check distribution of values\n",
    "print(aggregated_segments.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aggregated_segments_by_type = filtered_segments.groupby(\n",
    "    [\"id1\", \"id2\", \"IBD_type\"]\n",
    ").agg(\n",
    "    total_genetic_length=(\"genetic_length\", \"sum\"),\n",
    "    num_segments=(\"genetic_length\", \"count\"),\n",
    "    largest_segment=(\"genetic_length\", \"max\")\n",
    ").reset_index()\n",
    "\n",
    "pd.options.display.float_format = '{:.6f}'.format\n",
    "# Display the result\n",
    "ibd1_summary = aggregated_segments_by_type[aggregated_segments_by_type[\"IBD_type\"] == \"IBD1\"].describe()\n",
    "display(ibd1_summary)\n",
    "\n",
    "ibd2_summary = aggregated_segments_by_type[aggregated_segments_by_type[\"IBD_type\"] == \"IBD2\"].describe()\n",
    "display(ibd2_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aggregated_segments[[\"total_genetic_length\", \"num_segments\", \"largest_segment\", \"second_largest_segment\"]].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter the pairs that meet the criteria\n",
    "filtered_pairs1 = aggregated_segments[aggregated_segments[\"total_genetic_length\"] >= 3000]\n",
    "\n",
    "display(filtered_pairs1[[\"id1\", \"id2\", \"total_genetic_length\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter the pairs that meet the criteria\n",
    "filtered_pairs2 = aggregated_segments[aggregated_segments[\"total_genetic_length\"] >= 1000]\n",
    "\n",
    "display(filtered_pairs2[[\"id1\", \"id2\", \"total_genetic_length\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(aggregated_segments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Define columns to plot\n",
    "columns = [\"total_genetic_length\", \"num_segments\", \"largest_segment\"]\n",
    "\n",
    "# Create subplots\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "# Histogram for each metric\n",
    "for i, col in enumerate(columns):\n",
    "    sns.histplot(aggregated_segments[col], bins=30, kde=True, ax=axes[i], edgecolor=\"black\")\n",
    "    axes[i].set_title(f\"Distribution of {col.replace('_', ' ').title()}\")\n",
    "    axes[i].set_xlabel(col.replace('_', ' ').title())\n",
    "    axes[i].set_ylabel(\"Frequency\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Box Plot\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "for i, col in enumerate(columns):\n",
    "    sns.boxplot(y=aggregated_segments[col], ax=axes[i])\n",
    "    axes[i].set_title(f\"Box Plot of {col.replace('_', ' ').title()}\")\n",
    "    axes[i].set_ylabel(col.replace('_', ' ').title())\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
