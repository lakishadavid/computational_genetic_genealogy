{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 15: Model Calibration in Bonsai\n",
    "\n",
    "Building upon our exploration of the data structures in Bonsai in Lab 14, we now examine the process of model calibration in pedigree reconstruction. This lab focuses on how to adjust Bonsai's statistical models to better match real-world genetic inheritance patterns.\n",
    "\n",
    "> **Why This Matters:** The accuracy of pedigree reconstruction depends critically on how well statistical models match the biological reality of genetic inheritance. Model calibration bridges the gap between theoretical expectations and observed data patterns, ensuring that Bonsai can handle real-world genetic data with its inherent noise and biases.\n",
    "\n",
    "**Learning Objectives**:\n",
    "- Understand the importance of model calibration in pedigree reconstruction\n",
    "- Master techniques for calibrating IBD detection thresholds\n",
    "- Learn how to estimate parameters for relationship likelihood models\n",
    "- Develop methods to validate and refine calibration using known relationships\n",
    "- Apply cross-validation techniques to evaluate calibration effectiveness\n",
    "- Implement custom calibration approaches for specific populations or datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!poetry install --no-root"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import networkx as nx\n",
    "from scipy.stats import poisson, expon, norm, multivariate_normal\n",
    "from collections import defaultdict, deque\n",
    "from pathlib import Path\n",
    "from IPython.display import display, HTML\n",
    "from dotenv import load_dotenv\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Environment setup code removed for JupyterLite compatibility\n",
    "# In JupyterLite, files are accessed directly from the files directory\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. The Need for Model Calibration\n",
    "\n",
    "Genetic data in the real world rarely follows perfectly the theoretical distributions we expect. Several factors introduce systematic biases that must be accounted for:\n",
    "\n",
    "- **IBD Detection Errors:** False positives and false negatives in IBD detection\n",
    "- **Population-Specific Recombination:** Variation in recombination rates across populations\n",
    "- **Genotyping Technology:** Systematic biases introduced by different genotyping platforms\n",
    "- **Background Relatedness:** Low-level shared ancestry in endogamous populations\n",
    "- **Ascertainment Bias:** Non-random sampling of individuals for analysis\n",
    "\n",
    "Calibration adjusts Bonsai's internal models to account for these discrepancies, ensuring accurate relationship inference despite these real-world complications."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's visualize how calibration adjusts theoretical models to match observed data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate theoretical vs. observed IBD sharing\n",
    "np.random.seed(42)\n",
    "\n",
    "# Theoretical expectations for segment counts by relationship\n",
    "relationships = ['Parent-Child', 'Full Siblings', 'Half Siblings', 'First Cousins', 'Second Cousins']\n",
    "theoretical_counts = [38, 26, 15, 7, 3]  # theoretical expected segment counts\n",
    "\n",
    "# Simulated \"observed\" data with systematic biases\n",
    "observed_counts = [34, 22, 12, 5, 2]  # systematically lower due to false negatives\n",
    "observed_var = [3, 4, 3, 2, 1]  # variability in observations\n",
    "\n",
    "# Create data points for each relationship\n",
    "observed_data = []\n",
    "for i, rel in enumerate(relationships):\n",
    "    # Create multiple observations for each relationship\n",
    "    observations = np.random.normal(observed_counts[i], observed_var[i], 20)\n",
    "    for obs in observations:\n",
    "        observed_data.append({'relationship': rel, 'segment_count': max(1, int(obs))})\n",
    "\n",
    "# Convert to DataFrame\n",
    "observed_df = pd.DataFrame(observed_data)\n",
    "\n",
    "# Calculate calibration factors\n",
    "calibration_factors = [theoretical_counts[i] / observed_counts[i] for i in range(len(relationships))]\n",
    "\n",
    "# Apply calibration to get calibrated observations\n",
    "observed_df['calibrated_count'] = observed_df.apply(\n",
    "    lambda row: row['segment_count'] * calibration_factors[relationships.index(row['relationship'])],\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# Plot the comparison\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Boxplots of observed data\n",
    "plt.subplot(1, 2, 1)\n",
    "sns.boxplot(x='relationship', y='segment_count', data=observed_df, order=relationships)\n",
    "# Add theoretical counts as red X marks\n",
    "for i, count in enumerate(theoretical_counts):\n",
    "    plt.scatter(i, count, color='red', marker='x', s=100)\n",
    "plt.title('Before Calibration')\n",
    "plt.ylabel('IBD Segment Count')\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "# Boxplots of calibrated data\n",
    "plt.subplot(1, 2, 2)\n",
    "sns.boxplot(x='relationship', y='calibrated_count', data=observed_df, order=relationships)\n",
    "# Add theoretical counts as red X marks\n",
    "for i, count in enumerate(theoretical_counts):\n",
    "    plt.scatter(i, count, color='red', marker='x', s=100)\n",
    "plt.title('After Calibration')\n",
    "plt.ylabel('Calibrated IBD Segment Count')\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Calibrating IBD Detection Thresholds\n",
    "\n",
    "The first step in Bonsai calibration is determining appropriate thresholds for IBD segment detection."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Minimum Segment Length Calibration\n",
    "\n",
    "The choice of minimum segment length threshold balances sensitivity and specificity:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IBDSegment:\n",
    "    def __init__(self, ind1, ind2, chrom, start_pos, end_pos, is_ibd2, length_cm, snp_count=None, lod_score=None):\n",
    "        self.ind1 = ind1          # First individual ID\n",
    "        self.ind2 = ind2          # Second individual ID\n",
    "        self.chrom = chrom        # Chromosome number\n",
    "        self.start_pos = start_pos  # Start position (base pairs)\n",
    "        self.end_pos = end_pos    # End position (base pairs)\n",
    "        self.is_ibd2 = is_ibd2    # Whether this is an IBD2 segment\n",
    "        self.length_cm = length_cm  # Genetic length in centiMorgans\n",
    "        self.snp_count = snp_count  # Number of SNPs in the segment\n",
    "        self.lod_score = lod_score  # LOD score for the segment\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return f\"IBDSegment({self.ind1}, {self.ind2}, chr{self.chrom}, {self.length_cm:.2f}cM, {'IBD2' if self.is_ibd2 else 'IBD1'})\"\n",
    "\n",
    "def calibrate_min_segment_length(ibd_segments, known_relationships=None):\n",
    "    \"\"\"Calibrate the minimum segment length threshold.\n",
    "    \n",
    "    Args:\n",
    "        ibd_segments: List of detected IBD segments\n",
    "        known_relationships: Dictionary mapping pairs to known relationship types (optional)\n",
    "        \n",
    "    Returns:\n",
    "        Recommended minimum segment length threshold\n",
    "    \"\"\"\n",
    "    # Approach 1: ROC curve analysis (if known relationships available)\n",
    "    if known_relationships:\n",
    "        thresholds = np.arange(3, 15, 0.5)  # Test thresholds from 3-15 cM\n",
    "        results = []\n",
    "        \n",
    "        for threshold in thresholds:\n",
    "            # Filter segments by threshold\n",
    "            filtered_segments = [seg for seg in ibd_segments if seg.length_cm >= threshold]\n",
    "            \n",
    "            # Create pair-based index\n",
    "            pair_index = {}\n",
    "            for seg in filtered_segments:\n",
    "                pair = tuple(sorted([seg.ind1, seg.ind2]))\n",
    "                if pair not in pair_index:\n",
    "                    pair_index[pair] = []\n",
    "                pair_index[pair].append(seg)\n",
    "            \n",
    "            # Evaluate accuracy on known relationships\n",
    "            true_positives = 0\n",
    "            false_positives = 0\n",
    "            false_negatives = 0\n",
    "            \n",
    "            # Count pairs with segments above threshold\n",
    "            detected_pairs = set(pair_index.keys())\n",
    "            \n",
    "            # Count pairs with known relationships\n",
    "            known_pairs = set(known_relationships.keys())\n",
    "            \n",
    "            # Calculate metrics\n",
    "            true_positives = len(detected_pairs & known_pairs)\n",
    "            false_positives = len(detected_pairs - known_pairs)\n",
    "            false_negatives = len(known_pairs - detected_pairs)\n",
    "            \n",
    "            precision = true_positives / (true_positives + false_positives) if (true_positives + false_positives) > 0 else 0\n",
    "            recall = true_positives / (true_positives + false_negatives) if (true_positives + false_negatives) > 0 else 0\n",
    "            f1_score = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0\n",
    "            \n",
    "            results.append({\n",
    "                'threshold': threshold,\n",
    "                'precision': precision,\n",
    "                'recall': recall,\n",
    "                'f1_score': f1_score\n",
    "            })\n",
    "        \n",
    "        # Find threshold with best F1 score\n",
    "        best_threshold = max(results, key=lambda x: x['f1_score'])['threshold']\n",
    "        return best_threshold, results\n",
    "    \n",
    "    # Approach 2: Distribution-based calibration (if no known relationships)\n",
    "    else:\n",
    "        # Analyze the distribution of segment lengths\n",
    "        lengths = [seg.length_cm for seg in ibd_segments]\n",
    "        \n",
    "        # Look for the inflection point in the distribution\n",
    "        # Simple heuristic: 5th percentile\n",
    "        return max(7, np.percentile(lengths, 5)), None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's generate some synthetic IBD segments and known relationships to test the calibration function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate synthetic IBD segments\n",
    "np.random.seed(42)\n",
    "\n",
    "# Create a set of individuals\n",
    "individuals = list(range(1000, 1020))\n",
    "\n",
    "# Create some known relationships\n",
    "known_relationships = {\n",
    "    (1000, 1001): 'parent-child',\n",
    "    (1000, 1002): 'parent-child',\n",
    "    (1001, 1002): 'spouses',       # No IBD expected\n",
    "    (1003, 1004): 'siblings',\n",
    "    (1000, 1003): 'grandparent',\n",
    "    (1005, 1006): 'first-cousins',\n",
    "    (1007, 1008): 'second-cousins'\n",
    "}\n",
    "\n",
    "# Generate synthetic IBD segments\n",
    "synthetic_segments = []\n",
    "\n",
    "# Helper function to generate segments for a pair with noise\n",
    "def generate_segments_for_pair(ind1, ind2, relationship, num_segments, length_mean, length_std, noise=0.2):\n",
    "    segments = []\n",
    "    for _ in range(int(np.random.normal(num_segments, num_segments * noise))):\n",
    "        length = max(1, np.random.normal(length_mean, length_std))\n",
    "        chrom = np.random.randint(1, 23)\n",
    "        start_pos = np.random.randint(1, 200000000)\n",
    "        end_pos = start_pos + int(length * 1000000)  # Rough bp conversion\n",
    "        is_ibd2 = relationship == 'siblings' and np.random.random() < 0.25  # 25% chance for siblings\n",
    "        snp_count = int(length * 100)  # Approximate SNP count\n",
    "        lod_score = np.random.normal(length/2, 1)  # Simple LOD score approximation\n",
    "        \n",
    "        segments.append(IBDSegment(ind1, ind2, chrom, start_pos, end_pos, is_ibd2, length, snp_count, lod_score))\n",
    "    return segments\n",
    "\n",
    "# Generate segments for known relationships\n",
    "for (ind1, ind2), rel in known_relationships.items():\n",
    "    if rel == 'parent-child':\n",
    "        synthetic_segments.extend(generate_segments_for_pair(ind1, ind2, rel, 38, 90, 30))\n",
    "    elif rel == 'siblings':\n",
    "        synthetic_segments.extend(generate_segments_for_pair(ind1, ind2, rel, 26, 70, 25))\n",
    "    elif rel == 'grandparent':\n",
    "        synthetic_segments.extend(generate_segments_for_pair(ind1, ind2, rel, 15, 40, 15))\n",
    "    elif rel == 'first-cousins':\n",
    "        synthetic_segments.extend(generate_segments_for_pair(ind1, ind2, rel, 7, 25, 10))\n",
    "    elif rel == 'second-cousins':\n",
    "        synthetic_segments.extend(generate_segments_for_pair(ind1, ind2, rel, 3, 15, 5))\n",
    "    # No segments for spouses\n",
    "\n",
    "# Add some random segments for unrelated individuals (false positives)\n",
    "for _ in range(50):\n",
    "    # Choose random pairs\n",
    "    ind1, ind2 = np.random.choice(individuals, 2, replace=False)\n",
    "    pair = tuple(sorted([ind1, ind2]))\n",
    "    if pair not in known_relationships:\n",
    "        # Generate a small number of short segments\n",
    "        synthetic_segments.extend(generate_segments_for_pair(ind1, ind2, 'unrelated', 2, 6, 2))\n",
    "\n",
    "# Display summary of generated segments\n",
    "print(f\"Generated {len(synthetic_segments)} synthetic IBD segments\")\n",
    "print(f\"Length range: {min([s.length_cm for s in synthetic_segments]):.2f} - {max([s.length_cm for s in synthetic_segments]):.2f} cM\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the calibration function\n",
    "best_threshold, results = calibrate_min_segment_length(synthetic_segments, known_relationships)\n",
    "\n",
    "print(f\"Best minimum segment length threshold: {best_threshold:.2f} cM\")\n",
    "\n",
    "# Visualize the precision-recall tradeoff\n",
    "if results:\n",
    "    results_df = pd.DataFrame(results)\n",
    "    \n",
    "    plt.figure(figsize=(12, 5))\n",
    "    \n",
    "    # Plot precision, recall, and F1 score\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(results_df['threshold'], results_df['precision'], 'b-', label='Precision')\n",
    "    plt.plot(results_df['threshold'], results_df['recall'], 'r-', label='Recall')\n",
    "    plt.plot(results_df['threshold'], results_df['f1_score'], 'g-', label='F1 Score')\n",
    "    plt.axvline(x=best_threshold, color='black', linestyle='--', label=f'Best Threshold: {best_threshold:.2f} cM')\n",
    "    plt.xlabel('Minimum Segment Length Threshold (cM)')\n",
    "    plt.ylabel('Score')\n",
    "    plt.title('Precision-Recall Tradeoff')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot precision vs recall\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(results_df['recall'], results_df['precision'], 'b-')\n",
    "    # Mark points for various thresholds\n",
    "    for t in [3, 5, 7, 10, 12, 14]:\n",
    "        row = results_df[results_df['threshold'] == t].iloc[0] if not results_df[results_df['threshold'] == t].empty else None\n",
    "        if row is not None:\n",
    "            plt.scatter(row['recall'], row['precision'], color='red')\n",
    "            plt.annotate(f\"{t} cM\", (row['recall'], row['precision']), textcoords=\"offset points\", xytext=(0,10), ha='center')\n",
    "    plt.xlabel('Recall')\n",
    "    plt.ylabel('Precision')\n",
    "    plt.title('Precision vs Recall for Different Thresholds')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Appropriate thresholds can vary based on:\n",
    "- The IBD detection algorithm used (IBIS, Refined-IBD, HAP-IBD, etc.)\n",
    "- Population density of the dataset\n",
    "- Genotyping density and quality\n",
    "- The specific relationships of interest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### False Positive Control\n",
    "\n",
    "Additional filtering can be applied to reduce false positives:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_ibd_segments(segments):\n",
    "    \"\"\"Filter IBD segments to reduce false positives.\"\"\"\n",
    "    filtered = []\n",
    "    \n",
    "    for seg in segments:\n",
    "        # Filter by minimum genetic length\n",
    "        if seg.length_cm < 7:\n",
    "            continue\n",
    "            \n",
    "        # Filter by minimum physical length\n",
    "        if (seg.end_pos - seg.start_pos) < 500000:  # 500kb minimum\n",
    "            continue\n",
    "            \n",
    "        # Filter by SNP density\n",
    "        if hasattr(seg, 'snp_count') and seg.snp_count < 400:  # Minimum SNPs for confidence\n",
    "            continue\n",
    "            \n",
    "        # Filter by LOD score if available\n",
    "        if hasattr(seg, 'lod_score') and seg.lod_score < 5:\n",
    "            continue\n",
    "            \n",
    "        # Filter by chromosome (e.g., exclude problematic regions)\n",
    "        if str(seg.chrom) in ['X', 'Y', 'MT']:  # Special handling for sex/mitochondrial chromosomes\n",
    "            # Apply more stringent criteria\n",
    "            if seg.length_cm < 10:  # Higher threshold for sex chromosomes\n",
    "                continue\n",
    "        \n",
    "        filtered.append(seg)\n",
    "    \n",
    "    return filtered\n",
    "\n",
    "# Apply filtering to our synthetic segments\n",
    "filtered_segments = filter_ibd_segments(synthetic_segments)\n",
    "\n",
    "print(f\"Before filtering: {len(synthetic_segments)} segments\")\n",
    "print(f\"After filtering: {len(filtered_segments)} segments\")\n",
    "print(f\"Removed {len(synthetic_segments) - len(filtered_segments)} segments ({(1 - len(filtered_segments)/len(synthetic_segments))*100:.1f}%)\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}