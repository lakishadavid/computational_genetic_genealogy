{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from collections import Counter\n",
    "import logging\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import zipfile\n",
    "import urllib3\n",
    "from urllib3.util import Retry\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load your environment variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_comp_gen_dir():\n",
    "    \"\"\"Find the computational_genetic_genealogy directory by searching up from current directory.\"\"\"\n",
    "    current = Path.cwd()\n",
    "    \n",
    "    # Search up through parent directories\n",
    "    while current != current.parent:\n",
    "        # Check if target directory exists in current path\n",
    "        target = current / 'computational_genetic_genealogy'\n",
    "        if target.is_dir():\n",
    "            return target\n",
    "        # Move up one directory\n",
    "        current = current.parent\n",
    "    \n",
    "    raise FileNotFoundError(\"Could not find computational_genetic_genealogy directory\")\n",
    "\n",
    "def load_env_file():\n",
    "    \"\"\"Find and load the .env file from the computational_genetic_genealogy directory.\"\"\"\n",
    "    try:\n",
    "        # Find the computational_genetic_genealogy directory\n",
    "        comp_gen_dir = find_comp_gen_dir()\n",
    "        \n",
    "        # Look for .env file\n",
    "        env_path = comp_gen_dir / '.env'\n",
    "        if not env_path.exists():\n",
    "            print(f\"Warning: No .env file found in {comp_gen_dir}\")\n",
    "            return None\n",
    "        \n",
    "        # Load the .env file\n",
    "        load_dotenv(env_path, override=True)\n",
    "        print(f\"Loaded environment variables from: {env_path}\")\n",
    "        return env_path\n",
    "        \n",
    "    except FileNotFoundError as e:\n",
    "        print(f\"Error: {e}\")\n",
    "        return None\n",
    "\n",
    "# Use the function\n",
    "env_path = load_env_file()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let the environment know where bcftools is located."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['BCFTOOLS_PLUGINS'] = os.path.expanduser('~/.local/libexec/bcftools')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "working_directory = os.getenv('PROJECT_WORKING_DIR', default=None)\n",
    "data_directory = os.getenv('PROJECT_DATA_DIR', default=None)\n",
    "references_directory = os.getenv('PROJECT_REFERENCES_DIR', default=None)\n",
    "results_directory = os.getenv('PROJECT_RESULTS_DIR', default=None)\n",
    "utils_directory = os.getenv('PROJECT_UTILS_DIR', default=None)\n",
    "\n",
    "print(f\"Working Directory: {working_directory}\")\n",
    "print(f\"Data Directory: {data_directory}\")\n",
    "print(f\"References Directory: {references_directory}\")\n",
    "print(f\"Results Directory: {results_directory}\")\n",
    "print(f\"Utils Directory: {utils_directory}\")\n",
    "\n",
    "os.chdir(working_directory)\n",
    "print(f\"The current directory is {os.getcwd()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def configure_logging(log_filename, log_file_debug_level=\"INFO\", console_debug_level=\"INFO\"):\n",
    "    \"\"\"\n",
    "    Configure logging for both file and console handlers.\n",
    "\n",
    "    Args:\n",
    "        log_filename (str): Path to the log file where logs will be written.\n",
    "        log_file_debug_level (str): Logging level for the file handler.\n",
    "        console_debug_level (str): Logging level for the console handler.\n",
    "    \"\"\"\n",
    "    # Create a root logger\n",
    "    logger = logging.getLogger()\n",
    "    logger.setLevel(logging.DEBUG)  # Capture all messages at the root level\n",
    "\n",
    "    # Convert level names to numeric levels\n",
    "    file_level = getattr(logging, log_file_debug_level.upper(), logging.INFO)\n",
    "    console_level = getattr(logging, console_debug_level.upper(), logging.INFO)\n",
    "\n",
    "    # File handler: Logs messages at file_level and above to the file\n",
    "    file_handler = logging.FileHandler(log_filename)\n",
    "    file_handler.setLevel(file_level)\n",
    "    file_formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')\n",
    "    file_handler.setFormatter(file_formatter)\n",
    "\n",
    "    # Console handler: Logs messages at console_level and above to the console\n",
    "    console_handler = logging.StreamHandler(sys.stdout)\n",
    "    console_handler.setLevel(console_level)\n",
    "    console_formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')\n",
    "    console_handler.setFormatter(console_formatter)\n",
    "\n",
    "    # Add handlers to the root logger\n",
    "    logger.addHandler(file_handler)\n",
    "    logger.addHandler(console_handler)\n",
    "    \n",
    "def clear_logger():\n",
    "    \"\"\"Remove all handlers from the root logger.\"\"\"\n",
    "    logger = logging.getLogger()\n",
    "    for handler in logger.handlers[:]:\n",
    "        logger.removeHandler(handler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_filename = os.path.join(results_directory, \"lab4_log.txt\")\n",
    "print(f\"The Lab 4 log file is located at {log_filename}.\")\n",
    "\n",
    "# Ensure the results_directory exists\n",
    "if not os.path.exists(results_directory):\n",
    "    os.makedirs(results_directory)\n",
    "\n",
    "# Check if the file exists; if not, create it\n",
    "if not os.path.exists(log_filename):\n",
    "    with open(log_filename, 'w') as file:\n",
    "        pass  # The file is now created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clear_logger() # Clear the logger before reconfiguring it\n",
    "configure_logging(log_filename, log_file_debug_level=\"INFO\", console_debug_level=\"INFO\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Genetic Maps\n",
    "\n",
    "Genetic maps, also known as recombination maps, are essential tools that illustrate the relative positions of genetic markers (such as single nucleotide polymorphisms, or SNPs) along a chromosome. Unlike physical maps that measure distances in base pairs, genetic maps measure distances in centiMorgans (cM), where one centiMorgan represents a 1% probability of recombination between markers during meiosis.\n",
    "\n",
    "## Key Components of Genetic Maps\n",
    "\n",
    "- **Markers:**  \n",
    "  Identifiable DNA sequences used as reference points on the genome.\n",
    "\n",
    "- **Recombination Frequency:**  \n",
    "  The probability of a recombination event occurring between markers, which informs the genetic distances.\n",
    "\n",
    "- **Map Distance:**  \n",
    "  Expressed in centiMorgans (cM), reflecting the likelihood of recombination rather than the physical distance.\n",
    "\n",
    "## BEAGLE's Genetic Map\n",
    "\n",
    "BEAGLE is a widely used software package for phasing, genotype imputation, and identity-by-descent (IBD) analysis. Its performance is closely tied to the use of high-resolution genetic maps. Here are some distinctive features of BEAGLE's genetic map:\n",
    "\n",
    "- **High Marker Density:**  \n",
    "  The genetic maps provided with BEAGLE include a dense array of markers. This density allows for the precise capture of fine-scale recombination events, which in turn improves the accuracy of haplotype phasing and genotype imputation.\n",
    "\n",
    "- **Species and Population Specificity:**  \n",
    "  The maps are often developed from extensive pedigree or population studies. For human genetic studies, they are constructed based on large-scale recombination data, ensuring relevance to the population under study.\n",
    "\n",
    "- **Integration with Statistical Models:**  \n",
    "  BEAGLE utilizes these maps within its statistical algorithms to model recombination events effectively. This integration is crucial for accurately inferring missing genotypes and detecting IBD segments.\n",
    "\n",
    "- **Enhanced Analysis Accuracy:**  \n",
    "  The detailed recombination information in BEAGLE's genetic maps allows for better adjustment for linkage disequilibrium and recombination rates, ultimately leading to more robust downstream genetic analyses.\n",
    "\n",
    "## Benefits of Using BEAGLE's Genetic Map\n",
    "\n",
    "- **Improved Phasing Accuracy:**  \n",
    "  The high-resolution data facilitates precise haplotype reconstruction, reducing errors in phase determination.\n",
    "\n",
    "- **Robust Genotype Imputation:**  \n",
    "  Detailed recombination rate data enhances the accuracy of imputing missing genotypes, ensuring more reliable datasets.\n",
    "\n",
    "- **Streamlined Analysis Workflow:**  \n",
    "  The genetic map is specifically tailored to integrate seamlessly with BEAGLE’s algorithms, thereby optimizing the overall analysis process.\n",
    "\n",
    "## References\n",
    "\n",
    "1. Browning, B. L., & Browning, S. R. (2007). *Rapid and Accurate Haplotype Phasing and Missing-Data Inference for Whole-Genome Association Studies by Use of Localized Haplotype Clustering*. [American Journal of Human Genetics](https://www.cell.com/AJHG/fulltext/S0002-9297(07)63882-8)\n",
    "2. Browning, B. L., Zhou, Y., & Browning, S. R. (2018). *A One-Penny Imputed Genome from Next-Generation Reference Panels*. [American Journal of Human Genetics](https://pubmed.ncbi.nlm.nih.gov/30100085/)\n",
    "3. [BEAGLE Documentation](https://faculty.washington.edu/browning/beagle/beagle.html)\n",
    "4. [NHGRI Glossary: Genetic Map](https://www.genome.gov/genetics-glossary/Genetic-Map)\n",
    "5. Li, Y., Willer, C., Sanna, S., & Abecasis, G. (2009). *Genotype Imputation*. [Annual Review of Genomics and Human Genetics](https://www.annualreviews.org/content/journals/10.1146/annurev.genom.9.081307.164242)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download Genetic Maps (Beagle's plink version)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define download helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_session_with_retries():\n",
    "    \"\"\"Create a requests session with retry strategy\"\"\"\n",
    "    retry_strategy = Retry(\n",
    "        total=5,\n",
    "        backoff_factor=0.5,\n",
    "        status_forcelist=[500, 502, 503, 504]\n",
    "    )\n",
    "    http = urllib3.PoolManager(retries=retry_strategy)\n",
    "    return http\n",
    "\n",
    "session = create_session_with_retries()\n",
    "\n",
    "def download_with_progress(url, output_path):\n",
    "    \"\"\"Download file with progress tracking\"\"\"\n",
    "    response = session.request('GET', url, preload_content=False)\n",
    "\n",
    "    if response.status != 200:\n",
    "        raise Exception(f\"HTTP error occurred: {response.status} {response.reason}\")\n",
    "    \n",
    "    total_size = int(response.headers.get('content-length', 0))\n",
    "    block_size = 8192\n",
    "    progress_increment = max(1, total_size // 50) if total_size > 0 else block_size\n",
    "    \n",
    "    with open(output_path, 'wb') as f:\n",
    "        downloaded = 0\n",
    "        last_print = 0\n",
    "        while True:\n",
    "            chunk = response.read(block_size)\n",
    "            if not chunk:\n",
    "                break\n",
    "                \n",
    "            f.write(chunk)\n",
    "            downloaded += len(chunk)\n",
    "            if total_size > 0 and downloaded - last_print >= progress_increment:\n",
    "                last_print = downloaded\n",
    "                progress = (downloaded / total_size) * 100\n",
    "                logging.info(f\"Download progress: {progress:.1f}%\")\n",
    "                \n",
    "    response.release_conn()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download the genetic map files from Beagle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_from_beagle(assembly, output_directory):\n",
    "    \"\"\"Downloads and processes Beagle genetic maps\"\"\"\n",
    "    assembly_map_files = {\n",
    "        \"GRCh36\": \"plink.GRCh36.map.zip\",\n",
    "        \"GRCh37\": \"plink.GRCh37.map.zip\",\n",
    "        \"GRCh38\": \"plink.GRCh38.map.zip\"\n",
    "    }\n",
    "\n",
    "    if assembly not in assembly_map_files:\n",
    "        raise ValueError(f\"Unsupported assembly '{assembly}'. Must be one of {list(assembly_map_files.keys())}.\")\n",
    "\n",
    "    file_name = assembly_map_files[assembly]\n",
    "    url = f\"https://bochet.gcc.biostat.washington.edu/beagle/genetic_maps/{file_name}\"\n",
    "    output_path = os.path.join(output_directory, file_name)\n",
    "\n",
    "    try:\n",
    "        logging.info(f\"Downloading Beagle map from {url}\")\n",
    "        download_with_progress(url, output_path)\n",
    "        \n",
    "        # Verify zip file integrity\n",
    "        try:\n",
    "            with zipfile.ZipFile(output_path, 'r') as zip_ref:\n",
    "                # Test zip file before extraction\n",
    "                test_result = zip_ref.testzip()\n",
    "                if test_result is not None:\n",
    "                    raise zipfile.BadZipFile(f\"Corrupted file found in ZIP: {test_result}\")\n",
    "                \n",
    "                # Extract files\n",
    "                logging.info(f\"Extracting files to {output_directory}\")\n",
    "                zip_ref.extractall(output_directory)\n",
    "                \n",
    "                # Log extracted files\n",
    "                extracted_files = zip_ref.namelist()\n",
    "                logging.info(f\"Extracted {len(extracted_files)} files: {', '.join(extracted_files)}\")\n",
    "        \n",
    "        except zipfile.BadZipFile as e:\n",
    "            raise Exception(f\"Invalid ZIP file downloaded: {str(e)}\")\n",
    "        \n",
    "        # Clean up zip file\n",
    "        os.remove(output_path)\n",
    "        logging.info(\"ZIP file cleaned up\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error processing Beagle files: {str(e)}\")\n",
    "        if os.path.exists(output_path):\n",
    "            os.remove(output_path)\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up output directories\n",
    "genetic_maps_directory = os.path.join(references_directory, \"genetic_maps\")\n",
    "os.makedirs(genetic_maps_directory, exist_ok=True)\n",
    "\n",
    "beagle_genetic_maps = os.path.join(genetic_maps_directory, \"beagle_genetic_maps\")\n",
    "os.makedirs(beagle_genetic_maps, exist_ok=True)\n",
    "\n",
    "assembly = \"GRCh38\"\n",
    "output_directory = beagle_genetic_maps\n",
    "\n",
    "# Download Beagle genetic maps\n",
    "download_from_beagle(assembly, output_directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### New Resource\n",
    "\n",
    "Take a look at your `genetic_maps` directory. You should see the `beagle_genetic_maps` directory. Within `beagle_genetic_maps`, you should see your genetic map files, one for each chromosome. The naming convention is `plink.chr{chromosome_number}.GRCh38.map`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VCF Quality Control and Processing Pipeline\n",
    "\n",
    "This script implements a comprehensive quality control (QC) and processing pipeline for merged VCF files, designed specifically for downstream genetic analyses (e.g., genetic genealogy). The pipeline integrates several tools (e.g., PLINK2, bcftools, Beagle) to perform quality control, filtering, and conversion of VCF files into other formats."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore the Code Chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Validate the Merged VCF file as input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "\n",
    "def validate_merged_vcf(vcf_path):\n",
    "    \"\"\"Validate merged VCF and extract available chromosomes.\"\"\"\n",
    "    cmd_counts = [\"bcftools\", \"plugin\", \"counts\", vcf_path]\n",
    "    result_counts = subprocess.run(cmd_counts, capture_output=True, text=True, check=True)\n",
    "    logging.info(f\"Plugin 'counts' validation output for {vcf_path}:\\n{result_counts.stdout}\")\n",
    "    if result_counts.stderr:\n",
    "        logging.info(f\"Plugin 'counts' validation errors:\\n{result_counts.stderr}\")\n",
    "\n",
    "    num_samples = 0\n",
    "    for line in result_counts.stdout.splitlines():\n",
    "        if line.startswith(\"Number of samples:\"):\n",
    "            parts = line.split(\":\")\n",
    "            if len(parts) == 2:\n",
    "                num_samples = int(parts[1].strip())\n",
    "    if not num_samples:\n",
    "        logging.error(f\"No sample count found in VCF file: {vcf_path}\")\n",
    "\n",
    "    num_snps = 0\n",
    "    for line in result_counts.stdout.splitlines():\n",
    "        if line.startswith(\"Number of SNPs:\"):\n",
    "            parts = line.split(\":\")\n",
    "            if len(parts) == 2:\n",
    "                num_snps = int(parts[1].strip())\n",
    "    if not num_snps:\n",
    "        logging.error(f\"No sample count found in VCF file: {vcf_path}\")\n",
    "\n",
    "\n",
    "    logging.info(\"Extracting list of chromosomes from the VCF header.\")\n",
    "    cmd_chrom_contig = f\"bcftools view -h {vcf_path} | grep '^##contig' | cut -d'=' -f3 | cut -d',' -f1\"\n",
    "    result_chrom_contig = subprocess.run(cmd_chrom_contig, shell=True, capture_output=True, text=True, check=True)\n",
    "    chromosomes_contig = result_chrom_contig.stdout.splitlines()\n",
    "    if not chromosomes_contig:\n",
    "        logging.error(f\"No chromosomes found in VCF file: {vcf_path}\")\n",
    "    else:\n",
    "        logging.debug(f\"Chromosomes found in VCF file header: {', '.join(chromosomes_contig)}\")\n",
    "\n",
    "\n",
    "    logging.info(\"Extracting a list of chromosomes from the CHROM column..\")\n",
    "    cmd_chrom_field = f\"bcftools query -f '%CHROM\\n' {vcf_path} | sort -u\"\n",
    "    result_chrom_field = subprocess.run(cmd_chrom_field, shell=True, capture_output=True, text=True, check=True)\n",
    "    chromosomes_field = result_chrom_field.stdout.splitlines()\n",
    "    if not chromosomes_field:\n",
    "        logging.error(f\"No chromosomes found in VCF file in the CHROM field: {vcf_path}\")\n",
    "    else:\n",
    "        logging.debug(f\"Chromosomes found in VCF file in the CHROM field: {', '.join(chromosomes_field)}\")\n",
    "\n",
    "\n",
    "    if chromosomes_contig != chromosomes_field:\n",
    "        logging.error(\"Mismatch between chromosomes in contig and field headers.\")\n",
    "        logging.error(f\"Contig chromosomes: {chromosomes_contig}\")\n",
    "        logging.error(f\"Field chromosomes: {chromosomes_field}\")\n",
    "\n",
    "\n",
    "    logging.info(\"Extracting sample IDs from the VCF file.\")\n",
    "    cmd_sample_list = [\"bcftools\", \"query\", \"-l\", vcf_path]\n",
    "    result_sample_list = subprocess.run(cmd_sample_list, capture_output=True, text=True, check=True)\n",
    "    sample_ids = result_sample_list.stdout.splitlines()\n",
    "\n",
    "    if not sample_ids:\n",
    "        logging.error(f\"No sample IDs found in VCF file: {vcf_path}\")\n",
    "    else:\n",
    "        logging.debug(f\"Sample IDs found in VCF file: {', '.join(sample_ids)}\")\n",
    "\n",
    "    return num_samples, num_snps, chromosomes_field, sample_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VCF created in Lab3 Get Raw DNA Profile\n",
    "vcf_path = f\"{data_directory}/class_data/merged_opensnps_data.vcf.gz\"\n",
    "print(vcf_path)\n",
    "num_samples, num_snps, chromosomes, sample_ids = validate_merged_vcf(vcf_path)\n",
    "print(num_samples, num_snps, chromosomes, sample_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** The ERROR between the Contig chromosomes and Field chromosomes are okay for now. Try to see why there is an error here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prepare the Supplemental Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_sex_determination(determined_sex_file, failed_sex):\n",
    "    \"\"\"Parse the sex determination log and create a mapping of user IDs to sexes.\"\"\"\n",
    "    sex_mapping = {}\n",
    "    with open(determined_sex_file, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "\n",
    "    for line in lines:\n",
    "        line = line.strip()\n",
    "        if not line:  # Skip empty lines\n",
    "            continue\n",
    "\n",
    "        user_id, sex = line.split(\"\\t\")\n",
    "        sex_mapping[user_id] = \"1\" if sex == \"Male\" else \"2\"\n",
    "\n",
    "    with open(failed_sex, 'r') as p:\n",
    "        lines = p.readlines()\n",
    "\n",
    "    for line in lines:\n",
    "        line = line.strip()\n",
    "        if not line:\n",
    "            continue\n",
    "\n",
    "        user_id, sex = line.split(\"\\t\")\n",
    "        sex_mapping[user_id] = \"0\" # Unknown sex\n",
    "\n",
    "    # Count occurrences of each sex code\n",
    "    counts = Counter(sex_mapping.values())\n",
    "\n",
    "    # Print results\n",
    "    logging.info(f\"Count of SEX=0 (Unknown): {counts['0']}\")\n",
    "    logging.info(f\"Count of SEX=1 (Male): {counts['1']}\")\n",
    "    logging.info(f\"Count of SEX=2 (Female): {counts['2']}\")\n",
    "\n",
    "    return sex_mapping\n",
    "\n",
    "def write_sex_files(sex_mapping, sample_ids, psam_file_all, psam_file_Y, sex_update_file):\n",
    "    \"\"\"Write both PLINK2-compatible .psam files and sex update file.\"\"\"\n",
    "    \n",
    "    # Reorder sex_mapping based on sample_ids\n",
    "    ordered_sex_mapping = {sample_id: sex_mapping.get(sample_id, \"0\") for sample_id in sample_ids}\n",
    "    \n",
    "    # Write standard .psam file for all chromosomes\n",
    "    with open(psam_file_all, 'w') as f:\n",
    "        f.write(\"#FID\\tIID\\tSEX\\n\")  # Header for .psam file\n",
    "        for user_id, sex_code in ordered_sex_mapping.items():\n",
    "            if sex_code == \"0\":\n",
    "                continue  # Exclude unknown sexes\n",
    "            f.write(f\"{user_id}\\t{user_id}\\t{sex_code}\\n\")\n",
    "    \n",
    "    # Write .psam file for Y chromosome (males only)\n",
    "    with open(psam_file_Y, 'w') as f:\n",
    "        f.write(\"#FID\\tIID\\tSEX\\n\")\n",
    "        for user_id, sex_code in ordered_sex_mapping.items():\n",
    "            if sex_code != \"1\":\n",
    "                continue  # Exclude non-males\n",
    "            f.write(f\"{user_id}\\t{user_id}\\t{sex_code}\\n\")\n",
    "    \n",
    "    # Write sex update file for PLINK2 --update-sex\n",
    "    with open(sex_update_file, 'w') as f:\n",
    "        f.write(\"#IID\\tSEX\\n\")  # PLINK2 format for sex update\n",
    "        for user_id, sex_code in ordered_sex_mapping.items():\n",
    "            if sex_code == \"0\":\n",
    "                continue  # Exclude unknown sexes\n",
    "            f.write(f\"{user_id}\\t{sex_code}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "determined_sex_file = f\"{data_directory}/class_data/determined_sex.txt\"\n",
    "failed_sex = f\"{data_directory}/class_data/failed_sex.txt\"\n",
    "\n",
    "sex_mapping = parse_sex_determination(determined_sex_file, failed_sex)\n",
    "base_name = os.path.splitext(determined_sex_file)[0]\n",
    "psam_file_all = f\"{base_name}_all.psam\"\n",
    "psam_file_Y = f\"{base_name}_Y.psam\"\n",
    "sex_update_file = f\"{base_name}_update_sex.txt\"\n",
    "write_sex_files(sex_mapping, sample_ids, psam_file_all, psam_file_Y, sex_update_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Quality Control Filtering:**  \n",
    "   Applies a series of default QC filters to the VCF file:\n",
    "   - **Autosomal Filtering:** Only autosomal SNPs are retained (excludes X and Y chromosomes to simplify analysis).\n",
    "   - **Duplicate Removal:** Duplicated SNPs are removed (keeps the first occurrence).\n",
    "   - **VCF Half-Call Handling:** Half-calls are treated as missing.\n",
    "   - **SNP Filtering:** Keeps only SNPs with nucleotide calls in {A, C, G, T} (ignoring case).\n",
    "   - **Biallelic SNPs:** Filters for SNPs with exactly two alleles.\n",
    "   - **Genotype Missingness (`--geno`):** Excludes SNPs with a missingness rate exceeding the specified threshold.\n",
    "   - **Minor Allele Frequency (`--maf`):** Excludes SNPs with a frequency lower than the specified threshold.\n",
    "\n",
    "**Data Conversion and Filtering:**  \n",
    "   - **Step 1:** Converts the VCF file into PLINK format using `plink2` with QC parameters.\n",
    "   - **Step 2:** Filters the PLINK data by genotype missingness and minor allele frequency.\n",
    "   - **Step 3:** Splits the dataset by chromosome and exports each chromosome’s data as a VCF file. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Quality Control Step 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash -s \"$data_directory\" \"$utils_directory\" \"$results_directory\"\n",
    "\n",
    "data_directory=\"$1\"\n",
    "utils_directory=\"$2\"\n",
    "results_directory=\"$3\"\n",
    "\n",
    "#The first step of the pipeline involves parsing command-line arguments and applying default quality control parameters to the merged VCF file. These filters include:\n",
    "\n",
    "# - **Autosomal Filtering:** Only autosomal SNPs are retained.\n",
    "# - **Duplicate Removal:** Duplicate SNPs are removed (keeping the first occurrence).\n",
    "# - **VCF Half-Call Handling:** Half-calls are treated as missing.\n",
    "# - **SNP Filtering:** Only SNPs with nucleotide types {A, C, G, T} are kept.\n",
    "# - **Biallelic SNPs:** Only SNPs with exactly two alleles are retained.\n",
    "# - **Genotype Missingness (`--geno`):** Excludes SNPs with a missingness rate above a threshold.\n",
    "# - **Minor Allele Frequency (`--maf`):** Excludes SNPs with a frequency below a threshold.\n",
    "\n",
    "# Change to the utils directory if necessary\n",
    "cd \"${utils_directory}\"\n",
    "\n",
    "\n",
    "plink2 --vcf ${data_directory}/class_data/merged_opensnps_data.vcf.gz \\\n",
    "  --autosome \\\n",
    "  --snps-only just-acgt \\\n",
    "  --rm-dup exclude-all \\\n",
    "  --min-alleles 2 \\\n",
    "  --max-alleles 2 \\\n",
    "  --make-pgen \\\n",
    "  --out ${results_directory}/merged_opensnps_autosomes_step1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Quality Control Step 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash -s \"$data_directory\" \"$utils_directory\" \"$results_directory\"\n",
    "\n",
    "data_directory=\"$1\"\n",
    "utils_directory=\"$2\"\n",
    "results_directory=\"$3\"\n",
    "\n",
    "# Change to the utils directory if necessary\n",
    "cd \"${utils_directory}\"\n",
    "\n",
    "echo \"Filtering by genotype missingness (geno=${geno}) and minor allele frequency (maf=${maf})...\"\n",
    "plink2 --pfile ${results_directory}/merged_opensnps_autosomes_step1 \\\n",
    "  --geno .05 \\\n",
    "  --maf .05 \\\n",
    "  --sort-vars \\\n",
    "  --make-pgen \\\n",
    "  --out ${results_directory}/merged_opensnps_autosomes_step2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Quality Control Step 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash -s \"$data_directory\" \"$utils_directory\" \"$results_directory\"\n",
    "\n",
    "data_directory=\"$1\"\n",
    "utils_directory=\"$2\"\n",
    "results_directory=\"$3\"\n",
    "sample_file=\"merged_opensnps\"\n",
    "\n",
    "# The input prefix is the output from step 2\n",
    "input_prefix=\"${results_directory}/merged_opensnps_autosomes_step2\"\n",
    "\n",
    "echo \"Splitting by chromosome and exporting as VCF...\"\n",
    "\n",
    "# Loop over autosomal chromosomes 1 through 22\n",
    "for chromosome in {1..22}; do\n",
    "    echo \"Processing chromosome ${chromosome}...\"\n",
    "    \n",
    "    # Define the output prefix for the current chromosome\n",
    "    output_prefix=\"${results_directory}/${sample_file}_qcstart_chr${chromosome}\"\n",
    "    \n",
    "    # Export the chromosome-specific data as a VCF using plink2\n",
    "    plink2 --pfile \"$input_prefix\" \\\n",
    "           --chr \"${chromosome}\" \\\n",
    "           --export vcf \\\n",
    "           --out \"$output_prefix\"\n",
    "    \n",
    "    # Define the input and output for bcftools processing\n",
    "    bcftools_input=\"${output_prefix}.vcf\"\n",
    "    bcftools_output=\"${results_directory}/${sample_file}_qcfinished_chr${chromosome}.vcf.gz\"\n",
    "    \n",
    "    echo \"Filtering for biallelic variants on chromosome ${chromosome}...\"\n",
    "    # Filter for biallelic variants and compress the VCF using bcftools\n",
    "    bcftools view -m2 -M2 -Oz -o \"$bcftools_output\" \"$bcftools_input\"\n",
    "    \n",
    "    echo \"Indexing the filtered VCF for chromosome ${chromosome}...\"\n",
    "    # Index the compressed VCF file\n",
    "    bcftools index \"$bcftools_output\"\n",
    "done\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Phasing:**  \n",
    "   - After QC and filtering, the script calls an external shell script (`phase_chromosomes.sh`) to phase chromosomes using Beagle.  \n",
    "   - The script passes an input file prefix (derived from the processed VCF file) along with directory paths and the Beagle JAR file for further phasing operations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Phase the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash -s \"$data_directory\" \"$references_directory\" \"$results_directory\" \"$utils_directory\"\n",
    "\n",
    "data_directory=\"$1\"\n",
    "references_directory=\"$2\"\n",
    "results_directory=\"$3\"\n",
    "utils_directory=\"$4\"\n",
    "sample_file=\"merged_opensnps\"\n",
    "input_prefix=\"${sample_file}_qcfinished\"\n",
    "phased_directory=\"${results_directory}/phased_samples\"\n",
    "beagle=\"${utils_directory}/beagle.17Dec24.224.jar\"\n",
    "\n",
    "# Create the phased directory if it does not exist\n",
    "mkdir -p \"$phased_directory\"\n",
    "\n",
    "# Phase chromosomes using Beagle\n",
    "for CHR in {1..22}; do\n",
    "    echo \"Processing chromosome $CHR\"\n",
    "\n",
    "    INPUT_VCF=\"${results_directory}/${input_prefix}_chr${CHR}.vcf.gz\"\n",
    "    REF_VCF=\"${references_directory}/onethousandgenomes_genotype/onethousandgenomes_genotyped_phased.chr${CHR}.vcf.gz\"\n",
    "    MAP_FILE=\"${references_directory}/genetic_maps/beagle_genetic_maps/plink.chr${CHR}.GRCh38.map\"\n",
    "    OUTPUT_PREFIX=\"${phased_directory}/merged_opensnps_phased_chr${CHR}\"\n",
    "    PHASED_VCF=\"${OUTPUT_PREFIX}.vcf.gz\"\n",
    "\n",
    "    # Check if input VCF exists\n",
    "    if [ ! -f \"$INPUT_VCF\" ]; then\n",
    "        echo \"Input VCF file not found for chromosome $CHR. Skipping.\"\n",
    "        echo \"$INPUT_VCF\"\n",
    "        continue\n",
    "    fi\n",
    "\n",
    "    if [ -f \"$REF_VCF\" ]; then\n",
    "        # Run Beagle with reference file\n",
    "        java -jar ${beagle} \\\n",
    "            gt=\"$INPUT_VCF\" \\\n",
    "            ref=\"$REF_VCF\" \\\n",
    "            map=\"$MAP_FILE\" \\\n",
    "            out=\"$OUTPUT_PREFIX\"\n",
    "    else\n",
    "        echo \"Note: The reference file does not exist; the file is phased based on no reference panel.\"\n",
    "        # Run Beagle without reference file\n",
    "        java -jar ${beagle} \\\n",
    "            gt=\"$INPUT_VCF\" \\\n",
    "            map=\"$MAP_FILE\" \\\n",
    "            out=\"$OUTPUT_PREFIX\"\n",
    "    fi\n",
    "    \n",
    "    if [ $? -ne 0 ]; then\n",
    "        echo \"Beagle failed for chromosome $CHR. Skipping.\"\n",
    "        continue\n",
    "    fi\n",
    "\n",
    "    if [ ! -f \"$PHASED_VCF\" ]; then\n",
    "        echo \"Phasing failed for chromosome $CHR. Skipping.\"\n",
    "        continue\n",
    "    fi\n",
    "\n",
    "    # Sort and index the phased VCF\n",
    "    echo \"Sorting and indexing phased VCF for chromosome $CHR\"\n",
    "    tabix -p vcf \"$PHASED_VCF\"\n",
    "    SORTED_VCF=\"${phased_directory}/merged_opensnps_phased_chr${CHR}_sorted.vcf.gz\"\n",
    "    bcftools sort -Oz -o \"$SORTED_VCF\" \"$PHASED_VCF\"\n",
    "    tabix -p vcf \"$SORTED_VCF\"\n",
    "\n",
    "    # Replace original VCF with sorted version\n",
    "    mv \"$SORTED_VCF\" \"$PHASED_VCF\"\n",
    "    mv \"${SORTED_VCF}.tbi\" \"${PHASED_VCF}.tbi\"\n",
    "done"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's generate some stats on our files to manually inspect them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash -s \"$data_directory\" \"$results_directory\"\n",
    "\n",
    "data_directory=\"$1\"\n",
    "results_directory=\"$2\"\n",
    "phased_directory=\"${results_directory}/phased_samples\"\n",
    "\n",
    "# Generate stats for each chromosome\n",
    "for CHR in {1..22}; do\n",
    "    PHASED_VCF=\"${phased_directory}/merged_opensnps_phased_chr${CHR}.vcf.gz\"\n",
    "    if [ -f \"$PHASED_VCF\" ]; then\n",
    "        STATS_OUTPUT=\"${phased_directory}/merged_opensnps_phased_chr${CHR}_stats.vchk\"\n",
    "        bcftools stats -s - \"$PHASED_VCF\" > \"$STATS_OUTPUT\"\n",
    "        echo \"Stats generated for chromosome $CHR. See: $STATS_OUTPUT\"\n",
    "    else\n",
    "        echo \"Phased VCF not found for chromosome $CHR. Skipping stats generation.\"\n",
    "    fi\n",
    "done"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you are ready, run the following cell to delete the intermediary files that were created. The files in your `results/phased_samples` directory will remain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash -s \"$data_directory\" \"$results_directory\"\n",
    "\n",
    "data_directory=\"$1\"\n",
    "results_directory=\"$2\"\n",
    "\n",
    "# Final cleanup of QC files\n",
    "echo \"Cleaning up intermediate QC files\"\n",
    "for CHR in {1..22}; do\n",
    "    rm -f \"${results_directory}/merged_opensnps_qcstart_chr${CHR}.vcf\"\n",
    "    rm -f \"${results_directory}/merged_opensnps_qcstart_chr${CHR}.log\"\n",
    "    rm -f \"${results_directory}/merged_opensnps_qcfinished_chr${CHR}.vcf.gz\"\n",
    "    rm -f \"${results_directory}/merged_opensnps_qcfinished_chr${CHR}.log\"\n",
    "    rm -f \"${results_directory}/merged_opensnps_qcfinished_chr${CHR}.vcf.gz.csi\"\n",
    "done\n",
    "\n",
    "rm -f \"${results_directory}/merged_opensnps_autosomes_step1.log\"\n",
    "rm -f \"${results_directory}/merged_opensnps_autosomes_step1.pgen\"\n",
    "rm -f \"${results_directory}/merged_opensnps_autosomes_step1.psam\"\n",
    "rm -f \"${results_directory}/merged_opensnps_autosomes_step1.pvar\"\n",
    "rm -f \"${results_directory}/merged_opensnps_autosomes_step2.log\"\n",
    "rm -f \"${results_directory}/merged_opensnps_autosomes_step2.pgen\"\n",
    "rm -f \"${results_directory}/merged_opensnps_autosomes_step2.psam\"\n",
    "rm -f \"${results_directory}/merged_opensnps_autosomes_step2.pvar\"    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
