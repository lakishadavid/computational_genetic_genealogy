{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "import ipywidgets as widgets\n",
    "from lineage import Lineage\n",
    "from tqdm import tqdm\n",
    "import logging\n",
    "\n",
    "from collections import Counter\n",
    "from pathlib import Path\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "from IPython.display import display, HTML, Javascript"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_comp_gen_dir():\n",
    "    \"\"\"Find the computational_genetic_genealogy directory by searching up from current directory.\"\"\"\n",
    "    current = Path.cwd()\n",
    "    \n",
    "    # Search up through parent directories\n",
    "    while current != current.parent:\n",
    "        # Check if target directory exists in current path\n",
    "        target = current / 'computational_genetic_genealogy'\n",
    "        if target.is_dir():\n",
    "            return target\n",
    "        # Move up one directory\n",
    "        current = current.parent\n",
    "    \n",
    "    raise FileNotFoundError(\"Could not find computational_genetic_genealogy directory\")\n",
    "\n",
    "def load_env_file():\n",
    "    \"\"\"Find and load the .env file from the computational_genetic_genealogy directory.\"\"\"\n",
    "    try:\n",
    "        # Find the computational_genetic_genealogy directory\n",
    "        comp_gen_dir = find_comp_gen_dir()\n",
    "        \n",
    "        # Look for .env file\n",
    "        env_path = comp_gen_dir / '.env'\n",
    "        if not env_path.exists():\n",
    "            print(f\"Warning: No .env file found in {comp_gen_dir}\")\n",
    "            return None\n",
    "        \n",
    "        # Load the .env file\n",
    "        load_dotenv(env_path, override=True)\n",
    "        print(f\"Loaded environment variables from: {env_path}\")\n",
    "        return env_path\n",
    "        \n",
    "    except FileNotFoundError as e:\n",
    "        print(f\"Error: {e}\")\n",
    "        return None\n",
    "\n",
    "# Use the function\n",
    "env_path = load_env_file()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "working_directory = os.getenv('PROJECT_WORKING_DIR', default=None)\n",
    "data_directory = os.getenv('PROJECT_DATA_DIR', default=None)\n",
    "references_directory = os.getenv('PROJECT_REFERENCES_DIR', default=None)\n",
    "results_directory = os.getenv('PROJECT_RESULTS_DIR', default=None)\n",
    "utils_directory = os.getenv('PROJECT_UTILS_DIR', default=None)\n",
    "\n",
    "print(f\"Working Directory: {working_directory}\")\n",
    "print(f\"Data Directory: {data_directory}\")\n",
    "print(f\"References Directory: {references_directory}\")\n",
    "print(f\"Results Directory: {results_directory}\")\n",
    "print(f\"Utils Directory: {utils_directory}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def configure_logging(log_filename, log_file_debug_level=\"INFO\", console_debug_level=\"INFO\"):\n",
    "    \"\"\"\n",
    "    Configure logging for both file and console handlers.\n",
    "\n",
    "    Args:\n",
    "        log_filename (str): Path to the log file where logs will be written.\n",
    "        log_file_debug_level (str): Logging level for the file handler.\n",
    "        console_debug_level (str): Logging level for the console handler.\n",
    "    \"\"\"\n",
    "    # Create a root logger\n",
    "    logger = logging.getLogger()\n",
    "    logger.setLevel(logging.DEBUG)  # Capture all messages at the root level\n",
    "\n",
    "    # Convert level names to numeric levels\n",
    "    file_level = getattr(logging, log_file_debug_level.upper(), logging.INFO)\n",
    "    console_level = getattr(logging, console_debug_level.upper(), logging.INFO)\n",
    "\n",
    "    # File handler: Logs messages at file_level and above to the file\n",
    "    file_handler = logging.FileHandler(log_filename)\n",
    "    file_handler.setLevel(file_level)\n",
    "    file_formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')\n",
    "    file_handler.setFormatter(file_formatter)\n",
    "\n",
    "    # Console handler: Logs messages at console_level and above to the console\n",
    "    console_handler = logging.StreamHandler(sys.stdout)\n",
    "    console_handler.setLevel(console_level)\n",
    "    console_formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')\n",
    "    console_handler.setFormatter(console_formatter)\n",
    "\n",
    "    # Add handlers to the root logger\n",
    "    logger.addHandler(file_handler)\n",
    "    logger.addHandler(console_handler)\n",
    "    \n",
    "def clear_logger():\n",
    "    \"\"\"Remove all handlers from the root logger.\"\"\"\n",
    "    logger = logging.getLogger()\n",
    "    for handler in logger.handlers[:]:\n",
    "        logger.removeHandler(handler)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the `log_file_debug_level` and `console_debug_level` are set to `INFO` in the follwing cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up logging for testing\n",
    "log_filename_test = os.path.join(results_directory, \"test_log.txt\")\n",
    "print(f\"The test log file is located at {log_filename_test}.\")\n",
    "\n",
    "# Ensure the results_directory exists\n",
    "if not os.path.exists(results_directory):\n",
    "    os.makedirs(results_directory)\n",
    "\n",
    "# Check if the file exists; if not, create it\n",
    "if not os.path.exists(log_filename_test):\n",
    "    with open(log_filename_test, 'w') as file:\n",
    "        pass  # The file is now created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test case 1: INFO level for both file and console\n",
    "\n",
    "clear_logger() # Clear the logger before reconfiguring it\n",
    "configure_logging(log_filename_test, log_file_debug_level=\"INFO\", console_debug_level=\"INFO\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is how to set messages within your script.\n",
    "# See https://docs.python.org/3/library/logging.html for more information.\n",
    "logging.debug(\"DEBUG message: This should NOT appear when set to INFO level.\")\n",
    "logging.info(\"INFO message: This should appear in both the log file and console.\")\n",
    "logging.warning(\"WARNING message: This should appear in both the log file and console.\")\n",
    "\n",
    "# To verify:\n",
    "# 1. Check the console output. Only INFO and WARNING messages should be printed.\n",
    "# 2. Open 'test_log.txt' and verify that only INFO and WARNING messages are logged."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the `log_file_debug_level` and `console_debug_level` are set to `DEBUG` in the follwing cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test case 2: DEBUG level for both file and console\n",
    "\n",
    "clear_logger() # Clear the logger before reconfiguring it\n",
    "configure_logging(log_filename_test, log_file_debug_level=\"DEBUG\", console_debug_level=\"DEBUG\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These are the exact same log messages as before\n",
    "\n",
    "logging.debug(\"DEBUG message: This should NOT appear when set to INFO level.\")\n",
    "logging.info(\"INFO message: This should appear in both the log file and console.\")\n",
    "logging.warning(\"WARNING message: This should appear in both the log file and console.\")\n",
    "\n",
    "# Verify that now all three message types are logged in both the console and the log file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete the test log file if it exists\n",
    "if os.path.exists(log_filename_test):\n",
    "    os.remove(log_filename_test)\n",
    "    print(f\"{log_filename_test} has been deleted.\")\n",
    "else:\n",
    "    print(f\"{log_filename_test} does not exist.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have tested our logger, let's set our real log file for this lab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_filename = os.path.join(results_directory, \"lab3_log.txt\")\n",
    "print(f\"The Lab 3 log file is located at {log_filename}.\")\n",
    "\n",
    "# Ensure the results_directory exists\n",
    "if not os.path.exists(results_directory):\n",
    "    os.makedirs(results_directory)\n",
    "\n",
    "# Check if the file exists; if not, create it\n",
    "if not os.path.exists(log_filename):\n",
    "    with open(log_filename, 'w') as file:\n",
    "        pass  # The file is now created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clear_logger() # Clear the logger before reconfiguring it\n",
    "configure_logging(log_filename, log_file_debug_level=\"INFO\", console_debug_level=\"INFO\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you check your new lab log file, nothing will be there at the moment. You have only created the log file and reconfigured your logger. After you run cells with the `logging` command, you will see new logs in your file.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Parsing Genotype Files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parsing Genotype Files\n",
    "\n",
    "The following code block is responsible for parsing and processing raw genotype files located within a specified target subdirectory. Its primary objectives are to standardize the genotype data, ensure alignment to the GRCh38 (Build 38) reference genome, determine biological sex based on SNP data, and convert the processed information into TSV files for subsequent VCF conversion.\n",
    "\n",
    "## Key Features\n",
    "\n",
    "- **Directory Setup**\n",
    "  - Creates a subdirectory (`parsed_tsv_files`) within the target directory to store processed TSV files.\n",
    "  \n",
    "- **Lineage Object Initialization**\n",
    "  - Instantiates a `Lineage` object using the output directory, reference resources, and parallel processing options (10 processes) for efficient data handling.\n",
    "\n",
    "- **File Iteration and User Identification**\n",
    "  - Iterates over all files in the target subdirectory, filtering for files that start with `\"user\"`.\n",
    "  - Aggregates file paths by user identifier to manage cases where a single user may have multiple genotype files.\n",
    "\n",
    "- **Profile Creation and Data Standardization**\n",
    "  - Creates an individual profile for each user using the `Lineage` object.\n",
    "  - Validates the profile and checks whether it is mapped to Build 38. If not, attempts to remap and logs the outcome.\n",
    "  \n",
    "- **Sex Determination**\n",
    "  - Determines the sex of each individual based on specific thresholds for heterozygous SNPs on the X chromosome and the presence of Y chromosome SNPs.\n",
    "  - Logs successful sex determinations as well as any failures for further review.\n",
    "\n",
    "- **Progress Tracking and Logging**\n",
    "  - Utilizes `tqdm` to display a progress bar during the file processing.\n",
    "  - Maintains comprehensive logging, including error handling for remapping and sex determination issues.\n",
    "  \n",
    "- **Output Generation**\n",
    "  - Writes consolidated logs to files within the target directory (e.g., general errors, determined sex, and failed sex determinations).\n",
    "  - Returns the directory containing the parsed TSV files, which are used in later steps of the pipeline.\n",
    "  \n",
    "This module is critical for ensuring that raw genotype data is consistently processed and standardized, laying a robust foundation for accurate VCF conversion and further genetic analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check the data\n",
    "\n",
    "This lab assumes that you have at least one approapriate data file in your data directory. It also assumes that the file(s) are from OpenSNP which has a naming convention of starting each file with \"user\" and a user ID. If this is not your data structure, you will need to adjust the code for your situation.\n",
    "\n",
    "Let's check our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_data_files(raw_dna_profiles):\n",
    "    filenames = os.listdir(raw_dna_profiles)\n",
    "    # Filter files that follow the naming convention (e.g., start with \"user\")\n",
    "    data_files = [f for f in filenames if f.startswith(\"user\")]\n",
    "    file_count = len(data_files)\n",
    "    \n",
    "    if file_count == 0:\n",
    "        logging.error(f\"No data files found in {raw_dna_profiles} that follow the naming convention.\")\n",
    "    else:\n",
    "        logging.info(f\"Found {file_count} data file(s) in {raw_dna_profiles}.\")\n",
    "    \n",
    "    return data_files, file_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for data files\n",
    "class_data_directory = os.path.join(data_directory, \"class_data\")\n",
    "raw_dna_profiles = os.path.join(class_data_directory, \"raw_dna_profiles\")\n",
    "data_files, count = check_data_files(raw_dna_profiles)\n",
    "\n",
    "# If this code snippet was part of a full script, you could insert checks such as this\n",
    "# to gracefully exit the script when needed. \n",
    "if count == 0:\n",
    "    raise ValueError(\"No valid data files found. Aborting parsing.\")\n",
    "else:\n",
    "    print(f\"Found {count} data files in {raw_dna_profiles}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_user_file_paths(raw_dna_profiles):\n",
    "    user_files = {}\n",
    "    filenames = os.listdir(raw_dna_profiles)\n",
    "    \n",
    "    for filename in filenames:\n",
    "        # Only process files that follow the expected naming convention\n",
    "        if not filename.startswith(\"user\"):\n",
    "            continue\n",
    "        \n",
    "        file_path = os.path.join(raw_dna_profiles, filename)\n",
    "        logging.info(f\"Processing file {filename} at {file_path}...\")\n",
    "        \n",
    "        # Extract user identifier (assuming it is the part before the first underscore)\n",
    "        user_id = filename.split('_')[0]\n",
    "        logging.info(f\"Extracted User ID: {user_id}\")\n",
    "        \n",
    "        # Append file_path to the list corresponding to user_id\n",
    "        if user_id not in user_files:\n",
    "            user_files[user_id] = [file_path]\n",
    "        else:\n",
    "            user_files[user_id].append(file_path)\n",
    "    \n",
    "    return user_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract user IDs and their corresponding file paths\n",
    "user_files = get_user_file_paths(raw_dna_profiles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output for the previous cell should have given you the INFO as it ran the code. With a small number of files, this output menthod is okay. Imagine if it was hundreds or thousands of files. Think about how you might change the logger levels in `get_user_file_paths()`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The previous function gave us a dictionary in the format of `key: value`, which, in this case, is `userID: filepath`. Let's take a look at it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_genotype_files(raw_dna_profiles, references_directory, user_files):\n",
    "    \"\"\"\n",
    "    Parses all genotype files, ensures Build 38, determines sex, and converts to VCF.\n",
    "    \"\"\"\n",
    "    logging.info(f\"Parsing and processing genotype files in {raw_dna_profiles}...\")\n",
    "\n",
    "    # Create a directory to store the processed TSV files\n",
    "    tsv_dir = os.path.join(class_data_directory, \"parsed_tsv_files\")\n",
    "    os.makedirs(tsv_dir, exist_ok=True)\n",
    "\n",
    "    # Initialize the Lineage object with parallel processing options\n",
    "    lineages = Lineage(\n",
    "        output_dir=tsv_dir, \n",
    "        resources_dir=references_directory, \n",
    "        parallelize=True, \n",
    "        processes=10\n",
    "    )\n",
    "\n",
    "    # Initialize logs and tracking lists\n",
    "    failed_sample = []\n",
    "    failed_files_remapping = []  # Track files that fail processing or remapping\n",
    "    determined_sex_entries = []  # Track successful sex determinations\n",
    "    failed_sex_entries = []      # Track failed sex determinations\n",
    "\n",
    "    # Define file paths for logging and results\n",
    "    log_file_path = os.path.join(class_data_directory, \"parse_genotype_files.log\")\n",
    "    determined_sex_file_path = os.path.join(class_data_directory, \"determined_sex.txt\")\n",
    "    failed_sex_file_path = os.path.join(class_data_directory, \"failed_sex.txt\")\n",
    "\n",
    "    total_users = len(user_files)\n",
    "    logging.info(f\"Found {total_users} user(s) to process.\")\n",
    "\n",
    "    # Process each user based on the extracted file paths\n",
    "    with tqdm(total=total_users, desc=\"Processing users\", file=sys.stdout) as pbar:\n",
    "        for user_id, file_paths in user_files.items():\n",
    "            logging.info(\"\\n\")\n",
    "            logging.info(f\"Processing user {user_id} with file(s): {file_paths}\")\n",
    "            \n",
    "            # Attempt to create an individual profile for the user\n",
    "            try:\n",
    "                profile = lineages.create_individual(user_id, file_paths)\n",
    "            except Exception as e:\n",
    "                logging.error(f\"Failed to process files for user {user_id}: {e}\")\n",
    "                failed_files_remapping.append(user_id)\n",
    "                pbar.update(1)\n",
    "                continue\n",
    "            \n",
    "            if profile.count == 0:\n",
    "                failed_sample.append(user_id)\n",
    "                logging.error(f\"Failed to process files for user {user_id}\")\n",
    "                continue\n",
    "\n",
    "            # Ensure Build 38\n",
    "            try:\n",
    "                if not profile.build_detected or profile.build != 38:\n",
    "                    logging.info(f\"{user_id}: Current build is {profile.build}. Attempting to remap to Build 38...\")\n",
    "                    # chromosomes_remapped, chromosomes_not_remapped = profile.remap(38)\n",
    "                    profile.remap(38)\n",
    "\n",
    "                    # if chromosomes_not_remapped:\n",
    "                    #     logging.warning(f\"{user_id}: Some chromosomes could not be remapped: {chromosomes_not_remapped}\")\n",
    "\n",
    "                    if profile.build != 38:\n",
    "                        logging.error(f\"{user_id}: Remapping failed. Still not in Build 38.\")\n",
    "                        failed_files_remapping.append(user_id)\n",
    "                        continue  # Skip further processing for this file\n",
    "                    else:\n",
    "                        logging.info(f\"{user_id}: Successfully remapped to Build 38.\")\n",
    "                else:\n",
    "                    logging.info(f\"{user_id}: Already in Build 38.\")\n",
    "            except Exception as e:\n",
    "                logging.error(f\"{user_id}: Error during remapping to Build 38: {e}\")\n",
    "                failed_files_remapping.append(user_id)\n",
    "                logging.info(\"\\n\")\n",
    "                continue   # Skip to the next iteration of the loop\n",
    "\n",
    "            logging.debug(f\"Saving profile {user_id}\")\n",
    "            profile.save(user_id + \".tsv\")\n",
    "\n",
    "            # Determine sex\n",
    "            try:\n",
    "                sex = profile.determine_sex(\n",
    "                    heterozygous_x_snps_threshold=0.03,\n",
    "                    y_snps_not_null_threshold=0.3,\n",
    "                    chrom='X'\n",
    "                )\n",
    "                logging.info(sex)\n",
    "                if sex:\n",
    "                    determined_sex_entries.append(f\"{user_id}\\t{sex}\")\n",
    "                    logging.info(f\"Determined sex for {user_id}: {sex}\")\n",
    "                else:\n",
    "                    failed_sex_entries.append(f\"{user_id}\\tLow Confidence\")\n",
    "                    logging.warning(f\"Failed to determine sex for {user_id}: Low Confidence\")\n",
    "            except Exception as e:\n",
    "                failed_sex_entries.append(f\"{user_id}\\tError: {e}\")\n",
    "                logging.error(f\"Error determining sex for {user_id}: {e}\")\n",
    "\n",
    "            pbar.update()\n",
    "\n",
    "    # Write consolidated logs\n",
    "    with open(determined_sex_file_path, \"w\") as log_file:\n",
    "        log_file.write(\"\\n\".join(determined_sex_entries) + \"\\n\\n\")\n",
    "\n",
    "    with open(failed_sex_file_path, \"w\") as log_file:\n",
    "        log_file.write(\"\\n\".join(failed_sex_entries) + \"\\n\")\n",
    "\n",
    "    with open(log_file_path, \"w\") as log_file:\n",
    "        log_file.write(\"\\n\".join(failed_sample) + \"\\n\")\n",
    "\n",
    "    return tsv_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: When using the lineage package, it will download a file to help convert files to build 38. For example, you will see `Downloading ../references/GRCh37_GRCh38.tar.gz` in the log outputs used to convert the files from build 37 to build 38. If you go to your references directory, you'll see this file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parse_genotype_files(class_data_directory, references_directory, user_files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What happened?\n",
    "\n",
    "The `parse_genotype_files()` function, created a subdirectory in `data/class_data` called `parsed_tsv_files`. In the `parsed_tsv_files` subdirectory, you will see a new genotype profile for each user. Take a look and see that the files are there. Look at the contents of at least one of the files. You're looking at a person's genotype data!\n",
    "\n",
    "The `parse_genotype_files()` function also created three files in the `data/class_data` directory based on the following code snippet in the function.\n",
    "\n",
    "```\n",
    "    log_file_path = os.path.join(class_data_directory, \"parse_genotype_files.log\")\n",
    "    determined_sex_file_path = os.path.join(class_data_directory, \"determined_sex.txt\")\n",
    "    failed_sex_file_path = os.path.join(class_data_directory, \"failed_sex.txt\")\n",
    "```\n",
    "\n",
    "Take a look at the contents of these files (some may be empty). It's important to understand the structure of your data for proper computations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What Makes a FASTA File the \"Reference\" Genome for Humans?\n",
    "\n",
    "A **FASTA file** is a widely used text-based format for representing nucleotide (DNA/RNA) or protein sequences. It is structured to contain sequence data along with an identifying **header**.\n",
    "\n",
    "A **reference genome** is a **high-quality, curated DNA sequence** that serves as a **standard** for comparing and analyzing other genomes. The **FASTA file** used as the **human reference genome** contains the **consensus sequence of human DNA**, which researchers use as a **baseline** for mapping and identifying genetic variations.\n",
    "\n",
    "---\n",
    "\n",
    "## 1️⃣ What Defines a Reference Genome?\n",
    "A **reference genome** is:\n",
    "✅ **Assembled from multiple human samples** → It does not represent a single individual’s genome but an **aggregate \"best guess\"** of the human genome.  \n",
    "✅ **Organized by chromosomes** → The sequence is **divided into chromosomes** (chr1, chr2, ..., chrX, chrY, chrMT for mitochondria).  \n",
    "✅ **Continuously updated** → It is revised over time as sequencing technology improves.  \n",
    "✅ **Labeled with precise coordinates** → Every base pair position is assigned a fixed **genomic coordinate** (e.g., `chr1:1000000`).  \n",
    "\n",
    "---\n",
    "\n",
    "## 2️⃣ Why Is It Called a \"Reference\" Genome?\n",
    "- It provides a **consistent framework** for genetic studies.\n",
    "- Variations (SNPs, insertions, deletions, structural variants) are identified **relative** to this reference.\n",
    "- It is **not representative of all human diversity**, but it serves as a standardized **comparison point**.\n",
    "\n",
    "---\n",
    "\n",
    "## 3️⃣ Sources of the Human Reference Genome\n",
    "The most widely used human reference genomes are:\n",
    "\n",
    "| **Version** | **Source** | **Features** |\n",
    "|-------------|-----------|--------------|\n",
    "| **GRCh38/hg38** | Genome Reference Consortium (GRC) | The most up-to-date, widely used reference genome. |\n",
    "| **GRCh37/hg19** | UCSC Genome Browser, Ensembl | Older but still used for compatibility with legacy datasets. |\n",
    "| **T2T-CHM13** | Telomere-to-Telomere Consortium | A complete reference with full centromeres and telomeres. |\n",
    "\n",
    "---\n",
    "\n",
    "## 4️⃣ How Is a Reference Genome Stored in a FASTA File?\n",
    "A **reference genome FASTA file** contains:\n",
    "1. **Chromosome names** (headers starting with `>chrN`).\n",
    "2. **DNA sequences** (A, T, G, C, and N for unknown regions).\n",
    "\n",
    "### Example (GRCh38 reference FASTA snippet):\n",
    "```\n",
    "chr1 NNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNN AGCTAGCTAGCTAGCTAGCTAGCTAGCTAGCTAGCTAGCTAGCTAGCTAG TTCGATCGATCGATCGATCGATCGATCGATCGATCGATCGATCGATCGAT chr2 AGCTAGCTAGCTAGCTAGCTAGCTAGCTAGCTAGCTAGCTAGCTAGCTAG NNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNN\n",
    "```\n",
    "\n",
    "🔹 **Why the \"N\" Bases?**  \n",
    "- `N` means \"unknown base\" due to **low sequencing coverage** or **complex repetitive regions**.\n",
    "\n",
    "---\n",
    "\n",
    "## 5️⃣ How the Reference Genome Is Used\n",
    "🔬 **Mapping Reads:** Aligning sequencing data (FASTQ) to the reference to identify variations.  \n",
    "🧬 **Variant Calling:** Identifying SNPs, insertions, deletions, and structural variants.  \n",
    "📍 **Genome Annotation:** Identifying genes, exons, and regulatory elements.  \n",
    "🧪 **Disease Research:** Comparing patient genomes to the reference to find disease-associated mutations.\n",
    "\n",
    "### Example command to **align sequencing data to a reference genome**:\n",
    "```bash\n",
    "bwa mem Homo_sapiens.GRCh38.dna.allchromosomes.fa sample_reads.fastq > aligned.sam\n",
    "```\n",
    "\n",
    "### **Key Features of FASTA Files**\n",
    "- **Simple format:** Readable by humans and bioinformatics tools.\n",
    "- **Supports large datasets:** Used for entire genomes and protein databases.\n",
    "- **Compatible with major bioinformatics tools:** Used in `samtools`, `bcftools`, `BLAST`, and sequence alignment programs.\n",
    "\n",
    "### **Common Uses of FASTA Files**\n",
    "✅ **Reference Genomes:** FASTA files are used as reference sequences in **genome alignment** and **variant calling**.  \n",
    "✅ **BLAST Searches:** Querying DNA or protein databases for sequence similarity.  \n",
    "✅ **Multiple Sequence Alignments:** Used in phylogenetics and evolutionary analysis.  \n",
    "✅ **Genome Annotation:** Identifying genes and functional elements in DNA sequences.\n",
    "\n",
    "### **How to Work with FASTA Files**\n",
    "- **View a FASTA file**:\n",
    "    ```\n",
    "    head -n 20 genome.fa\n",
    "    ```\n",
    "- **Extract a specific chromosome**:\n",
    "    ```\n",
    "    samtools faidx genome.fa chr1:100000-200000\n",
    "    ```\n",
    "- **Search for a sequence within a FASTA file**:\n",
    "    ```\n",
    "    grep -A 2 \"AGCTAGCTAGCT\" genome.fa\n",
    "    ```\n",
    "FASTA files are essential in bioinformatics for storing and analyzing genetic data. 🚀"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download the FASTA file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash -s \"$references_directory\"\n",
    "\n",
    "references_directory=\"$1\"\n",
    "\n",
    "mkdir -p \"${references_directory}/fasta/GRCh38/\"\n",
    "\n",
    "# Source Ensembl\n",
    "\n",
    "wget --continue --retry-connrefused --timeout=60 --waitretry=60 --tries=3 \\\n",
    "    ftp://ftp.ensembl.org/pub/release-110/fasta/homo_sapiens/dna/Homo_sapiens.GRCh38.dna.primary_assembly.fa.gz \\\n",
    "    --output-document=${references_directory}/fasta/GRCh38/Homo_sapiens.GRCh38.dna.primary_assembly.fa.gz\n",
    "\n",
    "gunzip ${references_directory}/fasta/GRCh38/Homo_sapiens.GRCh38.dna.primary_assembly.fa.gz\n",
    "\n",
    "samtools faidx ${references_directory}/fasta/GRCh38/Homo_sapiens.GRCh38.dna.primary_assembly.fa\n",
    "\n",
    "# # FASTA file sources\n",
    "# # following the suggestion from Heng Li (with chr prefix)\n",
    "# wget --continue --retry-connrefused --timeout=60 --waitretry=60 --tries=3 \\\n",
    "#     ftp://ftp.ncbi.nlm.nih.gov/genomes/all/GCA/000/001/405/GCA_000001405.15_GRCh38/seqs_for_alignment_pipelines.ucsc_ids/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.gz \\\n",
    "#     --output-document=${references_directory}/fasta/GRCh38/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.gz\n",
    "# # UCSC (with chr prefix)\n",
    "# wget -O ${references_directory}/fasta/GRCh38/GRCh38.fa.gz \\\n",
    "#     ftp://hgdownload.soe.ucsc.edu/goldenPath/hg38/bigZips/hg38.fa.gz\n",
    "# # Ensembl (without chr prefix)\n",
    "# wget -O ${references_directory}/fasta/GRCh38/Homo_sapiens.GRCh38.dna.primary_assembly.fa.gz \\\n",
    "#     ftp://ftp.ensembl.org/pub/release-110/fasta/homo_sapiens/dna/Homo_sapiens.GRCh38.dna.primary_assembly.fa.gz\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Converting TSV to VCF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash -s \"$references_directory\" \"$data_directory\"\n",
    "\n",
    "references_directory=\"$1\"\n",
    "data_directory=\"$2\"\n",
    "TSV_DIR=\"${data_directory}/class_data/parsed_tsv_files\" # Directory containing the TSV files generated from the parsing step\n",
    "VCF_DIR=\"${data_directory}/class_data/converted_vcf_files\"  # Directory where converted VCF files will be stored\n",
    "REFERENCE_FASTA=\"${references_directory}/fasta/GRCh38/Homo_sapiens.GRCh38.dna.primary_assembly.fa\"\n",
    "\n",
    "# Create the output directory if it doesn't exist\n",
    "mkdir -p \"${VCF_DIR}\"\n",
    "\n",
    "# Loop through each TSV file in the TSV_DIR\n",
    "for TSV_FILE in \"${TSV_DIR}\"/*.tsv; do\n",
    "    # Extract the base name (user_id) from the TSV file name (assumes filename format: userID.tsv)\n",
    "    USER_ID=$(basename \"${TSV_FILE}\" .tsv)\n",
    "    \n",
    "    # Define the output VCF filename based on the user_id\n",
    "    OUTPUT_VCF=\"${VCF_DIR}/${USER_ID}.vcf.gz\"\n",
    "    \n",
    "    echo \"Converting ${TSV_FILE} for user ${USER_ID}...\"\n",
    "    \n",
    "    NUM_THREADS=$(nproc)  # Get the number of available CPU threads\n",
    "\n",
    "    echo \"Using ${NUM_THREADS} threads for bcftools conversion.\"\n",
    "    \n",
    "    # Run bcftools to convert the TSV file to a compressed VCF file\n",
    "    # Explanation of options:\n",
    "    # --haploid2diploid: Convert haploid genotypes to diploid.\n",
    "    # --tsv2vcf: Specify conversion from TSV to VCF.\n",
    "    # --columns: Map the TSV columns to VCF fields (ID, CHROM, POS, AA).\n",
    "    # --fasta-ref: Provide the reference genome.\n",
    "    # --samples: Name the sample using the user ID.\n",
    "    # --threads: Use 10 threads for faster processing.\n",
    "    # --output-type: Output as a compressed VCF (bgzip).\n",
    "    # --output: Specify the output VCF file path.\n",
    "    bcftools convert \\\n",
    "        --haploid2diploid \\\n",
    "        --tsv2vcf \"${TSV_FILE}\" \\\n",
    "        --columns ID,CHROM,POS,AA \\\n",
    "        --fasta-ref \"${REFERENCE_FASTA}\" \\\n",
    "        --samples \"${USER_ID}\" \\\n",
    "        --threads \"${NUM_THREADS}\" \\\n",
    "        --output-type z \\\n",
    "        --output \"${OUTPUT_VCF}\"\n",
    "\n",
    "        \n",
    "    # Check if bcftools conversion was successful\n",
    "    if [ $? -ne 0 ]; then\n",
    "        echo \"Error: VCF conversion failed for ${USER_ID}\" >&2\n",
    "        continue  # Move to the next TSV file if conversion fails\n",
    "    fi\n",
    "    \n",
    "    # Index the newly created VCF file using tabix\n",
    "    tabix -p vcf \"${OUTPUT_VCF}\"\n",
    "    if [ $? -ne 0 ]; then\n",
    "        echo \"Error: VCF indexing failed for ${USER_ID}\" >&2\n",
    "        continue  # Move to the next file if indexing fails\n",
    "    fi\n",
    "\n",
    "    echo \"Conversion and indexing successful for ${USER_ID}\"\n",
    "done\n",
    "\n",
    "echo \"VCF conversion process completed for all TSV files.\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Understanding VCF Conversion Summary Metrics**\n",
    "\n",
    "During the conversion of genotype TSV files to VCF format, several statistics are generated for each file. These statistics provide insight into the **quality**, **completeness**, and **content** of the converted data.\n",
    "\n",
    "---\n",
    "\n",
    "## **1. Rows Total**\n",
    "- This represents the **total number of rows** (or SNP records) present in the input TSV file.\n",
    "- Each row corresponds to a **single variant (SNP)** for the individual.\n",
    "\n",
    "**Example Output:**\n",
    "> Rows total: **649855**\n",
    "\n",
    "---\n",
    "\n",
    "## **2. Rows Skipped**\n",
    "- This indicates the number of rows in the TSV file that were **skipped** during conversion.\n",
    "- Possible reasons for skipping rows include:\n",
    "  - **Missing data**\n",
    "  - **Chromosome mismatch**\n",
    "  - **Invalid genotype format**\n",
    "  - **Sites that do not map correctly to the reference genome**\n",
    "\n",
    "**Example Output:**\n",
    "> Rows skipped: **8827**\n",
    "\n",
    "🔹 **Interpretation:**  \n",
    "If a large number of rows are skipped, it may indicate **data formatting issues** or an **incorrect reference genome**.\n",
    "\n",
    "---\n",
    "\n",
    "## **3. Sites Written**\n",
    "- This is the number of **variant sites successfully written** to the VCF file.\n",
    "- It is calculated as:\n",
    "\n",
    "  **Sites written = Rows total - Rows skipped**\n",
    "\n",
    "**Example Output:**\n",
    "> Sites written: **668568**\n",
    "\n",
    "🔹 **Interpretation:**  \n",
    "A high value here means that **most SNPs were successfully converted** into the VCF file.\n",
    "\n",
    "---\n",
    "\n",
    "## **4. Missing GTs (Missing Genotypes)**\n",
    "- Represents the number of **missing genotype calls** (denoted as `--` in the TSV file).\n",
    "- This happens when:\n",
    "  - The genotyping process **fails to detect an allele**.\n",
    "  - There is **low sequencing coverage**, meaning this position wasn't confidently called.\n",
    "  - The reference panel used for imputation did not include a likely genotype.\n",
    "\n",
    "**Example Output:**\n",
    "> Missing GTs: **331**\n",
    "\n",
    "🔹 **Interpretation:**  \n",
    "- A high **Missing GTs** count suggests **poor-quality SNP calls** or potential **data loss**.\n",
    "\n",
    "---\n",
    "\n",
    "## **5. Hom RR (Homozygous Reference)**\n",
    "- This is the count of sites where the individual is **homozygous for the reference allele**.\n",
    "- Example: If the reference genome has **A** at a position, and the individual's genotype is **AA**, it counts as **Hom RR**.\n",
    "\n",
    "**Example Output:**\n",
    "> Hom RR: **350352**\n",
    "\n",
    "🔹 **Interpretation:**  \n",
    "- A high **Hom RR** count means the individual has **many sites matching the reference genome**.\n",
    "\n",
    "---\n",
    "\n",
    "## **6. Het RA (Heterozygous Reference/Alternative)**\n",
    "- This is the count of sites where the individual carries **one reference allele and one alternative allele**.\n",
    "- Example: If the reference genome has **A**, and the individual's genotype is **AG**, this is a **heterozygous site**.\n",
    "\n",
    "**Example Output:**\n",
    "> Het RA: **199609**\n",
    "\n",
    "🔹 **Interpretation:**  \n",
    "- This value represents **genetic variation**—the higher it is, the **more heterozygous sites** the individual has.\n",
    "\n",
    "---\n",
    "\n",
    "## **7. Hom AA (Homozygous Alternative)**\n",
    "- This is the count of sites where the individual is **homozygous for the alternative allele**.\n",
    "- Example: If the reference genome has **A**, but the individual has **GG**, it is counted as **Hom AA**.\n",
    "\n",
    "**Example Output:**\n",
    "> Hom AA: **118818**\n",
    "\n",
    "🔹 **Interpretation:**  \n",
    "- A high **Hom AA** count suggests that the **individual carries many variations from the reference genome**.\n",
    "\n",
    "---\n",
    "\n",
    "## **8. Het AA (Heterozygous Alternative/Alternative)**\n",
    "- This is a **rare case** where an individual has **two different alternative alleles at the same site**.\n",
    "- Example: A multi-allelic site where one chromosome carries **G** and the other carries **A** → **GA**.\n",
    "\n",
    "**Example Output:**\n",
    "> Het AA: **149**\n",
    "\n",
    "🔹 **Interpretation:**  \n",
    "- This generally happens in **multi-allelic sites**, which are less common than standard biallelic SNPs.\n",
    "\n",
    "---\n",
    "\n",
    "## **Final Interpretation: What These Numbers Mean**\n",
    "These metrics help assess:\n",
    "1. **Data quality**: \n",
    "   - High **Rows Skipped** → Possible **format issues** or **reference mismatch**.\n",
    "   - High **Missing GTs** → Potential **poor-quality genotype calls**.\n",
    "  \n",
    "2. **Variant composition**:\n",
    "   - High **Hom RR** → Many SNPs match the reference genome.\n",
    "   - High **Het RA** → More **genetic diversity** in the sample.\n",
    "\n",
    "3. **Potential issues in reference genome compatibility**:\n",
    "   - If **Rows Skipped is high**, verify that the **TSV file matches the reference genome** used in `bcftools`.\n",
    "   - If the **Het RA to Hom AA ratio** is significantly skewed, consider whether the dataset contains **multi-allelic sites**.\n",
    "---\n",
    "\n",
    "## **Example Summary**\n",
    "Here’s how a typical conversion result looks:\n",
    "\n",
    "- **Rows total:** `701039` → **Total SNP sites in the TSV file**  \n",
    "- **Rows skipped:** `1` → **Only 1 site was skipped (good quality data)**  \n",
    "- **Sites written:** `701038` → **Nearly all SNPs were successfully written to VCF**  \n",
    "- **Missing GTs:** `16262` → **16,262 sites have missing genotype data**  \n",
    "- **Hom RR:** `350727` → **Half of the sites match the reference genome**  \n",
    "- **Het RA:** `202225` → **202,225 heterozygous sites (one ref, one alt allele)**  \n",
    "- **Hom AA:** `131708` → **131,708 sites where both alleles are alternative**  \n",
    "- **Het AA:** `116` → **Very rare cases where two alternative alleles are present**  \n",
    "\n",
    "## **Potential Errors and Warnings**\n",
    "- **Too many rows skipped?**  \n",
    "  - Double-check the **TSV file format** and **reference genome compatibility**.  \n",
    "  - Ensure that chromosomes are properly labeled (`1-22, X, Y`).  \n",
    "  - If using a liftover tool, verify that the conversion was **successful**.\n",
    "\n",
    "- **Too many missing genotypes (`--`)?**  \n",
    "  - This could indicate **poor-quality sequencing or genotyping**.  \n",
    "  - If filtering, consider setting a **minimum call rate threshold**.  \n",
    "  - If working with ancient DNA or degraded samples, missingness may be **expected**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Merging VCF Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash -s \"$data_directory\"\n",
    "\n",
    "data_directory=\"$1\"\n",
    "VCF_DIR=\"${data_directory}/class_data/converted_vcf_files\"  # Directory where converted VCF files will be stored\n",
    "MERGED_VCF=\"${data_directory}/class_data/merged_opensnps_data.vcf.gz\"  # Final merged VCF file\n",
    "\n",
    "# Collect all VCF files to merge\n",
    "vcf_files=(${VCF_DIR}/*.vcf.gz)\n",
    "\n",
    "# Check if VCF files exist\n",
    "if [ ${#vcf_files[@]} -eq 0 ]; then\n",
    "    echo \"Error: No VCF files found in ${VCF_DIR}\" >&2\n",
    "    exit 1\n",
    "fi\n",
    "\n",
    "# Merge all VCF files\n",
    "echo \"Merging ${#vcf_files[@]} VCF files...\"\n",
    "bcftools merge -O z -o \"${MERGED_VCF}\" \"${vcf_files[@]}\"\n",
    "\n",
    "# Check if merging was successful\n",
    "if [ $? -ne 0 ]; then\n",
    "    echo \"Error: VCF merging failed.\" >&2\n",
    "    exit 1\n",
    "fi\n",
    "\n",
    "# Index the merged VCF file\n",
    "echo \"Indexing the merged VCF file...\"\n",
    "bcftools index -t \"${MERGED_VCF}\"\n",
    "\n",
    "expected_num_samples=${#vcf_files[@]}\n",
    "num_samples=$(bcftools query -l \"${MERGED_VCF}\" | wc -l)\n",
    "\n",
    "if [ \"$num_samples\" -ne \"$expected_num_samples\" ]; then\n",
    "    echo \"Warning: Sample count mismatch in ${MERGED_VCF} - Expected: $expected_num_samples, Found: $num_samples\" >&2\n",
    "else\n",
    "    echo \"Merged VCF file created and Validation successful. Sample count matches: $num_samples\"\n",
    "fi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore your Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash -s \"$data_directory\"\n",
    "\n",
    "data_directory=\"$1\"\n",
    "MERGED_VCF=\"${data_directory}/class_data/merged_opensnps_data.vcf.gz\"  # Final merged VCF file\n",
    "\n",
    "echo \"Get the number of samples\"\n",
    "bcftools query -l \"${MERGED_VCF}\" | wc -l\n",
    "echo\n",
    "\n",
    "echo \"Displaying the full VCF header:\"\n",
    "bcftools view -h \"${MERGED_VCF}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash -s \"$data_directory\"\n",
    "\n",
    "data_directory=\"$1\"\n",
    "MERGED_VCF=\"${data_directory}/class_data/merged_opensnps_data.vcf.gz\"  # Final merged VCF file\n",
    "\n",
    "echo \"Get the stats\"\n",
    "bcftools stats \"${MERGED_VCF}\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
