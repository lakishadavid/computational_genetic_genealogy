{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 25: Real-World Datasets and Challenges\n",
    "\n",
    "## Overview\n",
    "\n",
    "This notebook explores the practical application of Bonsai v3 to real-world genetic datasets, examining common challenges, adaptation strategies, and validation approaches. We'll examine how to prepare and process data from different sources, adjust for population-specific considerations, and validate results in various scenarios.\n",
    "\n",
    "**Learning Objectives:**\n",
    "- Understand the types and characteristics of real-world genetic datasets\n",
    "- Learn data preparation and quality control techniques for diverse data sources\n",
    "- Explore population-specific considerations for relationship inference\n",
    "- Apply Bonsai to different case studies with real-world complexities\n",
    "- Evaluate and validate genetic relationship reconstruction results\n",
    "\n",
    "**Prerequisites:**\n",
    "- Completion of Lab 9: Pedigree Data Structures\n",
    "- Completion of Lab 12: Relationship Assessment\n",
    "- Completion of Lab 24: Complex Relationship Patterns\n",
    "\n",
    "**Estimated completion time:** 60-90 minutes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# üß¨ Google Colab Setup - Run this cell first!\n",
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import networkx as nx\n",
    "from IPython.display import display, HTML, Markdown\n",
    "\n",
    "def is_colab():\n",
    "    '''Check if running in Google Colab'''\n",
    "    try:\n",
    "        import google.colab\n",
    "        return True\n",
    "    except ImportError:\n",
    "        return False\n",
    "\n",
    "if is_colab():\n",
    "    print(\"üî¨ Setting up Google Colab environment...\")\n",
    "    \n",
    "    # Install dependencies\n",
    "    print(\"üì¶ Installing packages...\")\n",
    "    !pip install -q pysam biopython scikit-allel networkx pygraphviz seaborn plotly\n",
    "    !apt-get update -qq && apt-get install -qq samtools bcftools tabix graphviz-dev\n",
    "    \n",
    "    # Create directories\n",
    "    !mkdir -p /content/class_data /content/results\n",
    "    \n",
    "    # Download essential class data\n",
    "    print(\"üì• Downloading class data...\")\n",
    "    S3_BASE = \"https://computational-genetic-genealogy.s3.us-east-2.amazonaws.com/class_data/\"\n",
    "    data_files = [\n",
    "        \"pedigree.fam\", \"pedigree.def\", \n",
    "        \"merged_opensnps_autosomes_ped_sim.seg\",\n",
    "        \"merged_opensnps_autosomes_ped_sim-everyone.fam\",\n",
    "        \"ped_sim_run2.seg\", \"ped_sim_run2-everyone.fam\"\n",
    "    ]\n",
    "    \n",
    "    for file in data_files:\n",
    "        !wget -q -O /content/class_data/{file} {S3_BASE}{file}\n",
    "        print(f\"  ‚úÖ {file}\")\n",
    "    \n",
    "    # Define utility functions\n",
    "    def setup_environment():\n",
    "        return \"/content/class_data\", \"/content/results\"\n",
    "    \n",
    "    def save_results(dataframe, filename, description=\"results\"):\n",
    "        os.makedirs(\"/content/results\", exist_ok=True)\n",
    "        full_path = f\"/content/results/{filename}\"\n",
    "        dataframe.to_csv(full_path, index=False)\n",
    "        display(HTML(f'''\n",
    "        <div style=\"padding: 10px; background-color: #e3f2fd; border-left: 4px solid #2196f3; margin: 10px 0;\">\n",
    "            <p><strong>üíæ Results saved!</strong> To download: \n",
    "            <code>from google.colab import files; files.download('{full_path}')</code></p>\n",
    "        </div>\n",
    "        '''))\n",
    "        return full_path\n",
    "    \n",
    "    def save_plot(plt, filename, description=\"plot\"):\n",
    "        os.makedirs(\"/content/results\", exist_ok=True)\n",
    "        full_path = f\"/content/results/{filename}\"\n",
    "        plt.savefig(full_path, dpi=300, bbox_inches='tight')\n",
    "        plt.show()\n",
    "        display(HTML(f'''\n",
    "        <div style=\"padding: 10px; background-color: #e8f5e8; border-left: 4px solid #4caf50; margin: 10px 0;\">\n",
    "            <p><strong>üìä Plot saved!</strong> To download: \n",
    "            <code>from google.colab import files; files.download('{full_path}')</code></p>\n",
    "        </div>\n",
    "        '''))\n",
    "        return full_path\n",
    "    \n",
    "    print(\"‚úÖ Colab setup complete! Ready to explore genetic genealogy.\")\n",
    "    \n",
    "else:\n",
    "    print(\"üè† Local environment detected\")\n",
    "    def setup_environment():\n",
    "        return \"class_data\", \"results\"\n",
    "    def save_results(df, filename, description=\"\"):\n",
    "        os.makedirs(\"results\", exist_ok=True)\n",
    "        path = f\"results/{filename}\"\n",
    "        df.to_csv(path, index=False)\n",
    "        return path\n",
    "    def save_plot(plt, filename, description=\"\"):\n",
    "        os.makedirs(\"results\", exist_ok=True)\n",
    "        path = f\"results/{filename}\"\n",
    "        plt.savefig(path, dpi=300, bbox_inches='tight')\n",
    "        plt.show()\n",
    "        return path\n",
    "\n",
    "# Set up paths and configure visualization\n",
    "DATA_DIR, RESULTS_DIR = setup_environment()\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "sns.set_context(\"notebook\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Setup Bonsai module paths\n",
    "if not is_jupyterlite():\n",
    "    # In local environment, add the utils directory to system path\n",
    "    utils_dir = os.getenv('PROJECT_UTILS_DIR', os.path.join(os.path.dirname(DATA_DIR), 'utils'))\n",
    "    bonsaitree_dir = os.path.join(utils_dir, 'bonsaitree')\n",
    "    \n",
    "    # Add to path if it exists and isn't already there\n",
    "    if os.path.exists(bonsaitree_dir) and bonsaitree_dir not in sys.path:\n",
    "        sys.path.append(bonsaitree_dir)\n",
    "        print(f\"Added {bonsaitree_dir} to sys.path\")\n",
    "else:\n",
    "    # In JupyterLite, use a simplified approach\n",
    "    print(\"‚ö†Ô∏è Running in JupyterLite: Some Bonsai functionality may be limited.\")\n",
    "    print(\"This notebook is primarily designed for local execution where the Bonsai codebase is available.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Helper functions for exploring modules\n",
    "def display_module_classes(module_name):\n",
    "    \"\"\"Display classes and their docstrings from a module\"\"\"\n",
    "    try:\n",
    "        # Import the module\n",
    "        module = importlib.import_module(module_name)\n",
    "        \n",
    "        # Find all classes\n",
    "        classes = inspect.getmembers(module, inspect.isclass)\n",
    "        \n",
    "        # Filter classes defined in this module (not imported)\n",
    "        classes = [(name, cls) for name, cls in classes if cls.__module__ == module_name]\n",
    "        \n",
    "        if not classes:\n",
    "            print(f\"No classes found in module {module_name}\")\n",
    "            return\n",
    "            \n",
    "        # Print info for each class\n",
    "        for name, cls in classes:\n",
    "            display(Markdown(f\"### Class: {name}\"))\n",
    "            \n",
    "            # Get docstring\n",
    "            doc = inspect.getdoc(cls)\n",
    "            if doc:\n",
    "                display(Markdown(f\"**Documentation:**\\n{doc}\"))\n",
    "            else:\n",
    "                display(Markdown(\"*No documentation available*\"))\n",
    "            \n",
    "            # Get methods\n",
    "            methods = inspect.getmembers(cls, inspect.isfunction)\n",
    "            public_methods = [(method_name, method) for method_name, method in methods \n",
    "                             if not method_name.startswith('_')]\n",
    "            \n",
    "            if public_methods:\n",
    "                display(Markdown(\"**Public Methods:**\"))\n",
    "                for method_name, method in public_methods:\n",
    "                    sig = inspect.signature(method)\n",
    "                    display(Markdown(f\"- `{method_name}{sig}`\"))\n",
    "            else:\n",
    "                display(Markdown(\"*No public methods*\"))\n",
    "            \n",
    "            display(Markdown(\"---\"))\n",
    "    except ImportError as e:\n",
    "        print(f\"Error importing module {module_name}: {e}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing module {module_name}: {e}\")\n",
    "\n",
    "def display_module_functions(module_name):\n",
    "    \"\"\"Display functions and their docstrings from a module\"\"\"\n",
    "    try:\n",
    "        # Import the module\n",
    "        module = importlib.import_module(module_name)\n",
    "        \n",
    "        # Find all functions\n",
    "        functions = inspect.getmembers(module, inspect.isfunction)\n",
    "        \n",
    "        # Filter functions defined in this module (not imported)\n",
    "        functions = [(name, func) for name, func in functions if func.__module__ == module_name]\n",
    "        \n",
    "        if not functions:\n",
    "            print(f\"No functions found in module {module_name}\")\n",
    "            return\n",
    "            \n",
    "        # Filter public functions\n",
    "        public_functions = [(name, func) for name, func in functions if not name.startswith('_')]\n",
    "        \n",
    "        if not public_functions:\n",
    "            print(f\"No public functions found in module {module_name}\")\n",
    "            return\n",
    "            \n",
    "        # Print info for each function\n",
    "        for name, func in public_functions:                \n",
    "            display(Markdown(f\"### Function: {name}\"))\n",
    "            \n",
    "            # Get signature\n",
    "            sig = inspect.signature(func)\n",
    "            display(Markdown(f\"**Signature:** `{name}{sig}`\"))\n",
    "            \n",
    "            # Get docstring\n",
    "            doc = inspect.getdoc(func)\n",
    "            if doc:\n",
    "                display(Markdown(f\"**Documentation:**\\n{doc}\"))\n",
    "            else:\n",
    "                display(Markdown(\"*No documentation available*\"))\n",
    "                \n",
    "            display(Markdown(\"---\"))\n",
    "    except ImportError as e:\n",
    "        print(f\"Error importing module {module_name}: {e}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing module {module_name}: {e}\")\n",
    "\n",
    "def view_function_source(module_name, function_name):\n",
    "    \"\"\"Display the source code of a function\"\"\"\n",
    "    try:\n",
    "        # Import the module\n",
    "        module = importlib.import_module(module_name)\n",
    "        \n",
    "        # Get the function\n",
    "        func = getattr(module, function_name)\n",
    "        \n",
    "        # Get the source code\n",
    "        source = inspect.getsource(func)\n",
    "        \n",
    "        # Print the source code with syntax highlighting\n",
    "        display(Markdown(f\"### Source code for `{function_name}`\\n```python\\n{source}\\n```\"))\n",
    "    except ImportError as e:\n",
    "        print(f\"Error importing module {module_name}: {e}\")\n",
    "    except AttributeError:\n",
    "        print(f\"Function {function_name} not found in module {module_name}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing function {function_name}: {e}\")\n",
    "\n",
    "def view_class_source(module_name, class_name):\n",
    "    \"\"\"Display the source code of a class\"\"\"\n",
    "    try:\n",
    "        # Import the module\n",
    "        module = importlib.import_module(module_name)\n",
    "        \n",
    "        # Get the class\n",
    "        cls = getattr(module, class_name)\n",
    "        \n",
    "        # Get the source code\n",
    "        source = inspect.getsource(cls)\n",
    "        \n",
    "        # Print the source code with syntax highlighting\n",
    "        display(Markdown(f\"### Source code for class `{class_name}`\\n```python\\n{source}\\n```\"))\n",
    "    except ImportError as e:\n",
    "        print(f\"Error importing module {module_name}: {e}\")\n",
    "    except AttributeError:\n",
    "        print(f\"Class {class_name} not found in module {module_name}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing class {class_name}: {e}\")\n",
    "\n",
    "def explore_module(module_name):\n",
    "    \"\"\"Display a comprehensive overview of a module with classes and functions\"\"\"\n",
    "    try:\n",
    "        # Import the module\n",
    "        module = importlib.import_module(module_name)\n",
    "        \n",
    "        # Module docstring\n",
    "        doc = inspect.getdoc(module)\n",
    "        display(Markdown(f\"# Module: {module_name}\"))\n",
    "        \n",
    "        if doc:\n",
    "            display(Markdown(f\"**Module Documentation:**\\n{doc}\"))\n",
    "        else:\n",
    "            display(Markdown(\"*No module documentation available*\"))\n",
    "            \n",
    "        display(Markdown(\"---\"))\n",
    "        \n",
    "        # Display classes\n",
    "        display(Markdown(\"## Classes\"))\n",
    "        display_module_classes(module_name)\n",
    "        \n",
    "        # Display functions\n",
    "        display(Markdown(\"## Functions\"))\n",
    "        display_module_functions(module_name)\n",
    "        \n",
    "    except ImportError as e:\n",
    "        print(f\"Error importing module {module_name}: {e}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error exploring module {module_name}: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check Bonsai Installation\n",
    "\n",
    "Let's verify that the Bonsai v3 module is available for import:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "try:\n",
    "    from bonsaitree import v3\n",
    "    print(\"‚úÖ Successfully imported Bonsai v3 module\")\n",
    "    \n",
    "    # Print Bonsai version information if available\n",
    "    if hasattr(v3, \"__version__\"):\n",
    "        print(f\"Bonsai v3 version: {v3.__version__}\")\n",
    "    \n",
    "    # List key submodules\n",
    "    print(\"\\nAvailable Bonsai submodules:\")\n",
    "    for module_name in dir(v3):\n",
    "        if not module_name.startswith(\"_\") and not module_name.startswith(\"__\"):\n",
    "            print(f\"- {module_name}\")\n",
    "except ImportError as e:\n",
    "    print(f\"‚ùå Failed to import Bonsai v3 module: {e}\")\n",
    "    print(\"This lab requires access to the Bonsai v3 codebase.\")\n",
    "    print(\"Make sure you've properly set up your environment with the Bonsai repository.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Introduction\n\nTheoretical models and simulated datasets are essential for developing and testing genetic genealogy algorithms, but real-world applications introduce numerous practical challenges. In this lab, we'll explore how Bonsai v3 handles real-world genetic datasets with their inherent complexities, including data quality issues, diverse population structures, and the need for rigorous validation.\n\n**Key concepts we'll cover:**\n- Types and characteristics of real-world genetic datasets\n- Data preparation and quality control techniques\n- Population-specific considerations for accurate relationship inference\n- Case studies illustrating common real-world scenarios\n- Validation approaches for genetic relationship reconstruction\n\nThe transition from controlled simulations to real-world datasets is a critical step in applying genetic genealogy tools to practical problems. Real datasets often contain challenges like missing data, varying quality across samples, platform-specific biases, and diverse population structures that can affect IBD detection and relationship inference.\n\nThis lab builds on your knowledge of Bonsai's architecture and methods, focusing on practical applications and the adaptations required for successfully working with diverse datasets encountered in the field.",
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Part 1: Types of Real-World Genetic Datasets\n\nVarious types of genetic datasets are used in genealogical research, each with unique characteristics, strengths, and limitations. Understanding these differences is essential for properly configuring Bonsai and interpreting results.\n\n### 1.1 Direct-to-Consumer (DTC) Testing Data\n\nDTC genetic testing companies like 23andMe, AncestryDNA, MyHeritage, and FamilyTreeDNA have generated massive databases of consumer genetic data. These datasets have several important characteristics:\n\n- **Diverse SNP coverage**: Different companies use different genotyping arrays with varying numbers of SNPs (typically 500,000 to 1 million) and different SNP selections\n- **Variable data quality**: Quality can vary based on sample collection methods, processing protocols, and genotyping technology\n- **Format variations**: Each company uses proprietary file formats, requiring conversion to standard formats\n- **Privacy restrictions**: Access and sharing limitations based on user agreements and privacy policies\n- **Self-reported metadata**: User-provided information about ancestry, relationships, and demographics may be incomplete or inaccurate\n\n### 1.2 Research and Clinical Datasets\n\nAcademic and clinical research generates genetic datasets with different characteristics:\n\n- **Whole genome sequencing (WGS)**: Complete genome coverage with higher resolution than DTC tests\n- **Exome sequencing**: Focused on protein-coding regions with high depth but limited coverage for IBD detection\n- **Specialized arrays**: Custom arrays designed for specific research questions\n- **Well-documented metadata**: Often includes verified pedigrees and detailed phenotypic information\n- **Controlled access**: Usually restricted by ethics boards and data access committees\n\n### 1.3 Historical and Forensic Datasets\n\nThese datasets present unique challenges:\n\n- **Ancient DNA**: Degraded samples with high missing data rates\n- **Low coverage sequencing**: Limited genetic information requiring specialized analysis\n- **Forensic samples**: Often focused on identity markers rather than genealogical markers\n- **Mixed samples**: May contain DNA from multiple individuals\n- **Limited reference data**: May come from populations underrepresented in reference panels\n\n### 1.4 Population Genetic Datasets\n\nPublic reference datasets like the 1000 Genomes Project, HapMap, and gnomAD:\n\n- **High-quality reference data**: Well-validated genetic data across diverse populations\n- **Population structure information**: Detailed metadata on ancestral origins\n- **Complete documentation**: Published methodologies and quality metrics\n- **Open access**: Generally available for research and algorithm development\n- **Limited genealogical information**: Typically lacks detailed family structures",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "<cell_type>markdown</cell_type>## Implementation: Exploring Dataset Characteristics\n\nLet's implement some tools to explore and characterize genetic datasets that you might encounter in real-world applications. We'll create functions to analyze VCF files, which are a common format for storing genetic variation data.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# First, let's create some utility functions for reading and analyzing VCF files\nimport gzip\n\ndef parse_vcf_metadata(vcf_file, max_lines=1000):\n    \"\"\"Parse VCF metadata to extract basic information about the dataset.\n    \n    Args:\n        vcf_file (str): Path to VCF file (can be gzipped)\n        max_lines (int): Maximum number of header lines to read\n        \n    Returns:\n        dict: Dictionary containing metadata information\n    \"\"\"\n    is_gzipped = vcf_file.endswith('.gz')\n    opener = gzip.open if is_gzipped else open\n    mode = 'rt' if is_gzipped else 'r'\n    \n    metadata = {\n        'file_path': vcf_file,\n        'format_version': None,\n        'file_format': None,\n        'contigs': [],\n        'samples': [],\n        'filters': {},\n        'infos': {},\n        'formats': {},\n        'n_header_lines': 0\n    }\n    \n    with opener(vcf_file, mode) as f:\n        for i, line in enumerate(f):\n            if i >= max_lines:\n                break\n                \n            # Count header lines\n            metadata['n_header_lines'] += 1\n            \n            # Skip empty lines\n            if line.strip() == '':\n                continue\n                \n            # Extract metadata from header lines\n            if line.startswith('##'):\n                line = line.strip()\n                \n                # VCF version\n                if line.startswith('##fileformat='):\n                    metadata['file_format'] = line.split('=')[1]\n                \n                # Contig information\n                elif line.startswith('##contig='):\n                    contig_info = {}\n                    fields = line[9:].strip('<>').split(',')\n                    for field in fields:\n                        if '=' in field:\n                            key, value = field.split('=', 1)\n                            contig_info[key] = value\n                    if 'ID' in contig_info:\n                        metadata['contigs'].append(contig_info)\n                \n                # Filter information\n                elif line.startswith('##FILTER='):\n                    filter_info = {}\n                    fields = line[10:].strip('<>').split(',')\n                    for field in fields:\n                        if '=' in field:\n                            key, value = field.split('=', 1)\n                            filter_info[key] = value\n                    if 'ID' in filter_info:\n                        metadata['filters'][filter_info['ID']] = filter_info\n                \n                # INFO field information\n                elif line.startswith('##INFO='):\n                    info_field = {}\n                    fields = line[8:].strip('<>').split(',')\n                    for field in fields:\n                        if '=' in field:\n                            key, value = field.split('=', 1)\n                            info_field[key] = value\n                    if 'ID' in info_field:\n                        metadata['infos'][info_field['ID']] = info_field\n                \n                # FORMAT field information\n                elif line.startswith('##FORMAT='):\n                    format_field = {}\n                    fields = line[10:].strip('<>').split(',')\n                    for field in fields:\n                        if '=' in field:\n                            key, value = field.split('=', 1)\n                            format_field[key] = value\n                    if 'ID' in format_field:\n                        metadata['formats'][format_field['ID']] = format_field\n            \n            # Extract sample information from the header line\n            elif line.startswith('#CHROM'):\n                fields = line.strip().split('\\t')\n                if len(fields) > 9:  # VCF has samples\n                    metadata['samples'] = fields[9:]\n                break  # Found the header line, we're done with metadata\n    \n    # Calculate some summary statistics\n    metadata['n_samples'] = len(metadata['samples'])\n    metadata['n_contigs'] = len(metadata['contigs'])\n    \n    return metadata\n\ndef count_variants_in_vcf(vcf_file, max_variants=None):\n    \"\"\"Count the number of variants in a VCF file.\n    \n    Args:\n        vcf_file (str): Path to VCF file (can be gzipped)\n        max_variants (int, optional): Maximum number of variants to count\n        \n    Returns:\n        dict: Dictionary with variant counts per chromosome\n    \"\"\"\n    is_gzipped = vcf_file.endswith('.gz')\n    opener = gzip.open if is_gzipped else open\n    mode = 'rt' if is_gzipped else 'r'\n    \n    variant_counts = {'total': 0, 'by_chrom': {}}\n    variants_per_million_positions = {}\n    current_pos = 0\n    last_million_mark = 0\n    current_chrom = None\n    \n    with opener(vcf_file, mode) as f:\n        for line in f:\n            # Skip header lines\n            if line.startswith('#'):\n                continue\n            \n            fields = line.strip().split('\\t')\n            if len(fields) < 8:  # Minimum required VCF fields\n                continue\n            \n            chrom = fields[0]\n            pos = int(fields[1])\n            \n            # Initialize chromosome counter if needed\n            if chrom not in variant_counts['by_chrom']:\n                variant_counts['by_chrom'][chrom] = 0\n            \n            # Count this variant\n            variant_counts['by_chrom'][chrom] += 1\n            variant_counts['total'] += 1\n            \n            # Track variants per million positions\n            if current_chrom != chrom:\n                current_chrom = chrom\n                current_pos = pos\n                last_million_mark = 0\n                if chrom not in variants_per_million_positions:\n                    variants_per_million_positions[chrom] = []\n            \n            # New million position block\n            million_mark = pos // 1_000_000\n            if million_mark > last_million_mark:\n                for i in range(last_million_mark + 1, million_mark + 1):\n                    variants_per_million_positions[chrom].append(0)\n                last_million_mark = million_mark\n            \n            # Increment the counter for this million position block\n            if len(variants_per_million_positions[chrom]) > 0:\n                variants_per_million_positions[chrom][-1] += 1\n            \n            # Stop if we've reached the maximum number of variants\n            if max_variants and variant_counts['total'] >= max_variants:\n                break\n    \n    # Add the density information\n    variant_counts['density'] = variants_per_million_positions\n    \n    return variant_counts\n\ndef analyze_sample_genotypes(vcf_file, max_variants=1000):\n    \"\"\"Analyze genotype quality and missingness for samples in a VCF file.\n    \n    Args:\n        vcf_file (str): Path to VCF file (can be gzipped)\n        max_variants (int): Maximum number of variants to analyze\n        \n    Returns:\n        dict: Dictionary with sample quality metrics\n    \"\"\"\n    is_gzipped = vcf_file.endswith('.gz')\n    opener = gzip.open if is_gzipped else open\n    mode = 'rt' if is_gzipped else 'r'\n    \n    samples = []\n    sample_stats = {}\n    \n    with opener(vcf_file, mode) as f:\n        variant_count = 0\n        \n        for line in f:\n            if line.startswith('#CHROM'):\n                # Extract sample names\n                fields = line.strip().split('\\t')\n                if len(fields) > 9:\n                    samples = fields[9:]\n                    # Initialize statistics for each sample\n                    for sample in samples:\n                        sample_stats[sample] = {\n                            'total_genotypes': 0,\n                            'missing_genotypes': 0,\n                            'homozygous_ref': 0,\n                            'homozygous_alt': 0,\n                            'heterozygous': 0\n                        }\n                continue\n            \n            if line.startswith('#'):\n                continue\n            \n            fields = line.strip().split('\\t')\n            if len(fields) < 10:  # Need at least one sample\n                continue\n            \n            # Parse format field to find GT position\n            format_fields = fields[8].split(':')\n            try:\n                gt_index = format_fields.index('GT')\n            except ValueError:\n                # Skip variants without genotype information\n                continue\n            \n            # Analyze genotypes for each sample\n            for i, sample in enumerate(samples):\n                if i + 9 >= len(fields):\n                    # Missing sample field\n                    continue\n                \n                sample_data = fields[i + 9].split(':')\n                if gt_index >= len(sample_data):\n                    # Missing GT field\n                    sample_stats[sample]['missing_genotypes'] += 1\n                    continue\n                \n                gt = sample_data[gt_index]\n                \n                # Count genotype types\n                sample_stats[sample]['total_genotypes'] += 1\n                \n                if gt in ['.', './.', '.|.', './0', '0/.', '.|0', '0|.']:\n                    sample_stats[sample]['missing_genotypes'] += 1\n                elif gt in ['0/0', '0|0']:\n                    sample_stats[sample]['homozygous_ref'] += 1\n                elif gt in ['1/1', '1|1', '2/2', '2|2', '3/3', '3|3']:\n                    sample_stats[sample]['homozygous_alt'] += 1\n                else:\n                    sample_stats[sample]['heterozygous'] += 1\n            \n            variant_count += 1\n            if max_variants and variant_count >= max_variants:\n                break\n    \n    # Calculate summary statistics\n    for sample in sample_stats:\n        total = sample_stats[sample]['total_genotypes']\n        if total > 0:\n            sample_stats[sample]['missing_rate'] = sample_stats[sample]['missing_genotypes'] / total\n            sample_stats[sample]['heterozygosity'] = sample_stats[sample]['heterozygous'] / total\n    \n    return sample_stats\n\ndef summarize_dataset(vcf_file, max_metadata_lines=1000, max_variants=10000, max_count_variants=None):\n    \"\"\"Generate a comprehensive summary of a genetic dataset in VCF format.\n    \n    Args:\n        vcf_file (str): Path to VCF file (can be gzipped)\n        max_metadata_lines (int): Maximum number of header lines to read\n        max_variants (int): Maximum number of variants to analyze for sample quality\n        max_count_variants (int, optional): Maximum number of variants to count (None = all)\n        \n    Returns:\n        dict: Dictionary with dataset summary\n    \"\"\"\n    print(f\"Analyzing dataset: {vcf_file}\")\n    \n    # Parse metadata\n    print(\"  Parsing metadata...\")\n    metadata = parse_vcf_metadata(vcf_file, max_lines=max_metadata_lines)\n    \n    # Count variants (limited sample if specified)\n    print(\"  Counting variants...\")\n    variant_counts = count_variants_in_vcf(vcf_file, max_variants=max_count_variants)\n    \n    # Analyze sample quality\n    print(\"  Analyzing sample quality...\")\n    sample_stats = analyze_sample_genotypes(vcf_file, max_variants=max_variants)\n    \n    # Combine results\n    summary = {\n        'metadata': metadata,\n        'variant_counts': variant_counts,\n        'sample_stats': sample_stats\n    }\n    \n    # Add high-level summary\n    summary['summary'] = {\n        'file_path': vcf_file,\n        'format': metadata['file_format'],\n        'samples': metadata['n_samples'],\n        'contigs': metadata['n_contigs'],\n        'total_variants': variant_counts['total'],\n        'chromosomes': list(variant_counts['by_chrom'].keys()),\n        'avg_missing_rate': sum(s['missing_rate'] for s in sample_stats.values() if 'missing_rate' in s) / len(sample_stats) if sample_stats else None,\n        'avg_heterozygosity': sum(s['heterozygosity'] for s in sample_stats.values() if 'heterozygosity' in s) / len(sample_stats) if sample_stats else None\n    }\n    \n    print(\"Analysis complete!\")\n    return summary",
   "metadata": {},
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Visualization functions for dataset exploration\n\ndef plot_variant_distribution(variant_counts, title=\"Variant Distribution by Chromosome\"):\n    \"\"\"Plot the distribution of variants across chromosomes.\n    \n    Args:\n        variant_counts (dict): Dictionary with variant counts per chromosome\n        title (str): Plot title\n    \"\"\"\n    # Extract chromosome names and counts\n    chroms = []\n    counts = []\n    \n    # Handle both string and numeric chromosome names\n    for chrom in sorted(variant_counts['by_chrom'].keys(), \n                        key=lambda x: int(x.replace('chr', '')) if x.replace('chr', '').isdigit() else float('inf')):\n        chroms.append(chrom)\n        counts.append(variant_counts['by_chrom'][chrom])\n    \n    # Create the bar plot\n    plt.figure(figsize=(12, 6))\n    plt.bar(chroms, counts)\n    plt.title(title)\n    plt.xlabel('Chromosome')\n    plt.ylabel('Number of Variants')\n    plt.xticks(rotation=45, ha='right')\n    plt.tight_layout()\n    \n    # Add text with total count\n    total = variant_counts['total']\n    plt.text(0.95, 0.95, f'Total Variants: {total:,}', \n             transform=plt.gca().transAxes, ha='right', va='top', \n             bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))\n    \n    plt.show()\n    \n    return plt.gcf()  # Return the figure for saving if needed\n\ndef plot_variant_density(variant_counts, chromosomes=None, figsize=(15, 10)):\n    \"\"\"Plot the density of variants across chromosomes.\n    \n    Args:\n        variant_counts (dict): Dictionary with variant density information\n        chromosomes (list, optional): List of chromosomes to plot\n        figsize (tuple): Figure size\n    \"\"\"\n    density_data = variant_counts['density']\n    \n    if chromosomes is None:\n        # Use a subset of chromosomes if there are many\n        all_chroms = list(density_data.keys())\n        chromosomes = all_chroms[:min(6, len(all_chroms))]\n    \n    # Calculate number of rows and columns for subplots\n    n_chroms = len(chromosomes)\n    n_cols = min(3, n_chroms)\n    n_rows = (n_chroms + n_cols - 1) // n_cols\n    \n    # Create subplots\n    fig, axes = plt.subplots(n_rows, n_cols, figsize=figsize)\n    if n_rows == 1 and n_cols == 1:\n        axes = np.array([axes])\n    axes = axes.flatten()\n    \n    # Plot density for each chromosome\n    for i, chrom in enumerate(chromosomes):\n        if i < len(axes) and chrom in density_data:\n            ax = axes[i]\n            data = density_data[chrom]\n            x = list(range(1, len(data) + 1))\n            \n            ax.bar(x, data, width=0.8)\n            ax.set_title(f'Chromosome {chrom}')\n            ax.set_xlabel('Position (Mb)')\n            ax.set_ylabel('Variants per Mb')\n            \n            # Add mean line\n            if data:\n                mean_density = sum(data) / len(data)\n                ax.axhline(mean_density, color='red', linestyle='--', \n                           label=f'Mean: {mean_density:.1f}')\n                ax.legend()\n    \n    # Hide unused subplots\n    for i in range(n_chroms, len(axes)):\n        axes[i].axis('off')\n    \n    plt.suptitle('Variant Density Across Chromosomes (per Megabase)', fontsize=16)\n    plt.tight_layout(rect=[0, 0, 1, 0.96])\n    plt.show()\n    \n    return fig\n\ndef plot_sample_quality_metrics(sample_stats, metric='missing_rate', \n                               title=None, top_n=None, threshold=None):\n    \"\"\"Plot quality metrics for samples.\n    \n    Args:\n        sample_stats (dict): Dictionary with sample quality metrics\n        metric (str): Which metric to plot ('missing_rate', 'heterozygosity')\n        title (str, optional): Plot title\n        top_n (int, optional): Show only the top N samples with highest metric values\n        threshold (float, optional): Highlight samples above this threshold\n    \"\"\"\n    # Extract sample names and metric values\n    samples = []\n    values = []\n    \n    for sample, stats in sample_stats.items():\n        if metric in stats:\n            samples.append(sample)\n            values.append(stats[metric])\n    \n    # Sort by metric value\n    sorted_indices = np.argsort(values)[::-1]  # Descending order\n    \n    if top_n is not None:\n        sorted_indices = sorted_indices[:top_n]\n    \n    sorted_samples = [samples[i] for i in sorted_indices]\n    sorted_values = [values[i] for i in sorted_indices]\n    \n    # Create the bar plot\n    plt.figure(figsize=(12, 6))\n    bars = plt.bar(sorted_samples, sorted_values)\n    \n    # Highlight bars above threshold\n    if threshold is not None:\n        for i, value in enumerate(sorted_values):\n            if value > threshold:\n                bars[i].set_color('red')\n    \n    # Set title\n    if title is None:\n        metric_name = 'Missing Rate' if metric == 'missing_rate' else 'Heterozygosity'\n        title = f'Sample {metric_name}'\n    plt.title(title)\n    \n    plt.xlabel('Sample')\n    plt.ylabel(metric.replace('_', ' ').title())\n    plt.xticks(rotation=90)\n    \n    # Add threshold line if specified\n    if threshold is not None:\n        plt.axhline(threshold, color='red', linestyle='--', \n                   label=f'Threshold: {threshold}')\n        plt.legend()\n    \n    plt.tight_layout()\n    plt.show()\n    \n    return plt.gcf()\n\ndef plot_sample_missingness_vs_heterozygosity(sample_stats):\n    \"\"\"Plot missing rate vs heterozygosity for samples.\n    \n    Args:\n        sample_stats (dict): Dictionary with sample quality metrics\n    \"\"\"\n    # Extract values\n    samples = []\n    missing_rates = []\n    heterozygosities = []\n    \n    for sample, stats in sample_stats.items():\n        if 'missing_rate' in stats and 'heterozygosity' in stats:\n            samples.append(sample)\n            missing_rates.append(stats['missing_rate'])\n            heterozygosities.append(stats['heterozygosity'])\n    \n    # Create scatter plot\n    plt.figure(figsize=(10, 8))\n    plt.scatter(missing_rates, heterozygosities, alpha=0.7)\n    \n    # Add labels\n    plt.xlabel('Missing Rate')\n    plt.ylabel('Heterozygosity')\n    plt.title('Sample Quality: Missing Rate vs Heterozygosity')\n    \n    # Add grid\n    plt.grid(True, alpha=0.3)\n    \n    # Annotate some interesting points\n    interesting_points = []\n    \n    # Highest missing rate\n    idx_max_missing = np.argmax(missing_rates)\n    interesting_points.append((missing_rates[idx_max_missing], \n                              heterozygosities[idx_max_missing], \n                              samples[idx_max_missing]))\n    \n    # Highest heterozygosity\n    idx_max_het = np.argmax(heterozygosities)\n    interesting_points.append((missing_rates[idx_max_het], \n                              heterozygosities[idx_max_het], \n                              samples[idx_max_het]))\n    \n    # Lowest heterozygosity\n    idx_min_het = np.argmin(heterozygosities)\n    interesting_points.append((missing_rates[idx_min_het], \n                              heterozygosities[idx_min_het], \n                              samples[idx_min_het]))\n    \n    # Annotate points\n    for x, y, label in interesting_points:\n        plt.annotate(label, (x, y), xytext=(5, 5), textcoords='offset points')\n    \n    plt.tight_layout()\n    plt.show()\n    \n    return plt.gcf()\n\ndef display_dataset_summary(summary, include_plots=True):\n    \"\"\"Display a formatted summary of a genetic dataset.\n    \n    Args:\n        summary (dict): Dataset summary from summarize_dataset()\n        include_plots (bool): Whether to include visualizations\n    \"\"\"\n    metadata = summary['metadata']\n    variant_counts = summary['variant_counts']\n    sample_stats = summary['sample_stats']\n    high_summary = summary['summary']\n    \n    # Basic information\n    display(Markdown(f\"# Dataset Summary: {os.path.basename(high_summary['file_path'])}\"))\n    display(Markdown(f\"**VCF Format:** {high_summary['format']}\"))\n    display(Markdown(f\"**Samples:** {high_summary['samples']}\"))\n    display(Markdown(f\"**Chromosomes:** {len(high_summary['chromosomes'])}\"))\n    display(Markdown(f\"**Total Variants:** {high_summary['total_variants']:,}\"))\n    \n    if high_summary['avg_missing_rate'] is not None:\n        display(Markdown(f\"**Average Missing Rate:** {high_summary['avg_missing_rate']:.4f}\"))\n    \n    if high_summary['avg_heterozygosity'] is not None:\n        display(Markdown(f\"**Average Heterozygosity:** {high_summary['avg_heterozygosity']:.4f}\"))\n    \n    # Include plots if requested\n    if include_plots:\n        display(Markdown(\"## Variant Distribution\"))\n        plot_variant_distribution(variant_counts)\n        \n        if len(variant_counts['density']) > 0:\n            display(Markdown(\"## Variant Density\"))\n            # Plot density for a subset of chromosomes\n            chroms = list(variant_counts['density'].keys())[:6]\n            plot_variant_density(variant_counts, chromosomes=chroms)\n        \n        if sample_stats and any('missing_rate' in s for s in sample_stats.values()):\n            display(Markdown(\"## Sample Quality\"))\n            plot_sample_quality_metrics(sample_stats, metric='missing_rate')\n            \n            if any('heterozygosity' in s for s in sample_stats.values()):\n                plot_sample_quality_metrics(sample_stats, metric='heterozygosity')\n                plot_sample_missingness_vs_heterozygosity(sample_stats)\n    \n    # Return a summary dictionary for further use\n    return high_summary",
   "metadata": {},
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "<cell_type>markdown</cell_type>## Example: Analyzing a Sample Dataset\n\nLet's use our functions to analyze a sample VCF file from the class data. We'll examine the merged OpenSNPs dataset to understand its characteristics and quality metrics.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Path to the sample VCF file\nif is_jupyterlite():\n    # In JupyterLite, use a path relative to the files directory\n    sample_vcf = os.path.join('class_data', 'merged_opensnps_data.vcf.gz')\n    print(f\"Will attempt to load: {sample_vcf}\")\n    print(\"Note: This may not work in JupyterLite due to size limitations.\")\n    print(\"The code is provided as a reference for local execution.\")\nelse:\n    # Use the actual data directory path\n    sample_vcf = os.path.join(DATA_DIR, 'class_data', 'merged_opensnps_data.vcf.gz')\n    \n    # Check if the file exists\n    if os.path.exists(sample_vcf):\n        print(f\"Found sample VCF file: {sample_vcf}\")\n    else:\n        # Try alternative locations\n        alt_paths = [\n            os.path.join(DATA_DIR, 'merged_opensnps_data.vcf.gz'),\n            os.path.join(os.path.dirname(DATA_DIR), 'data', 'class_data', 'merged_opensnps_data.vcf.gz')\n        ]\n        \n        for path in alt_paths:\n            if os.path.exists(path):\n                sample_vcf = path\n                print(f\"Found sample VCF file at alternate location: {sample_vcf}\")\n                break\n        else:\n            print(\"Could not find the sample VCF file.\")\n            print(\"Please set the correct path to a VCF file in your environment.\")\n            sample_vcf = None",
   "metadata": {},
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Analyze the dataset if available\nif sample_vcf and not is_jupyterlite():\n    # Use limited analysis to prevent excessive computation\n    print(\"Analyzing the dataset (with limited samples for efficiency)...\")\n    dataset_summary = summarize_dataset(\n        sample_vcf, \n        max_metadata_lines=500,   # Only read 500 header lines\n        max_variants=5000,        # Only analyze 5000 variants for sample quality\n        max_count_variants=50000  # Only count 50000 variants for chromosome distribution\n    )\n    \n    # Display summary\n    display_dataset_summary(dataset_summary)\nelse:\n    print(\"Dataset analysis example:\")\n    print(\"1. We would load the VCF file\")\n    print(\"2. Process its metadata to understand structure\")\n    print(\"3. Count variants across chromosomes\")\n    print(\"4. Analyze sample quality metrics\")\n    print(\"5. Visualize the results\")\n    \n    # Show expected output format using dummy data\n    dummy_summary = {\n        'metadata': {\n            'file_path': 'example.vcf.gz',\n            'file_format': 'VCFv4.2',\n            'samples': ['sample1', 'sample2', 'sample3'],\n            'n_samples': 3,\n            'contigs': [{'ID': '1'}, {'ID': '2'}],\n            'n_contigs': 2\n        },\n        'variant_counts': {\n            'total': 10000,\n            'by_chrom': {'1': 5500, '2': 4500},\n            'density': {'1': [100, 120, 95], '2': [80, 90, 85]}\n        },\n        'sample_stats': {\n            'sample1': {\n                'missing_rate': 0.02,\n                'heterozygosity': 0.3\n            },\n            'sample2': {\n                'missing_rate': 0.01,\n                'heterozygosity': 0.25\n            },\n            'sample3': {\n                'missing_rate': 0.05,\n                'heterozygosity': 0.28\n            }\n        },\n        'summary': {\n            'file_path': 'example.vcf.gz',\n            'format': 'VCFv4.2',\n            'samples': 3,\n            'contigs': 2,\n            'total_variants': 10000,\n            'chromosomes': ['1', '2'],\n            'avg_missing_rate': 0.027,\n            'avg_heterozygosity': 0.277\n        }\n    }\n    \n    print(\"\\nExample output would look like this:\")\n    display_dataset_summary(dummy_summary, include_plots=True)",
   "metadata": {},
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "<cell_type>markdown</cell_type>## Exercise: Dataset Comparison Tool\n\nIn real-world scenarios, you'll often need to compare different genetic datasets to understand their compatibility for merging or joint analysis. Let's develop a tool to compare two VCF files and assess their similarity and compatibility for use in Bonsai.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Exercise: Implement a function to compare two datasets\n\ndef compare_datasets(vcf_file1, vcf_file2, max_variants=10000):\n    \"\"\"Compare two VCF files and assess their compatibility.\n    \n    Args:\n        vcf_file1 (str): Path to first VCF file\n        vcf_file2 (str): Path to second VCF file\n        max_variants (int): Maximum number of variants to analyze\n        \n    Returns:\n        dict: Dictionary with comparison results\n    \"\"\"\n    # TODO: Implement this function to:\n    # 1. Analyze both datasets using summarize_dataset\n    # 2. Compare sample overlap\n    # 3. Compare variant overlap\n    # 4. Compare chromosomes covered\n    # 5. Calculate overall compatibility score\n    \n    # For this exercise, stub implementation:\n    print(f\"Comparing {os.path.basename(vcf_file1)} and {os.path.basename(vcf_file2)}\")\n    \n    # Placeholder comparison results\n    comparison = {\n        'sample_overlap': {\n            'file1_samples': 0,\n            'file2_samples': 0,\n            'common_samples': 0,\n            'overlap_percent': 0.0\n        },\n        'chromosome_overlap': {\n            'file1_chroms': [],\n            'file2_chroms': [],\n            'common_chroms': [],\n            'overlap_percent': 0.0\n        },\n        'variant_overlap': {\n            'estimated_percent': 0.0  # Difficult to compute exactly without reading both files\n        },\n        'format_compatibility': {\n            'same_format': False,\n            'compatible_genotype_format': False\n        },\n        'overall_compatibility': 0.0  # 0 (incompatible) to 1 (perfect compatibility)\n    }\n    \n    # Return comparison results\n    return comparison\n\n# Hint: Your implementation should:\n# 1. Use summarize_dataset() to get metadata and basic stats for both files\n# 2. Compare the samples lists to find common samples: \n#    common = set(summary1['metadata']['samples']) & set(summary2['metadata']['samples'])\n# 3. Calculate sample overlap percentage\n# 4. Compare chromosome lists similarly\n# 5. For variant overlap, you would need to check positions, which is more complex\n#    A simple approach could just check if chromosomes and positions match between datasets\n# 6. Check format compatibility (VCF versions, genotype formats)\n# 7. Calculate a weighted overall compatibility score",
   "metadata": {},
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "<cell_type>markdown</cell_type>## Part 2: Data Preparation Challenges\n\nWhen working with real-world genetic datasets, various data preparation challenges must be addressed before applying IBD detection and relationship inference algorithms. This section explores common challenges and solutions.\n\n### 2.1 Format Conversion and Standardization\n\nDifferent genetic testing companies and research institutions use different file formats:\n\n- **Direct-to-consumer formats**:\n  - 23andMe (text files with rsIDs and genotypes)\n  - AncestryDNA (proprietary format with genotype data)\n  - FamilyTreeDNA (CSV format with SNPs and genotypes)\n  \n- **Research formats**:\n  - VCF (Variant Call Format) - standard for genetic variation data\n  - PLINK formats (.bed/.bim/.fam) - commonly used in statistical genetics\n  - EIGENSTRAT format - used in population genetics studies\n  \n- **Conversion considerations**:\n  - Reference genome alignment (hg19, hg38, etc.)\n  - Strand orientation differences\n  - SNP naming conventions (rsIDs, chromosome:position, etc.)\n  - Genotype encoding differences\n\n### 2.2 Dealing with Missing Data\n\nReal datasets invariably have missing genotypes due to:\n\n- **Technical limitations**: Low coverage or read quality in specific regions\n- **Platform differences**: Different SNP arrays targeting different markers\n- **Quality control filters**: Variants removed due to quality concerns\n- **Sample-specific issues**: DNA quality affecting certain regions\n\nAddressing missing data requires:\n\n- **Imputation**: Statistical inference of missing genotypes based on reference panels\n- **Filtering**: Removing variants/samples with high missingness rates\n- **Weighting**: Accounting for missingness in downstream analyses\n- **Simulation**: Testing how missingness affects results using controlled experiments\n\n### 2.3 Quality Control and Filtering\n\nEssential QC metrics to consider:\n\n- **Sample-level QC**:\n  - Missingness rate (percent of genotypes missing per sample)\n  - Heterozygosity rate (deviation suggests contamination or inbreeding)\n  - Sex checks (genetic vs. reported sex)\n  - Relatedness checks (unexpected duplicates or close relatives)\n  \n- **Variant-level QC**:\n  - Minor allele frequency (MAF)\n  - Hardy-Weinberg equilibrium (HWE)\n  - Missingness rate per variant\n  - Mendelian inconsistencies in family data\n  \n- **Batch effects**:\n  - Sample processing date/batch\n  - Genotyping platform differences\n  - Laboratory-specific artifacts\n\n### 2.4 IBD Detection Variations\n\nIBD detection algorithms have different sensitivities to data characteristics:\n\n- **GERMLINE**: Sensitive to marker density and phasing accuracy\n- **BEAGLE IBD**: Computationally intensive but handles unphased data well\n- **Refined IBD**: Good for detecting shorter segments but requires phased data\n- **IBIS**: Balances speed and accuracy, handles various input qualities\n- **hap-IBD**: High accuracy for well-phased data\n\nAdjustments needed for different data sources:\n\n- **Parameter tuning**: Error rates, minimum segment lengths\n- **Pre-processing**: Phasing quality, marker selection\n- **Post-processing**: Filtering spurious segments, merging adjacent segments\n- **Validation**: Cross-algorithm comparison for reliability",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "## Implementation: Data Preparation Pipeline\n\nLet's create a skeleton for a data preparation pipeline that processes raw genetic data for IBD detection and relationship inference with Bonsai.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Data preparation pipeline outline\nimport os\nimport subprocess\nimport pandas as pd\nimport numpy as np\nfrom collections import defaultdict\nimport json\n\nclass DataPreparationPipeline:\n    \"\"\"Pipeline for preparing genetic data for IBD detection and Bonsai analysis.\"\"\"\n    \n    def __init__(self, config=None):\n        \"\"\"Initialize the pipeline with configuration parameters.\n        \n        Args:\n            config (dict, optional): Configuration dictionary with parameters\n        \"\"\"\n        # Default configuration\n        self.config = {\n            'input_format': 'vcf',\n            'output_format': 'vcf',\n            'reference_genome': 'hg38',\n            'min_maf': 0.01,\n            'max_missing_rate': 0.05,\n            'min_hwe_pvalue': 1e-6,\n            'output_directory': 'processed_data',\n            'temp_directory': 'temp',\n            'ibd_algorithm': 'ibis',\n            'ibd_min_segment_cm': 7.0,\n            'ibd_max_gaps': 1,\n            'ibd_max_error': 0.01,\n            'phasing_algorithm': 'eagle',\n            'phasing_reference': None,\n            'threads': 4\n        }\n        \n        # Update with user configuration if provided\n        if config:\n            self.config.update(config)\n        \n        # Create output and temp directories if they don't exist\n        os.makedirs(self.config['output_directory'], exist_ok=True)\n        os.makedirs(self.config['temp_directory'], exist_ok=True)\n        \n        # Initialize tracking for processed files\n        self.processed_files = []\n        \n    def convert_format(self, input_file, input_format=None, output_format=None):\n        \"\"\"Convert between genetic data formats.\n        \n        Args:\n            input_file (str): Path to input file\n            input_format (str, optional): Input format (auto-detected if None)\n            output_format (str, optional): Output format (uses config default if None)\n            \n        Returns:\n            str: Path to converted file\n        \"\"\"\n        print(f\"Converting {input_file} from {input_format or 'auto-detected'} to {output_format or self.config['output_format']}\")\n        \n        # This would be a mock implementation - in reality would use tools like\n        # PLINK, bcftools, etc. to perform the actual conversion\n        \n        # Determine input format if not specified\n        if input_format is None:\n            if input_file.endswith('.vcf') or input_file.endswith('.vcf.gz'):\n                input_format = 'vcf'\n            elif input_file.endswith('.bed') or input_file.endswith('.bim') or input_file.endswith('.fam'):\n                input_format = 'plink'\n            elif input_file.endswith('.txt') or input_file.endswith('.csv'):\n                input_format = 'text'\n            else:\n                input_format = 'unknown'\n        \n        # Use config output format if not specified\n        if output_format is None:\n            output_format = self.config['output_format']\n        \n        # Create output filename\n        basename = os.path.splitext(os.path.basename(input_file))[0]\n        if basename.endswith('.vcf'):  # Handle .vcf.gz case\n            basename = os.path.splitext(basename)[0]\n            \n        output_file = os.path.join(\n            self.config['output_directory'], \n            f\"{basename}.{output_format}\"\n        )\n        \n        # Mock conversion process with description of what would happen\n        print(f\"  Conversion would use appropriate tools based on formats:\")\n        if input_format == 'vcf' and output_format == 'plink':\n            print(\"  - Would use 'plink2 --vcf input.vcf --make-bed --out output'\")\n        elif input_format == 'plink' and output_format == 'vcf':\n            print(\"  - Would use 'plink2 --bfile input --recode vcf --out output'\")\n        elif input_format == 'text' and output_format == 'vcf':\n            print(\"  - Would use custom parsing for the specific text format\")\n            print(\"  - Would map variant IDs to positions\")\n            print(\"  - Would convert genotypes to VCF format\")\n        \n        # In a real implementation, we would run the conversion command here\n        # but for this example, we just pretend we did\n        \n        self.processed_files.append({\n            'stage': 'format_conversion',\n            'input': input_file,\n            'output': output_file,\n            'parameters': {\n                'input_format': input_format,\n                'output_format': output_format\n            }\n        })\n        \n        print(f\"  Converted file would be saved to: {output_file}\")\n        return output_file\n    \n    def perform_qc(self, input_file, variant_qc=True, sample_qc=True):\n        \"\"\"Perform quality control on genetic data.\n        \n        Args:\n            input_file (str): Path to input file\n            variant_qc (bool): Whether to perform variant-level QC\n            sample_qc (bool): Whether to perform sample-level QC\n            \n        Returns:\n            str: Path to QC'd file\n        \"\"\"\n        print(f\"Performing QC on {input_file}\")\n        \n        # Create output filename\n        basename = os.path.splitext(os.path.basename(input_file))[0]\n        if basename.endswith('.vcf'):  # Handle .vcf.gz case\n            basename = os.path.splitext(basename)[0]\n            \n        output_file = os.path.join(\n            self.config['output_directory'], \n            f\"{basename}.qc.vcf\"\n        )\n        \n        # Mock QC process\n        filters_applied = []\n        \n        if variant_qc:\n            print(\"  Variant QC:\")\n            print(f\"  - Minor allele frequency filter: {self.config['min_maf']}\")\n            filters_applied.append(f\"MAF > {self.config['min_maf']}\")\n            \n            print(f\"  - Missing rate filter: {self.config['max_missing_rate']}\")\n            filters_applied.append(f\"Missing rate < {self.config['max_missing_rate']}\")\n            \n            print(f\"  - Hardy-Weinberg equilibrium filter: {self.config['min_hwe_pvalue']}\")\n            filters_applied.append(f\"HWE p-value > {self.config['min_hwe_pvalue']}\")\n        \n        if sample_qc:\n            print(\"  Sample QC:\")\n            print(\"  - Missing rate per sample\")\n            print(\"  - Heterozygosity outlier detection\")\n            print(\"  - Sex check\")\n            print(\"  - Duplicate detection\")\n            filters_applied.append(\"Sample QC filters\")\n        \n        self.processed_files.append({\n            'stage': 'quality_control',\n            'input': input_file,\n            'output': output_file,\n            'parameters': {\n                'variant_qc': variant_qc,\n                'sample_qc': sample_qc,\n                'filters_applied': filters_applied\n            }\n        })\n        \n        print(f\"  QC'd file would be saved to: {output_file}\")\n        return output_file\n    \n    def phase_genotypes(self, input_file):\n        \"\"\"Phase genotypes using the configured phasing algorithm.\n        \n        Args:\n            input_file (str): Path to input file\n            \n        Returns:\n            str: Path to phased file\n        \"\"\"\n        print(f\"Phasing genotypes in {input_file}\")\n        \n        # Create output filename\n        basename = os.path.splitext(os.path.basename(input_file))[0]\n        if basename.endswith('.vcf'):  # Handle .vcf.gz case\n            basename = os.path.splitext(basename)[0]\n            \n        output_file = os.path.join(\n            self.config['output_directory'], \n            f\"{basename}.phased.vcf\"\n        )\n        \n        # Mock phasing process\n        print(f\"  Using phasing algorithm: {self.config['phasing_algorithm']}\")\n        \n        if self.config['phasing_reference']:\n            print(f\"  Using reference panel: {self.config['phasing_reference']}\")\n        \n        print(f\"  Processing with {self.config['threads']} threads\")\n        \n        # In a real implementation, we would run the phasing command here\n        if self.config['phasing_algorithm'] == 'shapeit':\n            print(\"  - Would use SHAPEIT for phasing\")\n        elif self.config['phasing_algorithm'] == 'eagle':\n            print(\"  - Would use Eagle for phasing\")\n        elif self.config['phasing_algorithm'] == 'beagle':\n            print(\"  - Would use BEAGLE for phasing\")\n            \n        self.processed_files.append({\n            'stage': 'phasing',\n            'input': input_file,\n            'output': output_file,\n            'parameters': {\n                'algorithm': self.config['phasing_algorithm'],\n                'reference': self.config['phasing_reference'],\n                'threads': self.config['threads']\n            }\n        })\n        \n        print(f\"  Phased file would be saved to: {output_file}\")\n        return output_file\n    \n    def detect_ibd(self, input_file):\n        \"\"\"Detect IBD segments using the configured algorithm.\n        \n        Args:\n            input_file (str): Path to input file\n            \n        Returns:\n            str: Path to IBD segments file\n        \"\"\"\n        print(f\"Detecting IBD segments in {input_file}\")\n        \n        # Create output filename\n        basename = os.path.splitext(os.path.basename(input_file))[0]\n        if basename.endswith('.vcf'):  # Handle .vcf.gz case\n            basename = os.path.splitext(basename)[0]\n            \n        output_file = os.path.join(\n            self.config['output_directory'], \n            f\"{basename}.ibd.seg\"\n        )\n        \n        # Mock IBD detection process\n        print(f\"  Using IBD algorithm: {self.config['ibd_algorithm']}\")\n        print(f\"  Minimum segment length: {self.config['ibd_min_segment_cm']} cM\")\n        print(f\"  Maximum gaps allowed: {self.config['ibd_max_gaps']}\")\n        print(f\"  Maximum error rate: {self.config['ibd_max_error']}\")\n        \n        # In a real implementation, we would run the IBD detection command here\n        if self.config['ibd_algorithm'] == 'ibis':\n            print(\"  - Would use IBIS for IBD detection\")\n        elif self.config['ibd_algorithm'] == 'hapibd':\n            print(\"  - Would use hap-IBD for IBD detection\")\n        elif self.config['ibd_algorithm'] == 'refinedibd':\n            print(\"  - Would use Refined-IBD for IBD detection\")\n        elif self.config['ibd_algorithm'] == 'germline':\n            print(\"  - Would use GERMLINE for IBD detection\")\n            \n        self.processed_files.append({\n            'stage': 'ibd_detection',\n            'input': input_file,\n            'output': output_file,\n            'parameters': {\n                'algorithm': self.config['ibd_algorithm'],\n                'min_segment_cm': self.config['ibd_min_segment_cm'],\n                'max_gaps': self.config['ibd_max_gaps'],\n                'max_error': self.config['ibd_max_error']\n            }\n        })\n        \n        print(f\"  IBD segments would be saved to: {output_file}\")\n        return output_file\n    \n    def run_pipeline(self, input_file):\n        \"\"\"Run the complete data preparation pipeline.\n        \n        Args:\n            input_file (str): Path to input file\n            \n        Returns:\n            dict: Dictionary with paths to all output files\n        \"\"\"\n        print(f\"Running complete pipeline on {input_file}\")\n        \n        results = {}\n        \n        # Track original input\n        results['original_input'] = input_file\n        \n        # Step 1: Convert format if needed\n        input_format = None  # Auto-detect\n        if self.config['input_format'] != self.config['output_format']:\n            converted_file = self.convert_format(input_file, input_format)\n            results['converted_file'] = converted_file\n            current_file = converted_file\n        else:\n            current_file = input_file\n        \n        # Step 2: Perform QC\n        qc_file = self.perform_qc(current_file)\n        results['qc_file'] = qc_file\n        current_file = qc_file\n        \n        # Step 3: Phase genotypes\n        phased_file = self.phase_genotypes(current_file)\n        results['phased_file'] = phased_file\n        current_file = phased_file\n        \n        # Step 4: Detect IBD\n        ibd_file = self.detect_ibd(current_file)\n        results['ibd_file'] = ibd_file\n        \n        # Save processing log\n        log_file = os.path.join(\n            self.config['output_directory'],\n            f\"{os.path.basename(input_file)}.processing_log.json\"\n        )\n        \n        with open(log_file, 'w') as f:\n            json.dump({\n                'config': self.config,\n                'processed_files': self.processed_files,\n                'results': results\n            }, f, indent=2)\n        \n        results['log_file'] = log_file\n        \n        print(f\"\\nPipeline complete! Processing log saved to: {log_file}\")\n        return results",
   "metadata": {},
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Example usage of the data preparation pipeline\nif sample_vcf and not is_jupyterlite():\n    print(\"Running data preparation pipeline on sample dataset (simulation only)...\")\n    \n    # Configure pipeline with customized parameters\n    pipeline_config = {\n        'input_format': 'vcf',\n        'output_format': 'vcf',\n        'reference_genome': 'hg38',\n        'min_maf': 0.05,\n        'max_missing_rate': 0.1,\n        'output_directory': 'example_processed',\n        'ibd_algorithm': 'hapibd',\n        'ibd_min_segment_cm': 8.0,\n        'phasing_algorithm': 'eagle'\n    }\n    \n    # Initialize pipeline\n    pipeline = DataPreparationPipeline(config=pipeline_config)\n    \n    # Run the pipeline (simulation only)\n    results = pipeline.run_pipeline(sample_vcf)\n    \n    print(\"\\nGenerated output files (simulated):\")\n    for key, path in results.items():\n        print(f\"  {key}: {path}\")\nelse:\n    print(\"Pipeline usage example:\")\n    print(\"1. Configure the pipeline with appropriate parameters\")\n    print(\"2. Initialize the pipeline object\")\n    print(\"3. Run the pipeline on input data\")\n    print(\"4. Process the results for Bonsai analysis\")\n    \n    # Show example code\n    print(\"\\nExample code:\")\n    print(\"\"\"\n    # Configure pipeline\n    pipeline_config = {\n        'input_format': 'vcf',\n        'output_format': 'vcf',\n        'min_maf': 0.05,\n        'max_missing_rate': 0.1,\n        'output_directory': 'processed_data',\n        'ibd_algorithm': 'hapibd',\n        'ibd_min_segment_cm': 8.0,\n        'phasing_algorithm': 'eagle'\n    }\n    \n    # Initialize and run pipeline\n    pipeline = DataPreparationPipeline(config=pipeline_config)\n    results = pipeline.run_pipeline('input_data.vcf')\n    \n    # Use the IBD segments for Bonsai analysis\n    ibd_file = results['ibd_file']\n    # ... proceed with Bonsai analysis ...\n    \"\"\")",
   "metadata": {},
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "<cell_type>markdown</cell_type>## Part 3: Population-Specific Considerations\n\nDifferent human populations have distinct genetic characteristics that impact IBD detection and relationship inference. Understanding these differences is crucial for accurate genealogical reconstruction.\n\n### 3.1 Variation in Background IBD by Population\n\nThe background level of IBD sharing varies substantially across populations:\n\n- **Founder populations**: Groups descended from a small number of ancestors (e.g., Ashkenazi Jews, Finns) show elevated IBD sharing even between distant relatives\n- **Island populations**: Geographic isolation leads to higher background IBD (e.g., Sardinians, Icelanders)\n- **Recently admixed populations**: May show complex IBD patterns reflecting multiple ancestral sources\n- **Outbred populations**: Continental Europeans and East Asians typically show lower background IBD\n\nThese differences require:\n- Population-specific thresholds for detecting relationships\n- Adjustment of expected IBD sharing metrics by population\n- Calibration of likelihood models based on population structure\n\n### 3.2 Endogamy Levels and Impact\n\nEndogamy (marriage within a community) profoundly affects genetic genealogy:\n\n- **High endogamy**: Found in historically isolated communities, religious groups, and small populations\n- **Moderate endogamy**: Common in rural communities and traditional societies\n- **Low endogamy**: Typical in modern urban populations with high mobility\n\nImpact on relationship inference:\n- Multiple paths of relationship between individuals\n- Elevated total IBD sharing compared to expected in outbred populations\n- Difficulty distinguishing between relationship types (e.g., 2nd cousins vs. 3rd cousins)\n- Need for specialized algorithms that account for multiple relationship paths\n\n### 3.3 Different Demographic Histories\n\nPopulation histories affect IBD patterns:\n\n- **Bottlenecked populations**: Show longer shared segments due to recent common ancestry\n- **Expanding populations**: Display fewer and shorter IBD segments\n- **Populations with migration**: May show stratified IBD patterns by geographic region\n- **Recently isolated populations**: Often have higher and more varied IBD sharing\n\nImplications for Bonsai parameters:\n- Adjustment of expected IBD distributions based on demographic history\n- Population-specific age estimation models\n- Different priors for relationship probabilities\n\n### 3.4 Cultural Family Structure Variations\n\nCultural practices affect family structures and should inform analysis:\n\n- **Average family size**: Varies substantially across cultures and time periods\n- **Cousin marriage rates**: Common in some cultures, rare in others\n- **Polygamy**: Multiple spouses create complex sibship patterns\n- **Adoption practices**: Formal and informal adoption affects genetic vs. social relatedness\n- **Age at reproduction**: Affects generation time estimates in relationship inference",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "## Implementation: Population-Specific Relationship Inference\n\nLet's explore how Bonsai v3 can be configured to handle population-specific considerations in relationship inference. We'll implement functions to:\n\n1. Estimate the endogamy level in a dataset\n2. Adjust relationship likelihood models based on population information\n3. Generate appropriate priors for different populations",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Population-specific relationship inference functions\n\ndef estimate_endogamy_level(ibd_segments_df, population_label=None):\n    \"\"\"Estimate the endogamy level in a dataset based on IBD segment patterns.\n    \n    Args:\n        ibd_segments_df (DataFrame): DataFrame containing IBD segments with columns:\n                                    'id1', 'id2', 'chr', 'start', 'end', 'cm_length'\n        population_label (str, optional): Population label for reference values\n        \n    Returns:\n        dict: Dictionary with endogamy metrics\n    \"\"\"\n    # This is a simplified model of endogamy estimation\n    # In a real implementation, this would be more sophisticated\n    \n    # Create a mock IBD segments DataFrame if none provided\n    if ibd_segments_df is None or len(ibd_segments_df) == 0:\n        print(\"Using simulated IBD segments for demonstration\")\n        # Create a mock dataset for demonstration\n        np.random.seed(42)\n        n_samples = 100\n        n_segments = 1000\n        \n        # Generate random sample IDs\n        sample_ids = [f\"sample_{i}\" for i in range(n_samples)]\n        \n        # Generate random IBD segments\n        id1 = np.random.choice(sample_ids, n_segments)\n        id2 = np.random.choice(sample_ids, n_segments)\n        \n        # Ensure id1 != id2\n        for i in range(n_segments):\n            while id1[i] == id2[i]:\n                id2[i] = np.random.choice(sample_ids)\n        \n        chr_num = np.random.randint(1, 23, n_segments)\n        start_pos = np.random.randint(1, 200000000, n_segments)\n        seg_lengths = np.random.exponential(10, n_segments)  # Length in cM with mean 10\n        end_pos = start_pos + seg_lengths * 1000000  # Rough approximation\n        \n        ibd_segments_df = pd.DataFrame({\n            'id1': id1,\n            'id2': id2,\n            'chr': chr_num,\n            'start': start_pos,\n            'end': end_pos,\n            'cm_length': seg_lengths\n        })\n    \n    # Count unique pairs of individuals\n    unique_pairs = set(zip(ibd_segments_df['id1'], ibd_segments_df['id2']))\n    n_unique_pairs = len(unique_pairs)\n    \n    # Count unique individuals\n    unique_individuals = set(ibd_segments_df['id1']).union(set(ibd_segments_df['id2']))\n    n_unique_individuals = len(unique_individuals)\n    \n    # Calculate maximum possible pairs\n    max_possible_pairs = (n_unique_individuals * (n_unique_individuals - 1)) // 2\n    \n    # Calculate pair density (percentage of all possible pairs that share IBD)\n    pair_density = n_unique_pairs / max_possible_pairs if max_possible_pairs > 0 else 0\n    \n    # Calculate average number of segments per pair\n    segments_per_pair = len(ibd_segments_df) / n_unique_pairs if n_unique_pairs > 0 else 0\n    \n    # Calculate average total cM shared per pair\n    cm_per_pair = ibd_segments_df.groupby(['id1', 'id2'])['cm_length'].sum().mean()\n    \n    # Calculate distribution of segment lengths\n    segment_length_quantiles = ibd_segments_df['cm_length'].quantile([0.25, 0.5, 0.75]).to_dict()\n    \n    # Reference values for different endogamy levels\n    # These are simplified and would be calibrated with real data in practice\n    reference_values = {\n        'high_endogamy': {\n            'pair_density': 0.7,\n            'segments_per_pair': 5.0,\n            'avg_cm_per_pair': 100.0\n        },\n        'moderate_endogamy': {\n            'pair_density': 0.4,\n            'segments_per_pair': 3.0,\n            'avg_cm_per_pair': 50.0\n        },\n        'low_endogamy': {\n            'pair_density': 0.1,\n            'segments_per_pair': 1.5,\n            'avg_cm_per_pair': 20.0\n        },\n        # Population-specific reference values\n        'ashkenazi': {\n            'pair_density': 0.8,\n            'segments_per_pair': 8.0,\n            'avg_cm_per_pair': 150.0\n        },\n        'finnish': {\n            'pair_density': 0.6,\n            'segments_per_pair': 4.0,\n            'avg_cm_per_pair': 80.0\n        },\n        'european': {\n            'pair_density': 0.2,\n            'segments_per_pair': 2.0,\n            'avg_cm_per_pair': 30.0\n        },\n        'east_asian': {\n            'pair_density': 0.15,\n            'segments_per_pair': 1.8,\n            'avg_cm_per_pair': 25.0\n        }\n    }\n    \n    # Calculate a simple endogamy score\n    # This is a weighted average of normalized metrics\n    norm_pair_density = pair_density / reference_values['high_endogamy']['pair_density']\n    norm_segments_per_pair = segments_per_pair / reference_values['high_endogamy']['segments_per_pair']\n    norm_cm_per_pair = cm_per_pair / reference_values['high_endogamy']['avg_cm_per_pair']\n    \n    # Weighted average (weights could be adjusted)\n    endogamy_score = (norm_pair_density * 0.4 + \n                      norm_segments_per_pair * 0.3 + \n                      norm_cm_per_pair * 0.3)\n    \n    # Compare to reference population if provided\n    if population_label and population_label in reference_values:\n        ref = reference_values[population_label]\n        relative_to_reference = {\n            'pair_density_ratio': pair_density / ref['pair_density'],\n            'segments_ratio': segments_per_pair / ref['segments_per_pair'],\n            'cm_ratio': cm_per_pair / ref['avg_cm_per_pair']\n        }\n    else:\n        relative_to_reference = None\n    \n    # Classify endogamy level\n    if endogamy_score > 0.7:\n        endogamy_level = \"High\"\n    elif endogamy_score > 0.3:\n        endogamy_level = \"Moderate\"\n    else:\n        endogamy_level = \"Low\"\n    \n    # Return comprehensive results\n    return {\n        'metrics': {\n            'unique_pairs': n_unique_pairs,\n            'unique_individuals': n_unique_individuals,\n            'max_possible_pairs': max_possible_pairs,\n            'pair_density': pair_density,\n            'segments_per_pair': segments_per_pair,\n            'avg_cm_per_pair': cm_per_pair,\n            'segment_length_quantiles': segment_length_quantiles\n        },\n        'endogamy_score': endogamy_score,\n        'endogamy_level': endogamy_level,\n        'relative_to_reference': relative_to_reference\n    }\n\ndef adjust_relationship_likelihoods(base_likelihoods, endogamy_level, population=None):\n    \"\"\"Adjust relationship likelihoods based on endogamy level and population.\n    \n    Args:\n        base_likelihoods (dict): Base relationship likelihoods from Bonsai\n        endogamy_level (float or str): Endogamy level (numeric score or categorical)\n        population (str, optional): Population label for specific adjustments\n        \n    Returns:\n        dict: Adjusted relationship likelihoods\n    \"\"\"\n    # Convert categorical endogamy level to numeric if needed\n    if isinstance(endogamy_level, str):\n        if endogamy_level.lower() == \"high\":\n            endogamy_factor = 0.8\n        elif endogamy_level.lower() == \"moderate\":\n            endogamy_factor = 0.5\n        elif endogamy_level.lower() == \"low\":\n            endogamy_factor = 0.2\n        else:\n            endogamy_factor = 0.3  # Default\n    else:\n        # Use the numeric score directly\n        endogamy_factor = min(1.0, max(0.0, endogamy_level))\n    \n    # Base likelihood adjustments for endogamy\n    adjustments = {\n        'parent-child': 1.0,  # No adjustment for parent-child\n        'full-sibling': 1.0,  # No adjustment for full siblings\n        'half-sibling': 0.95,  # Slight adjustment\n        'aunt-nephew': 0.9,\n        'grandparent-grandchild': 0.9,\n        'first-cousin': 0.8,\n        'first-cousin-once-removed': 0.7,\n        'second-cousin': 0.6,\n        'second-cousin-once-removed': 0.5,\n        'third-cousin': 0.4,\n        'third-cousin-once-removed': 0.3,\n        'fourth-cousin': 0.2,\n        'unrelated': 0.1\n    }\n    \n    # Population-specific adjustments (multiplicative factors)\n    population_factors = {\n        'ashkenazi': {\n            'first-cousin': 0.7,\n            'second-cousin': 0.5,\n            'third-cousin': 0.3,\n            'fourth-cousin': 0.1\n        },\n        'finnish': {\n            'first-cousin': 0.8,\n            'second-cousin': 0.6,\n            'third-cousin': 0.4,\n            'fourth-cousin': 0.2\n        },\n        'european': {\n            'first-cousin': 0.9,\n            'second-cousin': 0.8,\n            'third-cousin': 0.7,\n            'fourth-cousin': 0.6\n        }\n    }\n    \n    # Apply adjustments\n    adjusted_likelihoods = {}\n    for rel, likelihood in base_likelihoods.items():\n        # Base adjustment factor\n        if rel in adjustments:\n            adj_factor = 1.0 - (endogamy_factor * (1.0 - adjustments[rel]))\n        else:\n            adj_factor = 1.0  # No adjustment for unknown relationships\n        \n        # Apply population-specific factor if available\n        if population in population_factors and rel in population_factors[population]:\n            pop_factor = population_factors[population][rel]\n            # Blend with endogamy-based adjustment\n            adj_factor = adj_factor * 0.7 + pop_factor * 0.3\n        \n        # Apply adjustment to likelihood\n        adjusted_likelihoods[rel] = likelihood * adj_factor\n    \n    # Normalize likelihoods to sum to 1\n    total = sum(adjusted_likelihoods.values())\n    if total > 0:\n        for rel in adjusted_likelihoods:\n            adjusted_likelihoods[rel] /= total\n    \n    return adjusted_likelihoods\n\ndef generate_population_priors(population, endogamy_level=None):\n    \"\"\"Generate appropriate relationship priors for different populations.\n    \n    Args:\n        population (str): Population label\n        endogamy_level (float or str, optional): Endogamy level\n        \n    Returns:\n        dict: Prior probabilities for different relationships\n    \"\"\"\n    # Base priors for an outbred population\n    base_priors = {\n        'parent-child': 0.05,\n        'full-sibling': 0.05,\n        'half-sibling': 0.04,\n        'aunt-nephew': 0.04,\n        'grandparent-grandchild': 0.04,\n        'first-cousin': 0.1,\n        'first-cousin-once-removed': 0.1,\n        'second-cousin': 0.1,\n        'second-cousin-once-removed': 0.1,\n        'third-cousin': 0.1,\n        'third-cousin-once-removed': 0.08,\n        'fourth-cousin': 0.08,\n        'unrelated': 0.12\n    }\n    \n    # Population-specific prior adjustments\n    population_adjustments = {\n        'ashkenazi': {\n            'first-cousin': 1.5,      # Higher probability of first-cousin marriages\n            'second-cousin': 1.5,      # Higher probability of second-cousin marriages\n            'third-cousin': 1.5,       # Higher probability of third-cousin relationships\n            'unrelated': 0.5           # Lower probability of truly unrelated individuals\n        },\n        'finnish': {\n            'first-cousin': 1.3,\n            'second-cousin': 1.3,\n            'third-cousin': 1.3,\n            'unrelated': 0.6\n        },\n        'european': {\n            'first-cousin': 0.8,      # Lower probability of first-cousin marriages\n            'unrelated': 1.2          # Higher probability of unrelated individuals\n        },\n        'east_asian': {\n            'first-cousin': 0.9,\n            'unrelated': 1.1\n        }\n    }\n    \n    # Apply population adjustments if available\n    adjusted_priors = base_priors.copy()\n    if population in population_adjustments:\n        for rel, factor in population_adjustments[population].items():\n            if rel in adjusted_priors:\n                adjusted_priors[rel] *= factor\n    \n    # Further adjust based on endogamy level if provided\n    if endogamy_level:\n        # Convert categorical to numeric if needed\n        if isinstance(endogamy_level, str):\n            if endogamy_level.lower() == \"high\":\n                endogamy_factor = 0.8\n            elif endogamy_level.lower() == \"moderate\":\n                endogamy_factor = 0.5\n            elif endogamy_level.lower() == \"low\":\n                endogamy_factor = 0.2\n            else:\n                endogamy_factor = 0.0\n        else:\n            endogamy_factor = min(1.0, max(0.0, endogamy_level))\n        \n        # Apply endogamy adjustments\n        if endogamy_factor > 0:\n            # Increase priors for distant relationships based on endogamy\n            adjusted_priors['second-cousin'] *= (1 + endogamy_factor * 0.5)\n            adjusted_priors['third-cousin'] *= (1 + endogamy_factor * 1.0)\n            adjusted_priors['fourth-cousin'] *= (1 + endogamy_factor * 1.5)\n            \n            # Decrease prior for unrelated\n            adjusted_priors['unrelated'] *= (1 - endogamy_factor * 0.5)\n    \n    # Normalize priors to sum to 1\n    total = sum(adjusted_priors.values())\n    for rel in adjusted_priors:\n        adjusted_priors[rel] /= total\n    \n    return adjusted_priors",
   "metadata": {},
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Demonstration of population-specific relationship inference\n\n# Simulate an IBD segments dataframe for testing\n# In practice, you would load this from a real IBD detection output\nibd_df = None  # Will be created inside the function\n\n# Estimate endogamy level from IBD patterns\nprint(\"Estimating endogamy level from IBD patterns...\")\nendogamy_results = estimate_endogamy_level(ibd_df, population_label='european')\n\n# Display endogamy metrics\nprint(f\"\\nEndogamy level: {endogamy_results['endogamy_level']}\")\nprint(f\"Endogamy score: {endogamy_results['endogamy_score']:.3f}\")\nprint(\"\\nKey metrics:\")\nfor metric, value in endogamy_results['metrics'].items():\n    if metric != 'segment_length_quantiles':\n        print(f\"  {metric}: {value:.4f}\" if isinstance(value, float) else f\"  {metric}: {value}\")\n\n# If we have population-specific reference\nif endogamy_results['relative_to_reference']:\n    print(\"\\nRelative to reference population:\")\n    for metric, ratio in endogamy_results['relative_to_reference'].items():\n        print(f\"  {metric}: {ratio:.4f}\")\n\n# Example base likelihoods from a Bonsai analysis\n# In practice, these would come from actual Bonsai output\nbase_likelihoods = {\n    'parent-child': 0.01,\n    'full-sibling': 0.05,\n    'half-sibling': 0.15,\n    'first-cousin': 0.30,\n    'second-cousin': 0.25,\n    'third-cousin': 0.15,\n    'fourth-cousin': 0.05,\n    'unrelated': 0.04\n}\n\n# Adjust likelihoods for population and endogamy\nprint(\"\\nAdjusting relationship likelihoods for population and endogamy...\")\nadjusted_likelihoods = adjust_relationship_likelihoods(\n    base_likelihoods, \n    endogamy_results['endogamy_level'],\n    population='european'\n)\n\n# Compare base and adjusted likelihoods\nprint(\"\\nRelationship likelihoods comparison:\")\nprint(f\"{'Relationship':<25} {'Base':<10} {'Adjusted':<10} {'Change':<10}\")\nprint(\"-\" * 55)\nfor rel in base_likelihoods:\n    base = base_likelihoods[rel]\n    adj = adjusted_likelihoods[rel]\n    change = (adj - base) / base * 100\n    print(f\"{rel:<25} {base:<10.4f} {adj:<10.4f} {change:+<10.1f}%\")\n\n# Generate population-specific priors\nprint(\"\\nGenerating population-specific relationship priors...\")\npopulations = ['european', 'ashkenazi', 'finnish', 'east_asian']\n\n# Base priors for reference - same as in the generate_population_priors function\nbase_priors = {\n    'parent-child': 0.05,\n    'full-sibling': 0.05,\n    'half-sibling': 0.04,\n    'aunt-nephew': 0.04,\n    'grandparent-grandchild': 0.04,\n    'first-cousin': 0.1,\n    'first-cousin-once-removed': 0.1,\n    'second-cousin': 0.1,\n    'second-cousin-once-removed': 0.1,\n    'third-cousin': 0.1,\n    'third-cousin-once-removed': 0.08,\n    'fourth-cousin': 0.08,\n    'unrelated': 0.12\n}\n\nprint(\"\\nRelationship priors by population:\")\n\n# Create a DataFrame to compare priors across populations\nrels = list(base_priors.keys())\npop_priors_df = pd.DataFrame(index=rels)\n\nfor pop in populations:\n    priors = generate_population_priors(pop, endogamy_level=endogamy_results['endogamy_level'])\n    pop_priors_df[pop] = [priors[rel] for rel in rels]\n\n# Display the comparison\ndisplay(pop_priors_df.style.format(\"{:.4f}\").background_gradient(cmap='viridis'))",
   "metadata": {},
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "<cell_type>markdown</cell_type>## Case Study: Population-Specific Analysis\n\nLet's examine a case study showing how population-specific considerations impact relationship inference in a real-world scenario. This case study demonstrates how Bonsai v3 can be configured for optimal results with different populations.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Case study: Comparison of population-specific settings in Bonsai\n# We'll simulate results for two populations: Ashkenazi Jewish and European\n\n# Setup simulated data\nclass SimulatedBonsaiConfig:\n    \"\"\"Simple class to represent Bonsai configuration for a population\"\"\"\n    \n    def __init__(self, population, endogamy_level):\n        self.population = population\n        self.endogamy_level = endogamy_level\n        \n        # Set appropriate parameters based on population\n        if population == 'ashkenazi':\n            self.min_segment_cm = 8.0\n            self.ibd_detection_algorithm = 'hapibd'\n            self.ibd_error_tolerance = 0.005\n            self.pedigree_depth_limit = 6\n            self.relationship_prior_strength = 2.0\n            \n        elif population == 'european':\n            self.min_segment_cm = 7.0\n            self.ibd_detection_algorithm = 'ibis'\n            self.ibd_error_tolerance = 0.01\n            self.pedigree_depth_limit = 4\n            self.relationship_prior_strength = 1.0\n            \n        else:\n            # Default settings\n            self.min_segment_cm = 7.0\n            self.ibd_detection_algorithm = 'ibis'\n            self.ibd_error_tolerance = 0.01\n            self.pedigree_depth_limit = 5\n            self.relationship_prior_strength = 1.0\n    \n    def display(self):\n        \"\"\"Display the configuration settings\"\"\"\n        print(f\"Bonsai Configuration for {self.population.capitalize()} Population\")\n        print(f\"Endogamy Level: {self.endogamy_level}\")\n        print(f\"Minimum IBD Segment (cM): {self.min_segment_cm}\")\n        print(f\"IBD Detection Algorithm: {self.ibd_detection_algorithm}\")\n        print(f\"IBD Error Tolerance: {self.ibd_error_tolerance}\")\n        print(f\"Pedigree Depth Limit: {self.pedigree_depth_limit}\")\n        print(f\"Relationship Prior Strength: {self.relationship_prior_strength}\")\n\n# Create configurations for different populations\nashkenazi_config = SimulatedBonsaiConfig('ashkenazi', 'high')\neuropean_config = SimulatedBonsaiConfig('european', 'low')\n\n# Simulate expected relationship inference outcomes\ndef simulate_relationship_inference(population, relationship, endogamy_level):\n    \"\"\"Simulate relationship inference outcomes for different populations\"\"\"\n    \n    # Base accuracy rates (reference values)\n    # These rates are simplified for the example; real values would be derived from validation\n    base_accuracy = {\n        'parent-child': 0.99,\n        'full-sibling': 0.98,\n        'half-sibling': 0.92,\n        'aunt-nephew': 0.90,\n        'grandparent-grandchild': 0.95,\n        'first-cousin': 0.85,\n        'first-cousin-once-removed': 0.75,\n        'second-cousin': 0.70,\n        'second-cousin-once-removed': 0.60,\n        'third-cousin': 0.50,\n        'third-cousin-once-removed': 0.40,\n        'fourth-cousin': 0.30\n    }\n    \n    # Population-specific modifiers\n    population_modifiers = {\n        'ashkenazi': {\n            'parent-child': 1.0,\n            'full-sibling': 1.0,\n            'half-sibling': 0.95,\n            'aunt-nephew': 0.95,\n            'grandparent-grandchild': 0.95,\n            'first-cousin': 0.9,\n            'first-cousin-once-removed': 0.85,\n            'second-cousin': 0.7,\n            'second-cousin-once-removed': 0.6,\n            'third-cousin': 0.5,\n            'third-cousin-once-removed': 0.4,\n            'fourth-cousin': 0.3\n        },\n        'european': {\n            'parent-child': 1.0,\n            'full-sibling': 1.0,\n            'half-sibling': 1.0,\n            'aunt-nephew': 1.0,\n            'grandparent-grandchild': 1.0,\n            'first-cousin': 1.0,\n            'first-cousin-once-removed': 1.0,\n            'second-cousin': 1.0,\n            'second-cousin-once-removed': 0.95,\n            'third-cousin': 0.9,\n            'third-cousin-once-removed': 0.85,\n            'fourth-cousin': 0.8\n        }\n    }\n    \n    # Endogamy level modifiers\n    endogamy_modifiers = {\n        'high': {\n            'parent-child': 1.0,\n            'full-sibling': 1.0,\n            'half-sibling': 0.95,\n            'aunt-nephew': 0.9,\n            'grandparent-grandchild': 0.9,\n            'first-cousin': 0.85,\n            'first-cousin-once-removed': 0.8,\n            'second-cousin': 0.7,\n            'second-cousin-once-removed': 0.6,\n            'third-cousin': 0.5,\n            'third-cousin-once-removed': 0.4,\n            'fourth-cousin': 0.3\n        },\n        'moderate': {\n            'parent-child': 1.0,\n            'full-sibling': 1.0,\n            'half-sibling': 0.98,\n            'aunt-nephew': 0.95,\n            'grandparent-grandchild': 0.95,\n            'first-cousin': 0.9,\n            'first-cousin-once-removed': 0.85,\n            'second-cousin': 0.8,\n            'second-cousin-once-removed': 0.7,\n            'third-cousin': 0.6,\n            'third-cousin-once-removed': 0.5,\n            'fourth-cousin': 0.4\n        },\n        'low': {\n            'parent-child': 1.0,\n            'full-sibling': 1.0,\n            'half-sibling': 1.0,\n            'aunt-nephew': 1.0,\n            'grandparent-grandchild': 1.0,\n            'first-cousin': 1.0,\n            'first-cousin-once-removed': 0.95,\n            'second-cousin': 0.9,\n            'second-cousin-once-removed': 0.85,\n            'third-cousin': 0.8,\n            'third-cousin-once-removed': 0.7,\n            'fourth-cousin': 0.6\n        }\n    }\n    \n    # Apply modifiers to base accuracy\n    if relationship in base_accuracy:\n        # Get base accuracy\n        accuracy = base_accuracy[relationship]\n        \n        # Apply population modifier if available\n        if population in population_modifiers and relationship in population_modifiers[population]:\n            accuracy *= population_modifiers[population][relationship]\n        \n        # Apply endogamy modifier if available\n        if endogamy_level in endogamy_modifiers and relationship in endogamy_modifiers[endogamy_level]:\n            accuracy *= endogamy_modifiers[endogamy_level][relationship]\n        \n        # Ensure accuracy is between 0 and 1\n        accuracy = min(1.0, max(0.0, accuracy))\n        \n        # Generate random outcomes based on accuracy\n        np.random.seed(hash(population + relationship + endogamy_level) % 10000)\n        n_trials = 100\n        correct_inferences = np.random.binomial(1, accuracy, n_trials).sum()\n        \n        # Calculate percent correct\n        percent_correct = correct_inferences / n_trials * 100\n        \n        return {\n            'accuracy': accuracy,\n            'n_trials': n_trials,\n            'correct_inferences': correct_inferences,\n            'percent_correct': percent_correct\n        }\n    else:\n        return {\n            'accuracy': 0.0,\n            'n_trials': 0,\n            'correct_inferences': 0,\n            'percent_correct': 0.0\n        }\n\n# List of relationships to test\nrelationships = [\n    'parent-child',\n    'full-sibling',\n    'half-sibling',\n    'aunt-nephew',\n    'grandparent-grandchild',\n    'first-cousin',\n    'first-cousin-once-removed',\n    'second-cousin',\n    'second-cousin-once-removed',\n    'third-cousin',\n    'fourth-cousin'\n]\n\n# Simulate inference accuracy for each population and relationship\nprint(\"Simulating relationship inference accuracy for different populations...\")\ninference_results = {'ashkenazi': {}, 'european': {}}\n\nfor pop in ['ashkenazi', 'european']:\n    endogamy = 'high' if pop == 'ashkenazi' else 'low'\n    for rel in relationships:\n        inference_results[pop][rel] = simulate_relationship_inference(pop, rel, endogamy)\n\n# Display the configurations\nprint(\"\\nPopulation-specific Bonsai configurations:\")\nprint(\"-\" * 50)\nashkenazi_config.display()\nprint(\"-\" * 50)\neuropean_config.display()\n\n# Create a DataFrame for comparison\nresults_df = pd.DataFrame(index=relationships, columns=['Ashkenazi Accuracy (%)', 'European Accuracy (%)'])\n\nfor rel in relationships:\n    results_df.loc[rel, 'Ashkenazi Accuracy (%)'] = inference_results['ashkenazi'][rel]['percent_correct']\n    results_df.loc[rel, 'European Accuracy (%)'] = inference_results['european'][rel]['percent_correct']\n\n# Display the results\nprint(\"\\nRelationship Inference Accuracy Comparison:\")\ndisplay(results_df.style.format(\"{:.1f}\").background_gradient(cmap='RdYlGn'))\n\n# Create a horizontal bar chart comparing the two populations\nplt.figure(figsize=(12, 8))\nbar_width = 0.35\nindex = np.arange(len(relationships))\n\nashkenazi_bars = plt.barh(index, results_df['Ashkenazi Accuracy (%)'], bar_width, label='Ashkenazi', color='#3498db')\neuropean_bars = plt.barh(index + bar_width, results_df['European Accuracy (%)'], bar_width, label='European', color='#e74c3c')\n\nplt.ylabel('Relationship Type')\nplt.xlabel('Inference Accuracy (%)')\nplt.title('Relationship Inference Accuracy by Population')\nplt.yticks(index + bar_width/2, relationships)\nplt.xlim(0, 105)\nplt.legend()\n\n# Add value labels on the bars\nfor i, bar in enumerate(ashkenazi_bars):\n    width = bar.get_width()\n    plt.text(width + 1, bar.get_y() + bar.get_height()/2, f'{width:.1f}%', \n             ha='left', va='center', fontsize=9)\n    \nfor i, bar in enumerate(european_bars):\n    width = bar.get_width()\n    plt.text(width + 1, bar.get_y() + bar.get_height()/2, f'{width:.1f}%', \n             ha='left', va='center', fontsize=9)\n\nplt.tight_layout()\nplt.show()\n\n# Key takeaways\nprint(\"\\nKey Takeaways from Population Comparison:\")\nprint(\"1. Close relationships (parent-child, siblings) show high accuracy across populations\")\nprint(\"2. Ashkenazi population shows reduced accuracy for distant relationships due to endogamy\")\nprint(\"3. European population maintains higher accuracy for distant relationships\")\nprint(\"4. Population-specific configuration parameters are essential for optimal results\")\nprint(\"5. Relationship priors should be calibrated based on endogamy level and population\")\nprint(\"6. IBD detection parameters need adjustment based on expected background IBD\")",
   "metadata": {},
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "<cell_type>markdown</cell_type>## Part 4: Validation Strategies\n\nWhen working with real-world genetic datasets, validating relationship inference results is crucial. This section explores strategies for validating Bonsai's output.\n\n### 4.1 Ground Truth Comparison\n\nWhen known pedigrees are available:\n\n- **Complete pedigrees**: Compare inferred relationships to known genealogical records\n- **Partial pedigrees**: Validate relationships for subsets of individuals with known connections\n- **Known relationship pairs**: Test specific relationship types (e.g., known siblings, cousins)\n- **Metrics**: Precision (percentage of inferred relationships that are correct), recall (percentage of true relationships that were detected), F1 score (harmonic mean of precision and recall)\n\n### 4.2 Cross-Validation Techniques\n\nWhen ground truth is limited:\n\n- **K-fold validation**: Divide data into K subsets, use K-1 subsets for training and 1 for testing\n- **Leave-one-out**: Exclude a single individual/pair from inference, then predict their relationships\n- **Bootstrap validation**: Generate multiple datasets by sampling with replacement\n- **Time-split validation**: Use older data to predict newer data (useful for longitudinal datasets)\n\n### 4.3 Hold-Out Testing\n\nFor unbiased evaluation:\n\n- **Hold-out subset**: Set aside a portion of data not used during development\n- **Blind validation**: Relationships unknown to the algorithm developer\n- **External validation**: Test on entirely different datasets\n- **Challenge datasets**: Use publicly available genealogical puzzles or competitions\n\n### 4.4 Consistency Checks\n\nInternal validation approaches:\n\n- **Transitivity checks**: If A is related to B and B is related to C, check consistency of A to C\n- **Genetic trait consistency**: Known genetic traits should segregate consistently through inferred pedigrees\n- **Age consistency**: Ensure parent-child relationships align with birth dates (if available)\n- **Geographic consistency**: Ensure relationships align with historical migration patterns\n- **Multiple algorithm consistency**: Compare results from different relationship inference algorithms",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "## Implementation: Validation Framework\n\nLet's implement a simple validation framework for Bonsai relationship inference results.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Validation framework for Bonsai relationship inference\nfrom sklearn.metrics import precision_recall_fscore_support, confusion_matrix, classification_report\n\nclass BonsaiValidationFramework:\n    \"\"\"Framework for validating Bonsai relationship inference results.\"\"\"\n    \n    def __init__(self, relationship_types=None):\n        \"\"\"Initialize the validation framework.\n        \n        Args:\n            relationship_types (list, optional): List of relationship types to consider.\n                                               If None, will be determined from the data.\n        \"\"\"\n        self.relationship_types = relationship_types\n        self.ground_truth = {}  # {(id1, id2): relationship}\n        self.predictions = {}   # {(id1, id2): relationship}\n        self.results = {}       # Validation results\n    \n    def add_ground_truth(self, id1, id2, relationship):\n        \"\"\"Add a ground truth relationship.\n        \n        Args:\n            id1 (str): ID of the first individual\n            id2 (str): ID of the second individual\n            relationship (str): The true relationship between id1 and id2\n        \"\"\"\n        # Ensure consistent ordering of pairs\n        pair = tuple(sorted([id1, id2]))\n        self.ground_truth[pair] = relationship\n    \n    def add_ground_truth_from_file(self, file_path, format_type='csv'):\n        \"\"\"Load ground truth relationships from a file.\n        \n        Args:\n            file_path (str): Path to the file containing relationships\n            format_type (str): File format ('csv', 'tsv', 'json')\n        \"\"\"\n        if format_type.lower() == 'csv':\n            df = pd.read_csv(file_path)\n            for _, row in df.iterrows():\n                self.add_ground_truth(row['id1'], row['id2'], row['relationship'])\n        elif format_type.lower() == 'tsv':\n            df = pd.read_csv(file_path, sep='\\t')\n            for _, row in df.iterrows():\n                self.add_ground_truth(row['id1'], row['id2'], row['relationship'])\n        elif format_type.lower() == 'json':\n            with open(file_path, 'r') as f:\n                data = json.load(f)\n            for item in data:\n                self.add_ground_truth(item['id1'], item['id2'], item['relationship'])\n        else:\n            raise ValueError(f\"Unsupported format: {format_type}\")\n    \n    def add_prediction(self, id1, id2, relationship):\n        \"\"\"Add a predicted relationship.\n        \n        Args:\n            id1 (str): ID of the first individual\n            id2 (str): ID of the second individual\n            relationship (str): The predicted relationship between id1 and id2\n        \"\"\"\n        # Ensure consistent ordering of pairs\n        pair = tuple(sorted([id1, id2]))\n        self.predictions[pair] = relationship\n    \n    def add_predictions_from_file(self, file_path, format_type='csv'):\n        \"\"\"Load predicted relationships from a file.\n        \n        Args:\n            file_path (str): Path to the file containing relationships\n            format_type (str): File format ('csv', 'tsv', 'json')\n        \"\"\"\n        if format_type.lower() == 'csv':\n            df = pd.read_csv(file_path)\n            for _, row in df.iterrows():\n                self.add_prediction(row['id1'], row['id2'], row['relationship'])\n        elif format_type.lower() == 'tsv':\n            df = pd.read_csv(file_path, sep='\\t')\n            for _, row in df.iterrows():\n                self.add_prediction(row['id1'], row['id2'], row['relationship'])\n        elif format_type.lower() == 'json':\n            with open(file_path, 'r') as f:\n                data = json.load(f)\n            for item in data:\n                self.add_prediction(item['id1'], item['id2'], item['relationship'])\n        else:\n            raise ValueError(f\"Unsupported format: {format_type}\")\n    \n    def compute_metrics(self):\n        \"\"\"Compute validation metrics.\"\"\"\n        # Find common pairs\n        common_pairs = set(self.ground_truth.keys()) & set(self.predictions.keys())\n        \n        if not common_pairs:\n            print(\"No common pairs found between ground truth and predictions.\")\n            return None\n        \n        # Extract relationships for common pairs\n        y_true = [self.ground_truth[pair] for pair in common_pairs]\n        y_pred = [self.predictions[pair] for pair in common_pairs]\n        \n        # Determine relationship types if not provided\n        if self.relationship_types is None:\n            self.relationship_types = sorted(list(set(y_true + y_pred)))\n        \n        # Compute precision, recall, F1 score\n        precision, recall, f1, support = precision_recall_fscore_support(\n            y_true, y_pred, labels=self.relationship_types, average=None\n        )\n        \n        # Compute macro averages\n        macro_precision, macro_recall, macro_f1, _ = precision_recall_fscore_support(\n            y_true, y_pred, labels=self.relationship_types, average='macro'\n        )\n        \n        # Compute weighted averages\n        weighted_precision, weighted_recall, weighted_f1, _ = precision_recall_fscore_support(\n            y_true, y_pred, labels=self.relationship_types, average='weighted'\n        )\n        \n        # Create results dictionary\n        self.results = {\n            'by_relationship': {\n                rel: {\n                    'precision': prec,\n                    'recall': rec,\n                    'f1': f1_score,\n                    'support': sup\n                }\n                for rel, prec, rec, f1_score, sup in zip(\n                    self.relationship_types, precision, recall, f1, support\n                )\n            },\n            'macro_avg': {\n                'precision': macro_precision,\n                'recall': macro_recall,\n                'f1': macro_f1\n            },\n            'weighted_avg': {\n                'precision': weighted_precision,\n                'recall': weighted_recall,\n                'f1': weighted_f1\n            },\n            'n_common_pairs': len(common_pairs),\n            'n_ground_truth': len(self.ground_truth),\n            'n_predictions': len(self.predictions),\n            'coverage': len(common_pairs) / len(self.ground_truth) if self.ground_truth else 0\n        }\n        \n        # Compute confusion matrix\n        self.results['confusion_matrix'] = confusion_matrix(\n            y_true, y_pred, labels=self.relationship_types\n        )\n        \n        return self.results\n    \n    def check_transitivity(self):\n        \"\"\"Check transitivity in predictions (A related to B, B to C => check A to C).\"\"\"\n        # This is a simplified implementation that checks basic transitivity\n        transitivity_results = {'valid': 0, 'invalid': 0, 'checks': 0}\n        \n        # Create a dictionary of all individuals and their relationships\n        individual_relationships = {}\n        for (id1, id2), rel in self.predictions.items():\n            if id1 not in individual_relationships:\n                individual_relationships[id1] = {}\n            if id2 not in individual_relationships:\n                individual_relationships[id2] = {}\n            \n            individual_relationships[id1][id2] = rel\n            individual_relationships[id2][id1] = rel\n        \n        # Define symmetric relationship types (same in both directions)\n        symmetric_relationships = {\n            'full-sibling', 'half-sibling', 'first-cousin', 'second-cousin', \n            'third-cousin', 'fourth-cousin'\n        }\n        \n        # Define transitive relationship rules (simplified)\n        # Format: (rel_A_B, rel_B_C) -> expected_rel_A_C\n        transitive_rules = {\n            ('parent-child', 'parent-child'): 'grandparent-grandchild',\n            ('full-sibling', 'parent-child'): 'aunt-nephew',\n            ('parent-child', 'full-sibling'): 'aunt-nephew',\n            ('parent-child', 'aunt-nephew'): 'first-cousin',\n            ('full-sibling', 'full-sibling'): 'full-sibling',\n            ('first-cousin', 'parent-child'): 'first-cousin-once-removed',\n            ('parent-child', 'first-cousin'): 'first-cousin-once-removed'\n        }\n        \n        # Check transitivity for all possible triplets\n        individuals = list(individual_relationships.keys())\n        for i in range(len(individuals)):\n            for j in range(len(individuals)):\n                if i == j:\n                    continue\n                    \n                id_A = individuals[i]\n                id_B = individuals[j]\n                \n                # Skip if A and B are not related\n                if id_B not in individual_relationships[id_A]:\n                    continue\n                    \n                rel_A_B = individual_relationships[id_A][id_B]\n                \n                for k in range(len(individuals)):\n                    if k == i or k == j:\n                        continue\n                        \n                    id_C = individuals[k]\n                    \n                    # Skip if B and C are not related\n                    if id_C not in individual_relationships[id_B]:\n                        continue\n                        \n                    rel_B_C = individual_relationships[id_B][id_C]\n                    \n                    # Skip if A and C are not related in our predictions\n                    if id_C not in individual_relationships[id_A]:\n                        continue\n                        \n                    rel_A_C = individual_relationships[id_A][id_C]\n                    \n                    # Check if this triplet matches a transitivity rule\n                    if (rel_A_B, rel_B_C) in transitive_rules:\n                        expected_rel_A_C = transitive_rules[(rel_A_B, rel_B_C)]\n                        \n                        transitivity_results['checks'] += 1\n                        if rel_A_C == expected_rel_A_C:\n                            transitivity_results['valid'] += 1\n                        else:\n                            transitivity_results['invalid'] += 1\n                            # Record the invalid triplet\n                            if 'invalid_triplets' not in transitivity_results:\n                                transitivity_results['invalid_triplets'] = []\n                            transitivity_results['invalid_triplets'].append({\n                                'ids': (id_A, id_B, id_C),\n                                'relationships': (rel_A_B, rel_B_C, rel_A_C),\n                                'expected_A_C': expected_rel_A_C\n                            })\n        \n        # Calculate transitivity validity percentage\n        if transitivity_results['checks'] > 0:\n            transitivity_results['validity_percent'] = (\n                transitivity_results['valid'] / transitivity_results['checks'] * 100\n            )\n        else:\n            transitivity_results['validity_percent'] = float('nan')\n        \n        self.results['transitivity'] = transitivity_results\n        return transitivity_results\n    \n    def display_results(self, include_confusion_matrix=True):\n        \"\"\"Display validation results in a readable format.\"\"\"\n        if not self.results:\n            print(\"No results available. Run compute_metrics() first.\")\n            return\n        \n        print(\"=== Bonsai Relationship Inference Validation Results ===\")\n        print(f\"Total ground truth pairs: {self.results['n_ground_truth']}\")\n        print(f\"Total predicted pairs: {self.results['n_predictions']}\")\n        print(f\"Common pairs for validation: {self.results['n_common_pairs']}\")\n        print(f\"Coverage: {self.results['coverage']:.2%}\")\n        \n        print(\"\\nAggregate Metrics:\")\n        print(f\"Macro Precision: {self.results['macro_avg']['precision']:.4f}\")\n        print(f\"Macro Recall: {self.results['macro_avg']['recall']:.4f}\")\n        print(f\"Macro F1 Score: {self.results['macro_avg']['f1']:.4f}\")\n        \n        print(\"\\nMetrics by Relationship Type:\")\n        metrics_df = pd.DataFrame(\n            {rel: {\n                'precision': self.results['by_relationship'][rel]['precision'],\n                'recall': self.results['by_relationship'][rel]['recall'],\n                'f1': self.results['by_relationship'][rel]['f1'],\n                'support': self.results['by_relationship'][rel]['support']\n            } for rel in self.results['by_relationship']}\n        ).transpose()\n        \n        display(metrics_df.style.format({'precision': '{:.4f}', 'recall': '{:.4f}', 'f1': '{:.4f}', 'support': '{:d}'})\n                .background_gradient(subset=['precision', 'recall', 'f1'], cmap='YlGn'))\n        \n        if include_confusion_matrix and 'confusion_matrix' in self.results:\n            cm = self.results['confusion_matrix']\n            \n            plt.figure(figsize=(10, 8))\n            sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n                       xticklabels=self.relationship_types,\n                       yticklabels=self.relationship_types)\n            plt.xlabel('Predicted')\n            plt.ylabel('True')\n            plt.title('Confusion Matrix')\n            plt.tight_layout()\n            plt.show()\n        \n        # Display transitivity results if available\n        if 'transitivity' in self.results:\n            trans = self.results['transitivity']\n            print(\"\\nTransitivity Check Results:\")\n            print(f\"Total checks: {trans['checks']}\")\n            print(f\"Valid transitivity: {trans['valid']}\")\n            print(f\"Invalid transitivity: {trans['invalid']}\")\n            if 'validity_percent' in trans:\n                print(f\"Transitivity validity: {trans['validity_percent']:.2f}%\")\n            \n            if 'invalid_triplets' in trans and trans['invalid_triplets']:\n                print(\"\\nSample Invalid Triplets:\")\n                for i, triplet in enumerate(trans['invalid_triplets'][:5]):  # Show first 5\n                    print(f\"  {i+1}. {triplet['ids'][0]} -> {triplet['ids'][1]} -> {triplet['ids'][2]}\")\n                    print(f\"     Relationships: {triplet['relationships'][0]} -> {triplet['relationships'][1]} -> {triplet['relationships'][2]}\")\n                    print(f\"     Expected {triplet['ids'][0]} -> {triplet['ids'][2]}: {triplet['expected_A_C']}\")\n                \n                if len(trans['invalid_triplets']) > 5:\n                    print(f\"  ... and {len(trans['invalid_triplets']) - 5} more.\")\n    \n    def plot_metrics_by_relationship(self):\n        \"\"\"Plot precision, recall, and F1 score for each relationship type.\"\"\"\n        if not self.results:\n            print(\"No results available. Run compute_metrics() first.\")\n            return\n        \n        # Extract data for plotting\n        rels = list(self.results['by_relationship'].keys())\n        precision = [self.results['by_relationship'][rel]['precision'] for rel in rels]\n        recall = [self.results['by_relationship'][rel]['recall'] for rel in rels]\n        f1 = [self.results['by_relationship'][rel]['f1'] for rel in rels]\n        support = [self.results['by_relationship'][rel]['support'] for rel in rels]\n        \n        # Create a figure with 2 subplots\n        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n        \n        # Plot precision, recall, and F1 score\n        x = np.arange(len(rels))\n        width = 0.25\n        \n        ax1.bar(x - width, precision, width, label='Precision')\n        ax1.bar(x, recall, width, label='Recall')\n        ax1.bar(x + width, f1, width, label='F1 Score')\n        \n        ax1.set_ylabel('Score')\n        ax1.set_title('Precision, Recall, and F1 Score by Relationship Type')\n        ax1.set_xticks(x)\n        ax1.set_xticklabels(rels, rotation=45, ha='right')\n        ax1.legend()\n        ax1.set_ylim(0, 1.1)\n        \n        # Plot support (number of samples)\n        ax2.bar(rels, support, color='skyblue')\n        ax2.set_ylabel('Number of Samples')\n        ax2.set_title('Number of Samples by Relationship Type')\n        ax2.set_xticklabels(rels, rotation=45, ha='right')\n        \n        plt.tight_layout()\n        plt.show()\n        \n        return fig",
   "metadata": {},
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Demonstration of the validation framework with simulated data\n\n# Define relationship types\nrelationship_types = [\n    'parent-child',\n    'full-sibling',\n    'half-sibling',\n    'aunt-nephew',\n    'grandparent-grandchild',\n    'first-cousin',\n    'second-cousin',\n    'third-cousin',\n    'fourth-cousin',\n    'unrelated'\n]\n\n# Initialize the validation framework\nvalidator = BonsaiValidationFramework(relationship_types=relationship_types)\n\n# Generate simulated ground truth and predictions\nnp.random.seed(42)\nn_pairs = 200\nindividual_ids = [f\"ind_{i}\" for i in range(50)]\n\n# Simulate ground truth relationships\nfor _ in range(n_pairs):\n    id1, id2 = np.random.choice(individual_ids, 2, replace=False)\n    rel = np.random.choice(relationship_types)\n    validator.add_ground_truth(id1, id2, rel)\n\n# Simulate predictions with varying accuracy\n# Close relationships have higher accuracy\nrelationship_accuracy = {\n    'parent-child': 0.95,\n    'full-sibling': 0.90,\n    'half-sibling': 0.85,\n    'aunt-nephew': 0.80,\n    'grandparent-grandchild': 0.85,\n    'first-cousin': 0.75,\n    'second-cousin': 0.65,\n    'third-cousin': 0.55,\n    'fourth-cousin': 0.45,\n    'unrelated': 0.70\n}\n\n# Add predictions with simulated errors\nfor pair, true_rel in validator.ground_truth.items():\n    # With probability equal to relationship accuracy, predict correctly\n    if np.random.random() < relationship_accuracy[true_rel]:\n        pred_rel = true_rel\n    else:\n        # Otherwise, predict a random relationship (error)\n        other_rels = [r for r in relationship_types if r != true_rel]\n        pred_rel = np.random.choice(other_rels)\n    \n    # Add the prediction\n    validator.add_prediction(pair[0], pair[1], pred_rel)\n\n# Add some extra predictions not in ground truth\nfor _ in range(50):\n    id1, id2 = np.random.choice(individual_ids, 2, replace=False)\n    pair = tuple(sorted([id1, id2]))\n    \n    # Skip if already in ground truth\n    if pair in validator.ground_truth:\n        continue\n    \n    rel = np.random.choice(relationship_types)\n    validator.add_prediction(id1, id2, rel)\n\n# Compute validation metrics\nprint(\"Computing validation metrics...\")\nvalidator.compute_metrics()\n\n# Check transitivity\nprint(\"Checking transitivity...\")\nvalidator.check_transitivity()\n\n# Display the results\nvalidator.display_results()\n\n# Plot metrics by relationship type\nvalidator.plot_metrics_by_relationship()",
   "metadata": {},
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "<cell_type>markdown</cell_type>## Summary\n\nIn this lab, we explored the application of Bonsai v3 to real-world genetic datasets, examining practical challenges and adaptation strategies. Key points covered include:\n\n1. **Types of real-world genetic datasets**: We examined different sources of genetic data (DTC testing, research cohorts, historical datasets) and their characteristics.\n\n2. **Data preparation challenges**: We implemented a pipeline to handle format conversion, quality control, phasing, and IBD detection for various data types.\n\n3. **Population-specific considerations**: We explored how population structure affects relationship inference and implemented population-aware adjustments for Bonsai.\n\n4. **Validation strategies**: We developed a validation framework to assess the accuracy of relationship inference through metrics like precision, recall, and transitivity.\n\nThese tools and approaches enable effective application of Bonsai v3 to diverse real-world datasets, accounting for the complexities encountered in practical genetic genealogy.",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "<cell_type>markdown</cell_type>## Self-Assessment\n\n**1. What factors distinguish direct-to-consumer (DTC) genetic data from research cohort data?**\n<details>\n<summary>Click to see answer</summary>\n\nDTC genetic data differs from research cohort data in several key ways:\n- **Coverage**: DTC tests typically use genotyping arrays with 500,000-1 million SNPs, while research often includes whole genome sequencing\n- **Quality control**: Research data generally undergoes more rigorous QC\n- **Metadata**: DTC relies on self-reported information, while research cohorts have clinically verified data\n- **Format**: DTC data comes in proprietary formats, research data typically uses standard formats like VCF\n- **Sample selection**: DTC data represents self-selected consumers, research cohorts are selected based on study design\n</details>\n\n**2. What are the main data preparation steps needed before applying Bonsai to genetic data?**\n<details>\n<summary>Click to see answer</summary>\n\nMain data preparation steps include:\n1. **Format conversion**: Converting between formats (e.g., 23andMe text to VCF)\n2. **Quality control**: Filtering for variant quality, sample quality, missingness, etc.\n3. **Phasing**: Determining haplotype phase for accurate IBD detection\n4. **IBD detection**: Running appropriate IBD detection algorithms\n5. **Parameter adjustment**: Tuning IBD parameters for dataset characteristics\n6. **Data harmonization**: Ensuring reference genome compatibility and consistent marker sets\n7. **Metadata preparation**: Organizing age, sex, and other information for relationship inference\n</details>\n\n**3. How does endogamy impact the accuracy of relationship inference?**\n<details>\n<summary>Click to see answer</summary>\n\nEndogamy impacts relationship inference in several ways:\n- **Elevated background IBD**: Higher levels of shared IBD between nominally unrelated individuals\n- **Multiple relationship paths**: Individuals may be related through multiple common ancestors\n- **Relationship classification difficulty**: Harder to distinguish between different relationship types\n- **False positives**: Increased risk of inferring relationships that don't exist\n- **Confidence reduction**: Lower confidence in distant relationship classifications\n- **Parameter adjustment needs**: Requires adjusting IBD thresholds and relationship priors\n- **Transitivity issues**: Simple transitivity rules may not hold in highly endogamous populations\n</details>\n\n**4. What metrics should you use to validate relationship inference results?**\n<details>\n<summary>Click to see answer</summary>\n\nKey validation metrics include:\n- **Precision**: Percentage of inferred relationships that are correct\n- **Recall**: Percentage of true relationships that were detected\n- **F1 score**: Harmonic mean of precision and recall\n- **Confusion matrix**: Shows the pattern of relationship classification errors\n- **Transitivity validity**: Percentage of relationship triplets that follow expected transitivity rules\n- **Coverage**: Percentage of ground truth relationships that were predicted\n- **Relationship-specific metrics**: Performance broken down by relationship type\n- **ROC and PR curves**: For evaluating performance across different threshold settings\n</details>\n\n**5. What population-specific adjustments should be made to Bonsai for optimal performance?**\n<details>\n<summary>Click to see answer</summary>\n\nPopulation-specific adjustments include:\n- **IBD detection parameters**: Adjust minimum segment length and error tolerance \n- **Relationship priors**: Modify prior probabilities based on known population structure\n- **Endogamy correction**: Apply population-specific endogamy models\n- **Age model parameters**: Adjust generation time estimates based on cultural factors\n- **Relationship likelihood functions**: Calibrate expected IBD distributions for the population\n- **Pedigree depth limits**: Adjust based on background IBD levels\n- **Transitivity rules**: Modify for population-specific family structure patterns\n- **Population-specific reference datasets**: Use appropriate reference data for phasing and imputation\n</details>\n\n**6. How would you handle a dataset with high levels of missing data?**\n<details>\n<summary>Click to see answer</summary>\n\nStrategies for handling high levels of missing data:\n1. **Imputation**: Use reference panels to statistically infer missing genotypes\n2. **Filtering**: Remove variants/samples with excessive missingness\n3. **Weighted analysis**: Downweight evidence from regions with high missingness\n4. **Confidence adjustment**: Reduce relationship confidence scores proportionally to missingness\n5. **Missing data simulation**: Test impact by simulating different levels of missingness\n6. **Platform-aware approaches**: Account for systematic platform differences\n7. **Marker selection**: Focus on high-coverage markers shared across samples\n8. **Region-specific analysis**: Analyze genome regions with good coverage separately\n</details>",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "<cell_type>markdown</cell_type>## Further Reading\n\nTo deepen your understanding of real-world dataset considerations in genetic genealogy, explore these resources:\n\n1. Browning, S. R., & Browning, B. L. (2020). Genetic phasing and imputation. Annual Review of Genomics and Human Genetics, 21, 285-308.\n\n2. Skov, L., & Schierup, M. H. (2017). Analysis of 62 hybrid assembled human Y chromosomes exposes rapid structural changes and high rates of gene conversion. PLoS Genetics, 13(8), e1006834.\n\n3. Carmi, S., Hui, K. Y., Kochav, E., Liu, X., Xue, J., Grady, F., ... & Pe'er, I. (2014). Sequencing an Ashkenazi reference panel supports population-targeted personal genomics and illuminates Jewish and European origins. Nature Communications, 5(1), 4835.\n\n4. Bycroft, C., Freeman, C., Petkova, D., Band, G., Elliott, L. T., Sharp, K., ... & Marchini, J. (2018). The UK Biobank resource with deep phenotyping and genomic data. Nature, 562(7726), 203-209.\n\n5. Diroma, M. A., Cinnirella, F., Pesole, G., & Picardi, E. (2020). Investigating population structure and genealogical relationships in human mitochondrial DNA variation: a comprehensive review. Nucleic Acids Research, 48(19), 10545-10566.\n\n6. Ramstetter, M. D., Dyer, T. D., Lehman, D. M., G√∂ring, H. H., Curran, J. E., Duggirala, R., ... & Williams, A. L. (2017). Benchmarking relatedness inference methods with genome-wide data from thousands of relatives. Genetics, 207(1), 75-82.\n\n7. Ball, C. A., Barber, M. J., Byrnes, J., Carbonetto, P., Chahine, K. G., Curtis, R. E., ... & Wang, Y. (2020). AncestryDNA solves for identity. Nature Communications, 11(1), 2711.",
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}