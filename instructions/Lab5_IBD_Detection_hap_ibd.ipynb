{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from collections import Counter\n",
    "import logging\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import subprocess\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from IPython.display import display, HTML\n",
    "import IPython\n",
    "import pandas as pd\n",
    "import boto3\n",
    "\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_comp_gen_dir():\n",
    "    \"\"\"Find the computational_genetic_genealogy directory by searching up from current directory.\"\"\"\n",
    "    current = Path.cwd()\n",
    "    \n",
    "    # Search up through parent directories\n",
    "    while current != current.parent:\n",
    "        # Check if target directory exists in current path\n",
    "        target = current / 'computational_genetic_genealogy'\n",
    "        if target.is_dir():\n",
    "            return target\n",
    "        # Move up one directory\n",
    "        current = current.parent\n",
    "    \n",
    "    raise FileNotFoundError(\"Could not find computational_genetic_genealogy directory\")\n",
    "\n",
    "def load_env_file():\n",
    "    \"\"\"Find and load the .env file from the computational_genetic_genealogy directory.\"\"\"\n",
    "    try:\n",
    "        # Find the computational_genetic_genealogy directory\n",
    "        comp_gen_dir = find_comp_gen_dir()\n",
    "        \n",
    "        # Look for .env file\n",
    "        env_path = comp_gen_dir / '.env'\n",
    "        if not env_path.exists():\n",
    "            print(f\"Warning: No .env file found in {comp_gen_dir}\")\n",
    "            return None\n",
    "        \n",
    "        # Load the .env file\n",
    "        load_dotenv(env_path, override=True)\n",
    "        print(f\"Loaded environment variables from: {env_path}\")\n",
    "        return env_path\n",
    "        \n",
    "    except FileNotFoundError as e:\n",
    "        print(f\"Error: {e}\")\n",
    "        return None\n",
    "\n",
    "# Use the function\n",
    "env_path = load_env_file()\n",
    "\n",
    "working_directory = os.getenv('PROJECT_WORKING_DIR', default=None)\n",
    "data_directory = os.getenv('PROJECT_DATA_DIR', default=None)\n",
    "references_directory = os.getenv('PROJECT_REFERENCES_DIR', default=None)\n",
    "results_directory = os.getenv('PROJECT_RESULTS_DIR', default=None)\n",
    "utils_directory = os.getenv('PROJECT_UTILS_DIR', default=None)\n",
    "\n",
    "os.environ[\"WORKING_DIRECTORY\"] = working_directory\n",
    "os.environ[\"DATA_DIRECTORY\"] = data_directory\n",
    "os.environ[\"REFERENCES_DIRECTORY\"] = references_directory\n",
    "os.environ[\"RESULTS_DIRECTORY\"] = results_directory\n",
    "os.environ[\"UTILS_DIRECTORY\"] = utils_directory\n",
    "\n",
    "print(f\"Working Directory: {working_directory}\")\n",
    "print(f\"Data Directory: {data_directory}\")\n",
    "print(f\"References Directory: {references_directory}\")\n",
    "print(f\"Results Directory: {results_directory}\")\n",
    "print(f\"Utils Directory: {utils_directory}\")\n",
    "\n",
    "os.chdir(working_directory)\n",
    "print(f\"The current directory is {os.getcwd()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def configure_logging(log_filename, log_file_debug_level=\"INFO\", console_debug_level=\"INFO\"):\n",
    "    \"\"\"\n",
    "    Configure logging for both file and console handlers.\n",
    "\n",
    "    Args:\n",
    "        log_filename (str): Path to the log file where logs will be written.\n",
    "        log_file_debug_level (str): Logging level for the file handler.\n",
    "        console_debug_level (str): Logging level for the console handler.\n",
    "    \"\"\"\n",
    "    # Create a root logger\n",
    "    logger = logging.getLogger()\n",
    "    logger.setLevel(logging.DEBUG)  # Capture all messages at the root level\n",
    "\n",
    "    # Convert level names to numeric levels\n",
    "    file_level = getattr(logging, log_file_debug_level.upper(), logging.INFO)\n",
    "    console_level = getattr(logging, console_debug_level.upper(), logging.INFO)\n",
    "\n",
    "    # File handler: Logs messages at file_level and above to the file\n",
    "    file_handler = logging.FileHandler(log_filename)\n",
    "    file_handler.setLevel(file_level)\n",
    "    file_formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')\n",
    "    file_handler.setFormatter(file_formatter)\n",
    "\n",
    "    # Console handler: Logs messages at console_level and above to the console\n",
    "    console_handler = logging.StreamHandler(sys.stdout)\n",
    "    console_handler.setLevel(console_level)\n",
    "    console_formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')\n",
    "    console_handler.setFormatter(console_formatter)\n",
    "\n",
    "    # Add handlers to the root logger\n",
    "    logger.addHandler(file_handler)\n",
    "    logger.addHandler(console_handler)\n",
    "    \n",
    "def clear_logger():\n",
    "    \"\"\"Remove all handlers from the root logger.\"\"\"\n",
    "    logger = logging.getLogger()\n",
    "    for handler in logger.handlers[:]:\n",
    "        logger.removeHandler(handler)\n",
    "        \n",
    "log_filename = os.path.join(results_directory, \"lab6.log\")\n",
    "print(f\"The Lab 6 log file is located at {log_filename}.\")\n",
    "\n",
    "# Ensure the results_directory exists\n",
    "if not os.path.exists(results_directory):\n",
    "    os.makedirs(results_directory)\n",
    "\n",
    "# Check if the file exists; if not, create it\n",
    "if not os.path.exists(log_filename):\n",
    "    with open(log_filename, 'w') as file:\n",
    "        pass  # The file is now created.\n",
    "    \n",
    "clear_logger() # Clear the logger before reconfiguring it\n",
    "configure_logging(log_filename, log_file_debug_level=\"INFO\", console_debug_level=\"INFO\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the hap-IBD Detection Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Select your VCF file**\n",
    "\n",
    "In the next cell, uncomment the file you want to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vcf_file = os.path.join(results_directory, \"merged_sample_autosomes_unphased.vcf.gz\")\n",
    "# vcf_directory = os.path.join(results_directory, \"real_data_autosomes\")\n",
    "\n",
    "vcf_file = os.path.join(results_directory, \"ped_sim_run2_autosomes.vcf.gz\")\n",
    "vcf_directory = os.path.join(results_directory, \"ped_sim_run2_autosomes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash -s \"$vcf_file\" \"$vcf_directory\"\n",
    "\n",
    "vcf_file=\"$1\"\n",
    "vcf_directory=\"$2\"\n",
    "\n",
    "# Define the hap-IBD executable path\n",
    "hap_ibd=\"${UTILS_DIRECTORY}/hap-ibd.jar\"\n",
    "\n",
    "# Ensure the hap-IBD executable exists\n",
    "if [[ ! -f \"${hap_ibd}\" ]]; then\n",
    "    echo \"Error: Hap-IBD executable not found: ${hap_ibd}\" >&2\n",
    "fi\n",
    "\n",
    "# Extract the file prefix (removing .vcf.gz extension)\n",
    "output_prefix=\"${vcf_file%.vcf.gz}\"\n",
    "# Get base name of the VCF file\n",
    "base_name=$(basename \"$output_prefix\")\n",
    "# base_name remove \"_autosomes\" suffixif present\n",
    "base_name=${base_name/_autosomes/}\n",
    "\n",
    "\n",
    "# Create the output directories\n",
    "mkdir -p \"${vcf_directory}/segments\"\n",
    "mkdir -p \"${vcf_directory}/phased_samples\"\n",
    "\n",
    "# Run hap-IBD analysis in loop by chromosome\n",
    "for chr in {1..22}; do\n",
    "    data_file=\"${vcf_directory}/phased_samples/${base_name}_phased_chr${chr}.vcf.gz\"\n",
    "    echo \"data_file: ${data_file}\"\n",
    "    if [[ ! -f \"${data_file}\" ]]; then\n",
    "        echo \"No matching VCF file found\" >&2\n",
    "        exit 1\n",
    "    fi\n",
    "\n",
    "    java -jar \"${hap_ibd}\" gt=\"${data_file}\" \\\n",
    "        map=\"${REFERENCES_DIRECTORY}/genetic_maps/beagle_genetic_maps/plink.chr${chr}.GRCh38.map\" \\\n",
    "        out=\"${vcf_directory}/segments/${base_name}_hapibd_chr${chr}.seg\" \\\n",
    "        nthreads=4\n",
    "done\n",
    "\n",
    "\n",
    "if [[ $? -eq 0 ]]; then\n",
    "    echo \"hap-IBD analysis completed successfully.\"\n",
    "else\n",
    "    echo \"Error running hap-IBD analysis.\" >&2\n",
    "fi\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash -s \"$vcf_file\" \"$vcf_directory\"\n",
    "\n",
    "vcf_file=\"$1\"\n",
    "vcf_directory=\"$2\"\n",
    "\n",
    "# Define the hap-IBD executable path\n",
    "hap_ibd=\"${UTILS_DIRECTORY}/hap-ibd.jar\"\n",
    "\n",
    "# Ensure the hap-IBD executable exists\n",
    "if [[ ! -f \"${hap_ibd}\" ]]; then\n",
    "    echo \"Error: Hap-IBD executable not found: ${hap_ibd}\" >&2\n",
    "fi\n",
    "\n",
    "# Extract the file prefix (removing .vcf.gz extension)\n",
    "output_prefix=\"${vcf_file%.vcf.gz}\"\n",
    "# Get base name of the VCF file\n",
    "base_name=$(basename \"$output_prefix\")\n",
    "# base_name remove \"_autosomes\" suffixif present\n",
    "base_name=${base_name/_autosomes/}\n",
    "\n",
    "# Create the output directories\n",
    "mkdir -p \"${vcf_directory}/segments\"\n",
    "mkdir -p \"${vcf_directory}/phased_samples\"\n",
    "\n",
    "# Create or empty the merged output file\n",
    ": > \"${vcf_directory}/segments/${base_name}_autosomes_hapibd.seg\"\n",
    "\n",
    "for chr in {1..22}; do\n",
    "    file1=\"${vcf_directory}/segments/${base_name}_hapibd_chr${chr}.seg.ibd.gz\"\n",
    "    if [[ -f \"${file1}\" ]]; then\n",
    "        zcat \"${file1}\" >> \"${vcf_directory}/segments/${base_name}_autosomes_hapibd.seg\"\n",
    "    else\n",
    "        echo \"Warning: File for chromosome ${chr} not found during concatenation.\" >&2\n",
    "    fi\n",
    "done\n",
    "\n",
    "# Remove temporary files using a loop\n",
    "for chr in {1..22}; do\n",
    "    rm -f \"${vcf_directory}/segments/${base_name}_hapibd_chr${chr}.seg.ibd.gz\"\n",
    "    rm -f \"${vcf_directory}/segments/${base_name}_hapibd_chr${chr}.seg.hbd.gz\"\n",
    "    rm -f \"${vcf_directory}/segments/${base_name}_hapibd_chr${chr}.seg.log\"\n",
    "done"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explore The Segments Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prefix = vcf_file.split(\".vcf.gz\")[0]\n",
    "print(prefix)\n",
    "base_name = os.path.basename(prefix)\n",
    "print(base_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to segments file\n",
    "segments = os.path.join(results_directory, f\"{base_name}/segments/{base_name}_hapibd.seg\")\n",
    "print(f\"The segments file is located at {segments}\")\n",
    "\n",
    "segments_temp = pd.read_csv(segments, sep=\"\\t\", header=None)\n",
    "segments_temp.columns = [\n",
    "    \"id1\", \"sample1_haplotype\", \"id2\", \"sample2_haplotype\",\n",
    "    \"chrom\", \"phys_start_pos\", \"phys_end_pos\", \n",
    "    \"genetic_length\"\n",
    "    ]\n",
    "segments = segments_temp.sort_values(\n",
    "    by=[\"chrom\", \"phys_start_pos\", \"phys_end_pos\"],\n",
    "    ascending=[True, True, True]\n",
    ")\n",
    "segments = segments.reset_index(drop=True)\n",
    "output_file = os.path.join(results_directory, \"merged_opensnps_autosomes_hapibd.csv\")\n",
    "segments.to_csv(output_file, sep=\"\\t\", index=False, header=False)\n",
    "segments.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "segments.head() # You can enter a number greater than 5 to view more rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "segments[\"genetic_length\"].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter segments on min_length=7, min_markers=436, max_error_density=0.004,\n",
    "\n",
    "filtered_segments_7cM = segments[segments[\"genetic_length\"] >= 7].copy()\n",
    "\n",
    "filtered_segments_7cM[\"genetic_length\"].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_segments_20cM = segments[segments[\"genetic_length\"] >= 20].copy()\n",
    "filtered_segments_20cM[\"genetic_length\"].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's aggregate the data by pairs instead of looking at it by segment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# First ensure id1 and id2 are consistently ordered\n",
    "filtered_segments_7cM[[\"id1\", \"id2\"]] = filtered_segments_7cM.apply(\n",
    "    lambda row: pd.Series((row[\"id1\"], row[\"id2\"])) if row[\"id1\"] < row[\"id2\"] \n",
    "    else pd.Series((row[\"id2\"], row[\"id1\"])), axis=1\n",
    ")\n",
    "\n",
    "pair_counts = filtered_segments_7cM.groupby([\"id1\", \"id2\"]).size().reset_index(name=\"pair_count\")\n",
    "pair_count_distribution = pair_counts[\"pair_count\"].value_counts().reset_index()\n",
    "pair_count_distribution.columns = [\"Number of Segments\", \"Number of Pairs\"]\n",
    "pair_count_distribution = pair_count_distribution.reset_index(drop=True)\n",
    "display(pair_count_distribution.style.hide(axis=\"index\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by id pairs and calculate all metrics at once\n",
    "aggregated_segments = filtered_segments_7cM.groupby([\"id1\", \"id2\"]).agg(\n",
    "    total_genetic_length=(\"genetic_length\", \"sum\"),\n",
    "    num_segments=(\"genetic_length\", \"count\"),\n",
    "    largest_segment=(\"genetic_length\", \"max\")\n",
    ").reset_index()\n",
    "\n",
    "# Check distribution of values\n",
    "display(aggregated_segments.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Define columns to plot\n",
    "columns = [\"total_genetic_length\", \"num_segments\", \"largest_segment\"]\n",
    "\n",
    "# Create subplots\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "# Histogram for each metric\n",
    "for i, col in enumerate(columns):\n",
    "    sns.histplot(aggregated_segments[col], bins=30, kde=True, ax=axes[i], edgecolor=\"black\")\n",
    "    axes[i].set_title(f\"Distribution of {col.replace('_', ' ').title()}\")\n",
    "    axes[i].set_xlabel(col.replace('_', ' ').title())\n",
    "    axes[i].set_ylabel(\"Frequency\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Box Plot\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "for i, col in enumerate(columns):\n",
    "    sns.boxplot(y=aggregated_segments[col], ax=axes[i])\n",
    "    axes[i].set_title(f\"Box Plot of {col.replace('_', ' ').title()}\")\n",
    "    axes[i].set_ylabel(col.replace('_', ' ').title())\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
