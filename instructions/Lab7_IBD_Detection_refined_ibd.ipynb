{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from collections import Counter\n",
    "import logging\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import subprocess\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from IPython.display import display, HTML\n",
    "import IPython\n",
    "import pandas as pd\n",
    "import boto3\n",
    "\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_comp_gen_dir():\n",
    "    \"\"\"Find the computational_genetic_genealogy directory by searching up from current directory.\"\"\"\n",
    "    current = Path.cwd()\n",
    "    \n",
    "    # Search up through parent directories\n",
    "    while current != current.parent:\n",
    "        # Check if target directory exists in current path\n",
    "        target = current / 'computational_genetic_genealogy'\n",
    "        if target.is_dir():\n",
    "            return target\n",
    "        # Move up one directory\n",
    "        current = current.parent\n",
    "    \n",
    "    raise FileNotFoundError(\"Could not find computational_genetic_genealogy directory\")\n",
    "\n",
    "def load_env_file():\n",
    "    \"\"\"Find and load the .env file from the computational_genetic_genealogy directory.\"\"\"\n",
    "    try:\n",
    "        # Find the computational_genetic_genealogy directory\n",
    "        comp_gen_dir = find_comp_gen_dir()\n",
    "        \n",
    "        # Look for .env file\n",
    "        env_path = comp_gen_dir / '.env'\n",
    "        if not env_path.exists():\n",
    "            print(f\"Warning: No .env file found in {comp_gen_dir}\")\n",
    "            return None\n",
    "        \n",
    "        # Load the .env file\n",
    "        load_dotenv(env_path, override=True)\n",
    "        print(f\"Loaded environment variables from: {env_path}\")\n",
    "        return env_path\n",
    "        \n",
    "    except FileNotFoundError as e:\n",
    "        print(f\"Error: {e}\")\n",
    "        return None\n",
    "\n",
    "# Use the function\n",
    "env_path = load_env_file()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "working_directory = os.getenv('PROJECT_WORKING_DIR', default=None)\n",
    "data_directory = os.getenv('PROJECT_DATA_DIR', default=None)\n",
    "references_directory = os.getenv('PROJECT_REFERENCES_DIR', default=None)\n",
    "results_directory = os.getenv('PROJECT_RESULTS_DIR', default=None)\n",
    "utils_directory = os.getenv('PROJECT_UTILS_DIR', default=None)\n",
    "\n",
    "print(f\"Working Directory: {working_directory}\")\n",
    "print(f\"Data Directory: {data_directory}\")\n",
    "print(f\"References Directory: {references_directory}\")\n",
    "print(f\"Results Directory: {results_directory}\")\n",
    "print(f\"Utils Directory: {utils_directory}\")\n",
    "\n",
    "os.chdir(working_directory)\n",
    "print(f\"The current directory is {os.getcwd()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def configure_logging(log_filename, log_file_debug_level=\"INFO\", console_debug_level=\"INFO\"):\n",
    "    \"\"\"\n",
    "    Configure logging for both file and console handlers.\n",
    "\n",
    "    Args:\n",
    "        log_filename (str): Path to the log file where logs will be written.\n",
    "        log_file_debug_level (str): Logging level for the file handler.\n",
    "        console_debug_level (str): Logging level for the console handler.\n",
    "    \"\"\"\n",
    "    # Create a root logger\n",
    "    logger = logging.getLogger()\n",
    "    logger.setLevel(logging.DEBUG)  # Capture all messages at the root level\n",
    "\n",
    "    # Convert level names to numeric levels\n",
    "    file_level = getattr(logging, log_file_debug_level.upper(), logging.INFO)\n",
    "    console_level = getattr(logging, console_debug_level.upper(), logging.INFO)\n",
    "\n",
    "    # File handler: Logs messages at file_level and above to the file\n",
    "    file_handler = logging.FileHandler(log_filename)\n",
    "    file_handler.setLevel(file_level)\n",
    "    file_formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')\n",
    "    file_handler.setFormatter(file_formatter)\n",
    "\n",
    "    # Console handler: Logs messages at console_level and above to the console\n",
    "    console_handler = logging.StreamHandler(sys.stdout)\n",
    "    console_handler.setLevel(console_level)\n",
    "    console_formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')\n",
    "    console_handler.setFormatter(console_formatter)\n",
    "\n",
    "    # Add handlers to the root logger\n",
    "    logger.addHandler(file_handler)\n",
    "    logger.addHandler(console_handler)\n",
    "    \n",
    "def clear_logger():\n",
    "    \"\"\"Remove all handlers from the root logger.\"\"\"\n",
    "    logger = logging.getLogger()\n",
    "    for handler in logger.handlers[:]:\n",
    "        logger.removeHandler(handler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_filename = os.path.join(results_directory, \"lab7.log\")\n",
    "print(f\"The Lab 7 log file is located at {log_filename}.\")\n",
    "\n",
    "# Ensure the results_directory exists\n",
    "if not os.path.exists(results_directory):\n",
    "    os.makedirs(results_directory)\n",
    "\n",
    "# Check if the file exists; if not, create it\n",
    "if not os.path.exists(log_filename):\n",
    "    with open(log_filename, 'w') as file:\n",
    "        pass  # The file is now created.\n",
    "    \n",
    "clear_logger() # Clear the logger before reconfiguring it\n",
    "configure_logging(log_filename, log_file_debug_level=\"INFO\", console_debug_level=\"INFO\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the Refined-IBD Detection Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash -s \"$data_directory\" \"$results_directory\" \"$utils_directory\" \"$references_directory\"\n",
    "\n",
    "data_directory=\"$1\"\n",
    "results_directory=\"$2\"\n",
    "utils_directory=\"$3\"\n",
    "references_directory=\"$4\"\n",
    "\n",
    "# Create or empty the final merged output file\n",
    ": > \"${results_directory}/final_merged_opensnps_autosomes_refinedibd.seg\"\n",
    "\n",
    "# Define the Refined-IBD executable path\n",
    "refined_ibd=\"${utils_directory}/refined-ibd.17Jan20.102.jar\"\n",
    "merge_ibd_segments=\"${utils_directory}/merge-ibd-segments.17Jan20.102.jar\"\n",
    "\n",
    "# Ensure the Refined-IBD executable exists\n",
    "if [[ ! -f \"${refined_ibd}\" ]]; then\n",
    "    echo \"Error: Hap-IBD executable not found: ${refined_ibd}\" >&2\n",
    "fi\n",
    "\n",
    "# Run Refined-IBD analysis in loop by chromosome\n",
    "for run in {1..3}; do\n",
    "    for chr in {1..22}; do\n",
    "        phased_samples=\"${results_directory}/phased_samples/merged_opensnps_phased_chr${chr}_sorted.vcf.gz\"\n",
    "        if [[ ! -f \"${phased_samples}\" ]]; then\n",
    "            echo \"No matching VCF file found\" >&2\n",
    "            exit 1\n",
    "        fi\n",
    "\n",
    "        java -jar \"${refined_ibd}\" gt=\"${phased_samples}\" \\\n",
    "            map=\"${references_directory}/genetic_maps/beagle_genetic_maps/plink.chr${chr}.GRCh38.map\" \\\n",
    "            out=\"${results_directory}/merged_opensnps_autosomes_refinedibd_chr${chr}_run${run}.seg\" \\\n",
    "            nthreads=4\n",
    "    done\n",
    "done\n",
    "\n",
    "# Merge IBD segments for each chromosome\n",
    "gap_threshold=0.6  # Adjust as needed\n",
    "discord_threshold=1  # Adjust as needed\n",
    "\n",
    "for chr in {1..22}; do\n",
    "    phased_samples=\"${results_directory}/phased_samples/merged_opensnps_phased_chr${chr}_sorted.vcf.gz\"\n",
    "    genetic_map=\"${references_directory}/genetic_maps/beagle_genetic_maps/plink.chr${chr}.GRCh38.map\"\n",
    "    merged_output=\"${results_directory}/merged_opensnps_autosomes_refinedibd_chr${chr}.seg\"\n",
    "\n",
    "    # Concatenate all three runs of Refined IBD for this chromosome\n",
    "    zcat \"${results_directory}/merged_opensnps_autosomes_refinedibd_chr${chr}_run1.seg.ibd.gz\" \\\n",
    "        \"${results_directory}/merged_opensnps_autosomes_refinedibd_chr${chr}_run2.seg.ibd.gz\" \\\n",
    "        \"${results_directory}/merged_opensnps_autosomes_refinedibd_chr${chr}_run3.seg.ibd.gz\" | \\\n",
    "    java -jar \"${merge_ibd_segments}\" \"${phased_samples}\" \"${genetic_map}\" \"${gap_threshold}\" \"${discord_threshold}\" > \"${merged_output}\"\n",
    "done\n",
    "\n",
    "# Concatenate all merged chromosome-specific IBD files\n",
    "for chr in {1..22}; do\n",
    "    merged_file=\"${results_directory}/merged_opensnps_autosomes_refinedibd_chr${chr}.seg\"\n",
    "    if [[ -f \"${merged_file}\" ]]; then\n",
    "        cat \"${merged_file}\" >> \"${results_directory}/final_merged_opensnps_autosomes_refinedibd.seg\"\n",
    "    else\n",
    "        echo \"Warning: File for chromosome ${chr} not found during final merging.\" >&2\n",
    "    fi\n",
    "done\n",
    "\n",
    "# Remove intermediate merged chromosome-specific files\n",
    "if [[ -f \"${results_directory}/final_merged_opensnps_autosomes_refinedibd.seg\" ]]; then\n",
    "    for chr in {1..22}; do\n",
    "        rm -f \"${results_directory}/merged_opensnps_autosomes_refinedibd_chr${chr}\"*.seg.ibd.gz\n",
    "        rm -f \"${results_directory}/merged_opensnps_autosomes_refinedibd_chr${chr}\"*.seg.hbd.gz\n",
    "        rm -f \"${results_directory}/merged_opensnps_autosomes_refinedibd_chr${chr}\"*.seg.log\n",
    "        rm -f \"${results_directory}/merged_opensnps_autosomes_refinedibd_chr${chr}.seg\"\n",
    "    done\n",
    "fi\n",
    "\n",
    "echo \"Final merged IBD file: ${results_directory}/final_merged_opensnps_autosomes_refinedibd.seg\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explore The Segments Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "segments = os.path.join(results_directory, \"final_merged_opensnps_autosomes_refinedibd.seg\")\n",
    "\n",
    "segments_temp = pd.read_csv(segments, sep=\"\\t\", header=None)\n",
    "segments_temp.columns = [\n",
    "    \"id1\", \"sample1_haplotype\", \"id2\", \"sample2_haplotype\",\n",
    "    \"chrom\", \"phys_start_pos\", \"phys_end_pos\", \n",
    "    \"lod_score\", \"genetic_length\"\n",
    "    ]\n",
    "segments = segments_temp.sort_values(\n",
    "    by=[\"chrom\", \"phys_start_pos\", \"phys_end_pos\"],\n",
    "    ascending=[True, True, True]\n",
    ")\n",
    "segments = segments.reset_index(drop=True)\n",
    "output_file = os.path.join(results_directory, \"merged_opensnps_autosomes_refinedibd.csv\")\n",
    "segments.to_csv(output_file, sep=\"\\t\", index=False, header=False)\n",
    "segments.info() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "segments.head() # You can enter a number greater than 5 to view more rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "segments[\"genetic_length\"].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter segments on min_length=7, min_markers=436, max_error_density=0.004,\n",
    "\n",
    "filtered_segments_7cM = segments[segments[\"genetic_length\"] >= 7].copy()\n",
    "\n",
    "filtered_segments_7cM[\"genetic_length\"].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# First ensure id1 and id2 are consistently ordered\n",
    "filtered_segments_7cM[[\"id1\", \"id2\"]] = filtered_segments_7cM.apply(\n",
    "    lambda row: pd.Series((row[\"id1\"], row[\"id2\"])) if row[\"id1\"] < row[\"id2\"] \n",
    "    else pd.Series((row[\"id2\"], row[\"id1\"])), axis=1\n",
    ")\n",
    "\n",
    "pair_counts = filtered_segments_7cM.groupby([\"id1\", \"id2\"]).size().reset_index(name=\"pair_count\")\n",
    "pair_count_distribution = pair_counts[\"pair_count\"].value_counts().reset_index()\n",
    "pair_count_distribution.columns = [\"Number of Segments\", \"Number of Pairs\"]\n",
    "pair_count_distribution = pair_count_distribution.reset_index(drop=True)\n",
    "display(pair_count_distribution.style.hide(axis=\"index\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by id pairs and calculate all metrics at once\n",
    "aggregated_segments = filtered_segments_7cM.groupby([\"id1\", \"id2\"]).agg(\n",
    "    total_genetic_length=(\"genetic_length\", \"sum\"),\n",
    "    num_segments=(\"genetic_length\", \"count\"),\n",
    "    largest_segment=(\"genetic_length\", \"max\")\n",
    ").reset_index()\n",
    "\n",
    "# Check distribution of values\n",
    "display(aggregated_segments.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Define columns to plot\n",
    "columns = [\"total_genetic_length\", \"num_segments\", \"largest_segment\"]\n",
    "\n",
    "# Create subplots\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "# Histogram for each metric\n",
    "for i, col in enumerate(columns):\n",
    "    sns.histplot(aggregated_segments[col], bins=30, kde=True, ax=axes[i], edgecolor=\"black\")\n",
    "    axes[i].set_title(f\"Distribution of {col.replace('_', ' ').title()}\")\n",
    "    axes[i].set_xlabel(col.replace('_', ' ').title())\n",
    "    axes[i].set_ylabel(\"Frequency\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Box Plot\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "for i, col in enumerate(columns):\n",
    "    sns.boxplot(y=aggregated_segments[col], ax=axes[i])\n",
    "    axes[i].set_title(f\"Box Plot of {col.replace('_', ' ').title()}\")\n",
    "    axes[i].set_ylabel(col.replace('_', ' ').title())\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
