{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 19: Caching Mechanisms for Computational Efficiency\n",
    "\n",
    "## Overview\n",
    "\n",
    "In this lab, we'll explore the caching mechanisms used in Bonsai v3 to improve computational efficiency. By storing and reusing previously computed results, these mechanisms significantly reduce redundant calculations and improve performance for large-scale pedigree reconstruction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard imports\n",
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import networkx as nx\n",
    "from IPython.display import display, HTML, Markdown\n",
    "import inspect\n",
    "import importlib\n",
    "import copy\n",
    "import random\n",
    "import math\n",
    "import time\n",
    "import hashlib\n",
    "import json\n",
    "import functools\n",
    "from collections import defaultdict, OrderedDict\n",
    "\n",
    "sys.path.append(os.path.dirname(os.getcwd()))\n",
    "\n",
    "# Cross-compatibility setup\n",
    "from scripts_support.lab_cross_compatibility import setup_environment, is_jupyterlite, save_results, save_plot\n",
    "\n",
    "# Set up environment-specific paths\n",
    "DATA_DIR, RESULTS_DIR = setup_environment()\n",
    "\n",
    "# Set visualization styles\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "sns.set_context(\"notebook\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup Bonsai module paths\n",
    "if not is_jupyterlite():\n",
    "    # In local environment, add the utils directory to system path\n",
    "    utils_dir = os.getenv('PROJECT_UTILS_DIR', os.path.join(os.path.dirname(DATA_DIR), 'utils'))\n",
    "    bonsaitree_dir = os.path.join(utils_dir, 'bonsaitree')\n",
    "    \n",
    "    # Add to path if it exists and isn't already there\n",
    "    if os.path.exists(bonsaitree_dir) and bonsaitree_dir not in sys.path:\n",
    "        sys.path.append(bonsaitree_dir)\n",
    "        print(f\"Added {bonsaitree_dir} to sys.path\")\n",
    "else:\n",
    "    # In JupyterLite, use a simplified approach\n",
    "    print(\"⚠️ Running in JupyterLite: Some Bonsai functionality may be limited.\")\n",
    "    print(\"This notebook is primarily designed for local execution where the Bonsai codebase is available.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions for exploring modules\n",
    "def display_module_classes(module_name):\n",
    "    \"\"\"Display classes and their docstrings from a module\"\"\"\n",
    "    try:\n",
    "        # Import the module\n",
    "        module = importlib.import_module(module_name)\n",
    "        \n",
    "        # Find all classes\n",
    "        classes = inspect.getmembers(module, inspect.isclass)\n",
    "        \n",
    "        # Filter classes defined in this module (not imported)\n",
    "        classes = [(name, cls) for name, cls in classes if cls.__module__ == module_name]\n",
    "        \n",
    "        # Print info for each class\n",
    "        for name, cls in classes:\n",
    "            print(f\"\\n## {name}\")\n",
    "            \n",
    "            # Get docstring\n",
    "            doc = inspect.getdoc(cls)\n",
    "            if doc:\n",
    "                print(f\"Docstring: {doc}\")\n",
    "            else:\n",
    "                print(\"No docstring available\")\n",
    "            \n",
    "            # Get methods\n",
    "            methods = inspect.getmembers(cls, inspect.isfunction)\n",
    "            if methods:\n",
    "                print(\"\\nMethods:\")\n",
    "                for method_name, method in methods:\n",
    "                    if not method_name.startswith('_'):  # Skip private methods\n",
    "                        print(f\"- {method_name}\")\n",
    "    except ImportError as e:\n",
    "        print(f\"Error importing module {module_name}: {e}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing module {module_name}: {e}\")\n",
    "\n",
    "def display_module_functions(module_name):\n",
    "    \"\"\"Display functions and their docstrings from a module\"\"\"\n",
    "    try:\n",
    "        # Import the module\n",
    "        module = importlib.import_module(module_name)\n",
    "        \n",
    "        # Find all functions\n",
    "        functions = inspect.getmembers(module, inspect.isfunction)\n",
    "        \n",
    "        # Filter functions defined in this module (not imported)\n",
    "        functions = [(name, func) for name, func in functions if func.__module__ == module_name]\n",
    "        \n",
    "        # Print info for each function\n",
    "        for name, func in functions:\n",
    "            if name.startswith('_'):  # Skip private functions\n",
    "                continue\n",
    "                \n",
    "            print(f\"\\n## {name}\")\n",
    "            \n",
    "            # Get signature\n",
    "            sig = inspect.signature(func)\n",
    "            print(f\"Signature: {name}{sig}\")\n",
    "            \n",
    "            # Get docstring\n",
    "            doc = inspect.getdoc(func)\n",
    "            if doc:\n",
    "                print(f\"Docstring: {doc}\")\n",
    "            else:\n",
    "                print(\"No docstring available\")\n",
    "    except ImportError as e:\n",
    "        print(f\"Error importing module {module_name}: {e}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing module {module_name}: {e}\")\n",
    "\n",
    "def view_source(obj):\n",
    "    \"\"\"Display the source code of an object (function or class)\"\"\"\n",
    "    try:\n",
    "        source = inspect.getsource(obj)\n",
    "        display(Markdown(f\"```python\\n{source}\\n```\"))\n",
    "    except Exception as e:\n",
    "        print(f\"Error retrieving source: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check Bonsai Installation\n",
    "\n",
    "Let's verify that the Bonsai v3 module is available for import:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    from utils.bonsaitree.bonsaitree import v3\n",
    "    print(\"✅ Successfully imported Bonsai v3 module\")\n",
    "except ImportError as e:\n",
    "    print(f\"❌ Failed to import Bonsai v3 module: {e}\")\n",
    "    print(\"This lab requires access to the Bonsai v3 codebase.\")\n",
    "    print(\"Make sure you've properly set up your environment with the Bonsai repository.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lab 19: Caching Mechanisms for Computational Efficiency\n",
    "\n",
    "In this lab, we'll explore the sophisticated caching mechanisms used in Bonsai v3 to improve computational efficiency. Pedigree reconstruction involves many repetitive calculations, such as:\n",
    "\n",
    "1. Computing likelihoods for the same relationship configuration multiple times\n",
    "2. Finding ancestors or descendants of the same individual repeatedly\n",
    "3. Evaluating the same IBD segments in different contexts\n",
    "\n",
    "By implementing effective caching strategies, Bonsai v3 can avoid these redundant calculations and significantly improve performance. We'll focus on several key caching mechanisms:\n",
    "\n",
    "1. **Memoization**: Storing the results of expensive function calls and returning the cached result when the same inputs occur again\n",
    "2. **LRU Cache**: Using Least Recently Used (LRU) caching to maintain a fixed-size cache of the most recently accessed items\n",
    "3. **Persistent Caching**: Storing computation results to disk for reuse across different runs\n",
    "4. **Hierarchical Caching**: Using multi-level caching strategies for different types of calculations\n",
    "\n",
    "We'll implement simplified versions of these mechanisms to understand how they work and why they're important for large-scale pedigree reconstruction."
   ]
  },
  {
   "cell_type": "markdown",
   "source": "## Part 1: Memoization\n\nMemoization is a caching technique where the results of expensive function calls are stored so that the same computation isn't performed repeatedly for the same inputs. This is particularly useful in Bonsai v3, where many functions are called repeatedly with the same parameters during pedigree reconstruction.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Import Bonsai caching modules if available\nif not is_jupyterlite():\n    try:\n        from utils.bonsaitree.bonsaitree.v3.caching import memoize\n        \n        # Display the source code if available\n        print(\"Source code for memoize:\")\n        view_source(memoize)\n    except (ImportError, AttributeError) as e:\n        print(f\"Could not import function: {e}\")\nelse:\n    print(\"Cannot display source code in JupyterLite environment.\")",
   "metadata": {},
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### 1.1 Basic Memoization\n\nLet's implement a basic memoization decorator that caches function results based on input arguments:",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "def memoize(func):\n    \"\"\"\n    A simple memoization decorator that caches function results.\n    \n    Args:\n        func: The function to memoize\n        \n    Returns:\n        A wrapper function that implements memoization\n    \"\"\"\n    cache = {}\n    \n    @functools.wraps(func)\n    def wrapper(*args, **kwargs):\n        # Create a key from the arguments\n        key = str(args) + str(sorted(kwargs.items()))\n        \n        # Return cached result if available\n        if key in cache:\n            return cache[key]\n        \n        # Compute and cache the result\n        result = func(*args, **kwargs)\n        cache[key] = result\n        return result\n    \n    # Add a method to clear the cache\n    def clear_cache():\n        cache.clear()\n    \n    wrapper.clear_cache = clear_cache\n    \n    # Add a method to get cache info\n    def cache_info():\n        return {\"cache_hits\": 0, \"cache_misses\": 0, \"cache_size\": len(cache)}\n    \n    wrapper.cache_info = cache_info\n    \n    return wrapper\n\n# Example of an expensive function that could benefit from memoization\ndef calculate_fibonacci(n):\n    \"\"\"\n    Calculate the nth Fibonacci number recursively.\n    This is intentionally inefficient to demonstrate memoization.\n    \"\"\"\n    if n <= 1:\n        return n\n    return calculate_fibonacci(n-1) + calculate_fibonacci(n-2)\n\n# Create a memoized version\n@memoize\ndef calculate_fibonacci_memoized(n):\n    \"\"\"\n    Calculate the nth Fibonacci number recursively, with memoization.\n    \"\"\"\n    if n <= 1:\n        return n\n    return calculate_fibonacci_memoized(n-1) + calculate_fibonacci_memoized(n-2)\n\n# Let's see how memoization improves performance\ndef benchmark_fibonacci(n):\n    \"\"\"Benchmark regular vs. memoized Fibonacci calculation\"\"\"\n    # Regular version\n    start_time = time.time()\n    result = calculate_fibonacci(n)\n    regular_time = time.time() - start_time\n    \n    # Memoized version\n    calculate_fibonacci_memoized.clear_cache()  # Start with an empty cache\n    start_time = time.time()\n    memoized_result = calculate_fibonacci_memoized(n)\n    memoized_time = time.time() - start_time\n    \n    # Check results match\n    assert result == memoized_result, \"Results don't match!\"\n    \n    # Calculate speedup\n    speedup = regular_time / memoized_time if memoized_time > 0 else float('inf')\n    \n    return {\n        \"n\": n,\n        \"result\": result,\n        \"regular_time\": regular_time,\n        \"memoized_time\": memoized_time,\n        \"speedup\": speedup\n    }\n\n# Benchmark for different values of n\nresults = []\nfor n in range(20, 36, 5):\n    print(f\"Benchmarking n={n}...\")\n    results.append(benchmark_fibonacci(n))\n\n# Display results in a table\nprint(\"\\nBenchmark Results:\")\nprint(f\"{'n':<5} | {'Result':<15} | {'Regular Time':<15} | {'Memoized Time':<15} | {'Speedup':<10}\")\nprint(\"-\" * 65)\n\nfor result in results:\n    print(f\"{result['n']:<5} | {result['result']:<15} | {result['regular_time']:.6f} s | {result['memoized_time']:.6f} s | {result['speedup']:.2f}x\")\n\n# Plot the results\nplt.figure(figsize=(12, 6))\nplt.title(\"Memoization Performance Improvement\")\n\n# Extract data for plotting\nns = [r[\"n\"] for r in results]\nregular_times = [r[\"regular_time\"] for r in results]\nmemoized_times = [r[\"memoized_time\"] for r in results]\n\n# Create a logarithmic scale plot to handle large differences\nplt.semilogy(ns, regular_times, 'o-', label='Regular', linewidth=2, markersize=8)\nplt.semilogy(ns, memoized_times, 'o-', label='Memoized', linewidth=2, markersize=8)\n\nplt.xlabel(\"Fibonacci Number (n)\")\nplt.ylabel(\"Computation Time (seconds, log scale)\")\nplt.grid(True, linestyle='--', alpha=0.7)\nplt.legend()\nplt.tight_layout()\nplt.show()",
   "metadata": {},
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### 1.2 Improved Memoization with Metrics\n\nLet's improve our memoization decorator to track cache hits and misses, which will help us understand how effective the cache is:",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "def memoize_with_metrics(func):\n    \"\"\"\n    An enhanced memoization decorator that tracks cache metrics.\n    \n    Args:\n        func: The function to memoize\n        \n    Returns:\n        A wrapper function that implements memoization with metrics\n    \"\"\"\n    cache = {}\n    hits = 0\n    misses = 0\n    \n    @functools.wraps(func)\n    def wrapper(*args, **kwargs):\n        nonlocal hits, misses\n        \n        # Create a key from the arguments\n        key = str(args) + str(sorted(kwargs.items()))\n        \n        # Return cached result if available\n        if key in cache:\n            hits += 1\n            return cache[key]\n        \n        # Compute and cache the result\n        misses += 1\n        result = func(*args, **kwargs)\n        cache[key] = result\n        return result\n    \n    # Add a method to clear the cache\n    def clear_cache():\n        nonlocal hits, misses\n        cache.clear()\n        hits = 0\n        misses = 0\n    \n    wrapper.clear_cache = clear_cache\n    \n    # Add a method to get cache info\n    def cache_info():\n        return {\n            \"cache_hits\": hits,\n            \"cache_misses\": misses,\n            \"cache_size\": len(cache),\n            \"hit_ratio\": hits / (hits + misses) if hits + misses > 0 else 0\n        }\n    \n    wrapper.cache_info = cache_info\n    \n    return wrapper\n\n# Example function - calculating binomial coefficients\ndef calculate_binomial(n, k):\n    \"\"\"Calculate binomial coefficient C(n,k) using a recursive formula\"\"\"\n    if k == 0 or k == n:\n        return 1\n    return calculate_binomial(n-1, k-1) + calculate_binomial(n-1, k)\n\n@memoize_with_metrics\ndef calculate_binomial_memoized(n, k):\n    \"\"\"Calculate binomial coefficient C(n,k) using a recursive formula with memoization\"\"\"\n    if k == 0 or k == n:\n        return 1\n    return calculate_binomial_memoized(n-1, k-1) + calculate_binomial_memoized(n-1, k)\n\n# Let's calculate some binomial coefficients and monitor cache performance\ndef demonstrate_binomial_memoization():\n    # Clear cache to start fresh\n    calculate_binomial_memoized.clear_cache()\n    \n    # Calculate C(20,10)\n    start_time = time.time()\n    result = calculate_binomial_memoized(20, 10)\n    elapsed_time = time.time() - start_time\n    \n    print(f\"C(20,10) = {result}, calculated in {elapsed_time:.6f} seconds\")\n    \n    # Check cache metrics\n    metrics = calculate_binomial_memoized.cache_info()\n    print(f\"Cache hits: {metrics['cache_hits']}\")\n    print(f\"Cache misses: {metrics['cache_misses']}\")\n    print(f\"Cache size: {metrics['cache_size']}\")\n    print(f\"Hit ratio: {metrics['hit_ratio']:.2%}\")\n    \n    # Calculate C(20,11) - should reuse many cached results\n    start_time = time.time()\n    result = calculate_binomial_memoized(20, 11)\n    elapsed_time = time.time() - start_time\n    \n    print(f\"\\nC(20,11) = {result}, calculated in {elapsed_time:.6f} seconds\")\n    \n    # Check cache metrics again\n    metrics = calculate_binomial_memoized.cache_info()\n    print(f\"Cache hits: {metrics['cache_hits']}\")\n    print(f\"Cache misses: {metrics['cache_misses']}\")\n    print(f\"Cache size: {metrics['cache_size']}\")\n    print(f\"Hit ratio: {metrics['hit_ratio']:.2%}\")\n    \n    # Calculate hit ratio improvement for the second calculation\n    first_hits = metrics['cache_hits'] - metrics['cache_misses']\n    second_metrics = {}\n    second_metrics['cache_hits'] = metrics['cache_hits'] - first_hits\n    second_metrics['cache_misses'] = metrics['cache_misses'] - metrics['cache_misses']\n    \n    return metrics\n\n# Let's run the demonstration\nmetrics = demonstrate_binomial_memoization()\n\n# Visualize cache performance\nlabels = ['Hits', 'Misses']\nsizes = [metrics['cache_hits'], metrics['cache_misses']]\ncolors = ['#66b3ff', '#ff9999']\nexplode = (0.1, 0)  # explode the 1st slice (Hits)\n\nplt.figure(figsize=(8, 8))\nplt.pie(sizes, explode=explode, labels=labels, colors=colors, autopct='%1.1f%%',\n        shadow=True, startangle=90)\nplt.axis('equal')  # Equal aspect ratio ensures that pie is drawn as a circle\nplt.title('Cache Performance Metrics')\nplt.tight_layout()\nplt.show()",
   "metadata": {},
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### 1.3 Application to Pedigree Reconstruction\n\nLet's see how memoization can be applied to pedigree reconstruction functions in Bonsai v3, such as finding ancestors or descendants in a pedigree:",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Non-memoized version of get_ancestors function\ndef get_ancestors(iid, up_dct, max_degree=None):\n    \"\"\"\n    Get all ancestors of an individual in a pedigree.\n    \n    Args:\n        iid: ID of the individual\n        up_dct: Up-node dictionary representing the pedigree\n        max_degree: Maximum generational distance to consider\n        \n    Returns:\n        Set of ancestor IDs\n    \"\"\"\n    ancestors = set()\n    current_degree = 0\n    current_gen = {iid}\n    \n    while current_gen and (max_degree is None or current_degree < max_degree):\n        next_gen = set()\n        for current_id in current_gen:\n            # Get parents from the up_dct\n            if current_id in up_dct:\n                for parent_id in up_dct[current_id]:\n                    if parent_id not in ancestors:\n                        next_gen.add(parent_id)\n                        ancestors.add(parent_id)\n        \n        current_gen = next_gen\n        current_degree += 1\n    \n    return ancestors\n\n# Memoized version\n@memoize_with_metrics\ndef get_ancestors_memoized(iid, up_dct, max_degree=None):\n    \"\"\"\n    Get all ancestors of an individual in a pedigree (memoized version).\n    \n    Args:\n        iid: ID of the individual\n        up_dct: Up-node dictionary representing the pedigree\n        max_degree: Maximum generational distance to consider\n        \n    Returns:\n        Set of ancestor IDs\n    \"\"\"\n    # We need to convert up_dct to a hashable representation for the memoization key\n    # In this simplified version, we'll use a frozenset of (id, parent_id) tuples\n    up_dct_hashable = frozenset(\n        (id_val, parent_id) \n        for id_val, parents in up_dct.items() \n        for parent_id in parents\n    )\n    \n    ancestors = set()\n    current_degree = 0\n    current_gen = {iid}\n    \n    while current_gen and (max_degree is None or current_degree < max_degree):\n        next_gen = set()\n        for current_id in current_gen:\n            # Get parents from the up_dct\n            if current_id in up_dct:\n                for parent_id in up_dct[current_id]:\n                    if parent_id not in ancestors:\n                        next_gen.add(parent_id)\n                        ancestors.add(parent_id)\n        \n        current_gen = next_gen\n        current_degree += 1\n    \n    return ancestors\n\n# Generate a large test pedigree\ndef generate_test_pedigree(num_generations=5, branching_factor=2):\n    \"\"\"\n    Generate a test pedigree with a specified number of generations.\n    \n    Args:\n        num_generations: Number of generations in the pedigree\n        branching_factor: Number of children per individual\n        \n    Returns:\n        up_dct: Up-node dictionary representing the pedigree\n        id_to_gen: Dict mapping IDs to their generation\n    \"\"\"\n    up_dct = {}\n    id_to_gen = {}\n    next_id = 1\n    \n    # Create the first generation (founders)\n    founders = []\n    for _ in range(branching_factor ** (num_generations - 1)):\n        id_to_gen[next_id] = 0  # Generation 0\n        up_dct[next_id] = {}\n        founders.append(next_id)\n        next_id += 1\n    \n    # Create subsequent generations\n    for gen in range(1, num_generations):\n        parents = [id_val for id_val, g in id_to_gen.items() if g == gen - 1]\n        \n        for parent_id in parents:\n            for _ in range(branching_factor):\n                id_to_gen[next_id] = gen\n                up_dct[next_id] = {parent_id: 1}\n                next_id += 1\n    \n    return up_dct, id_to_gen\n\n# Benchmark ancestor calculation with and without memoization\ndef benchmark_ancestor_calculation(up_dct, id_to_gen, num_queries=100):\n    \"\"\"\n    Benchmark ancestor calculation with and without memoization.\n    \n    Args:\n        up_dct: Up-node dictionary representing the pedigree\n        id_to_gen: Dict mapping IDs to their generation\n        num_queries: Number of queries to perform\n        \n    Returns:\n        Dict with benchmark results\n    \"\"\"\n    # Clear cache to start fresh\n    get_ancestors_memoized.clear_cache()\n    \n    # Get all IDs in the pedigree\n    all_ids = list(up_dct.keys())\n    \n    # Generate random queries (ID, max_degree)\n    queries = []\n    for _ in range(num_queries):\n        iid = random.choice(all_ids)\n        max_degree = random.randint(1, 5)\n        queries.append((iid, max_degree))\n    \n    # Time the non-memoized version\n    start_time = time.time()\n    regular_results = []\n    \n    for iid, max_degree in queries:\n        ancestors = get_ancestors(iid, up_dct, max_degree)\n        regular_results.append(ancestors)\n    \n    regular_time = time.time() - start_time\n    \n    # Time the memoized version\n    start_time = time.time()\n    memoized_results = []\n    \n    for iid, max_degree in queries:\n        ancestors = get_ancestors_memoized(iid, up_dct, max_degree)\n        memoized_results.append(ancestors)\n    \n    memoized_time = time.time() - start_time\n    \n    # Verify results match\n    for i, (regular, memoized) in enumerate(zip(regular_results, memoized_results)):\n        if regular != memoized:\n            print(f\"Warning: Results don't match for query {i}!\")\n    \n    # Get cache metrics\n    metrics = get_ancestors_memoized.cache_info()\n    \n    # Calculate speedup\n    speedup = regular_time / memoized_time if memoized_time > 0 else float('inf')\n    \n    return {\n        \"queries\": num_queries,\n        \"regular_time\": regular_time,\n        \"memoized_time\": memoized_time,\n        \"speedup\": speedup,\n        \"cache_hits\": metrics[\"cache_hits\"],\n        \"cache_misses\": metrics[\"cache_misses\"],\n        \"hit_ratio\": metrics[\"hit_ratio\"]\n    }\n\n# Generate test pedigree\nprint(\"Generating test pedigree...\")\nup_dct, id_to_gen = generate_test_pedigree(num_generations=6, branching_factor=2)\nprint(f\"Pedigree has {len(up_dct)} individuals across {max(id_to_gen.values()) + 1} generations\")\n\n# Run benchmark\nprint(\"\\nRunning benchmark...\")\nresults = benchmark_ancestor_calculation(up_dct, id_to_gen, num_queries=100)\n\n# Display results\nprint(\"\\nBenchmark Results:\")\nprint(f\"Number of queries: {results['queries']}\")\nprint(f\"Non-memoized time: {results['regular_time']:.6f} seconds\")\nprint(f\"Memoized time: {results['memoized_time']:.6f} seconds\")\nprint(f\"Speedup: {results['speedup']:.2f}x\")\nprint(f\"Cache hits: {results['cache_hits']}\")\nprint(f\"Cache misses: {results['cache_misses']}\")\nprint(f\"Hit ratio: {results['hit_ratio']:.2%}\")\n\n# Create a bar chart comparing performance\nplt.figure(figsize=(10, 6))\nplt.bar(['Non-memoized', 'Memoized'], \n        [results['regular_time'], results['memoized_time']], \n        color=['#ff9999', '#66b3ff'])\nplt.title('Ancestor Calculation Performance')\nplt.ylabel('Time (seconds)')\nplt.grid(axis='y', linestyle='--', alpha=0.7)\nplt.tight_layout()\nplt.show()",
   "metadata": {},
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Part 2: Least Recently Used (LRU) Cache\n\nWhile memoization is effective, it can lead to unbounded memory usage as the cache grows over time. Least Recently Used (LRU) caching addresses this by maintaining a fixed-size cache of the most recently used items, discarding the least recently used items when the cache is full.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Import Bonsai LRU caching modules if available\nif not is_jupyterlite():\n    try:\n        from utils.bonsaitree.bonsaitree.v3.caching import lru_cache\n        \n        # Display the source code if available\n        print(\"Source code for lru_cache:\")\n        view_source(lru_cache)\n    except (ImportError, AttributeError) as e:\n        print(f\"Could not import function: {e}\")\nelse:\n    print(\"Cannot display source code in JupyterLite environment.\")",
   "metadata": {},
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### 2.1 Implementing a Simple LRU Cache\n\nLet's implement a simple LRU cache using an OrderedDict, which maintains insertion order and allows us to move recently accessed items to the end:",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "class LRUCache:\n    \"\"\"\n    A simple implementation of an LRU (Least Recently Used) cache.\n    \n    This cache has a maximum size and evicts the least recently used items\n    when that size is exceeded.\n    \n    Args:\n        max_size: The maximum number of items to store in the cache\n    \"\"\"\n    def __init__(self, max_size=128):\n        self.cache = OrderedDict()\n        self.max_size = max_size\n        self.hits = 0\n        self.misses = 0\n    \n    def get(self, key):\n        \"\"\"\n        Get an item from the cache, marking it as recently used.\n        \n        Args:\n            key: The key to look up\n            \n        Returns:\n            The cached value, or None if not found\n        \"\"\"\n        if key in self.cache:\n            # Move the key to the end (most recently used)\n            value = self.cache.pop(key)\n            self.cache[key] = value\n            self.hits += 1\n            return value\n        self.misses += 1\n        return None\n    \n    def put(self, key, value):\n        \"\"\"\n        Add an item to the cache, evicting the least recently used item if necessary.\n        \n        Args:\n            key: The key to store\n            value: The value to store\n            \n        Returns:\n            None\n        \"\"\"\n        if key in self.cache:\n            # Remove the existing entry\n            self.cache.pop(key)\n        elif len(self.cache) >= self.max_size:\n            # Remove the least recently used item (first item in OrderedDict)\n            self.cache.popitem(last=False)\n        \n        # Add the new item\n        self.cache[key] = value\n    \n    def clear(self):\n        \"\"\"Clear the cache and reset statistics.\"\"\"\n        self.cache.clear()\n        self.hits = 0\n        self.misses = 0\n    \n    def info(self):\n        \"\"\"Return statistics about the cache.\"\"\"\n        total_accesses = self.hits + self.misses\n        hit_ratio = self.hits / total_accesses if total_accesses > 0 else 0\n        return {\n            \"size\": len(self.cache),\n            \"max_size\": self.max_size,\n            \"hits\": self.hits,\n            \"misses\": self.misses,\n            \"hit_ratio\": hit_ratio\n        }",
   "metadata": {},
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### 2.2 Creating an LRU Cache Decorator\n\nNow let's create a decorator that applies an LRU cache to functions, similar to Python's `functools.lru_cache`:",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "def lru_cache(maxsize=128):\n    \"\"\"\n    Decorator to wrap a function with an LRU cache.\n    \n    Args:\n        maxsize: Maximum number of entries to keep in the cache\n        \n    Returns:\n        Decorator function\n    \"\"\"\n    def decorator(func):\n        cache = LRUCache(max_size=maxsize)\n        \n        @functools.wraps(func)\n        def wrapper(*args, **kwargs):\n            # Create a key from the arguments\n            key = str(args) + str(sorted(kwargs.items()))\n            \n            # Try to get from cache\n            result = cache.get(key)\n            if result is not None:\n                return result\n            \n            # Compute and store the result\n            result = func(*args, **kwargs)\n            cache.put(key, result)\n            return result\n        \n        # Add methods to access cache information\n        def cache_info():\n            return cache.info()\n        \n        def cache_clear():\n            cache.clear()\n        \n        wrapper.cache_info = cache_info\n        wrapper.cache_clear = cache_clear\n        \n        return wrapper\n    \n    return decorator",
   "metadata": {},
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### 2.3 Benchmarking LRU Cache Performance\n\nLet's apply our LRU cache to the Fibonacci function and benchmark its performance with different cache sizes:",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Create a Fibonacci function with our LRU cache\n@lru_cache(maxsize=64)\ndef fib_lru(n):\n    \"\"\"Calculate the nth Fibonacci number using LRU cache.\"\"\"\n    if n <= 1:\n        return n\n    return fib_lru(n-1) + fib_lru(n-2)\n\n# Benchmark different cache sizes\ndef benchmark_lru_cache_sizes(n=35):\n    \"\"\"\n    Benchmark LRU cache with different cache sizes.\n    \n    Args:\n        n: Fibonacci number to calculate\n        \n    Returns:\n        Dict with benchmark results\n    \"\"\"\n    results = []\n    \n    # Define a function to create and test a fib function with a specific cache size\n    def test_cache_size(cache_size):\n        # Create a new function with the specified cache size\n        @lru_cache(maxsize=cache_size)\n        def fib_test(n):\n            if n <= 1:\n                return n\n            return fib_test(n-1) + fib_test(n-2)\n        \n        # Calculate the fibonacci number and measure time\n        start_time = time.time()\n        result = fib_test(n)\n        elapsed_time = time.time() - start_time\n        \n        # Get cache info\n        info = fib_test.cache_info()\n        \n        return {\n            \"cache_size\": cache_size,\n            \"time\": elapsed_time,\n            \"hits\": info[\"hits\"],\n            \"misses\": info[\"misses\"],\n            \"hit_ratio\": info[\"hit_ratio\"]\n        }\n    \n    # Test different cache sizes\n    for cache_size in [8, 16, 32, 64, 128, 256]:\n        print(f\"Testing cache size: {cache_size}\")\n        results.append(test_cache_size(cache_size))\n    \n    return results\n\n# Run the benchmark\nbenchmark_results = benchmark_lru_cache_sizes(n=35)\n\n# Create a visualization of the results\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n\n# Plot computation time vs cache size\ncache_sizes = [r[\"cache_size\"] for r in benchmark_results]\ntimes = [r[\"time\"] for r in benchmark_results]\nhit_ratios = [r[\"hit_ratio\"] * 100 for r in benchmark_results]\n\nax1.plot(cache_sizes, times, 'o-', linewidth=2, markersize=8)\nax1.set_xlabel(\"Cache Size\")\nax1.set_ylabel(\"Computation Time (seconds)\")\nax1.set_title(\"Computation Time vs Cache Size\")\nax1.grid(True, linestyle='--', alpha=0.7)\n\n# Plot hit ratio vs cache size\nax2.plot(cache_sizes, hit_ratios, 'o-', linewidth=2, markersize=8, color='green')\nax2.set_xlabel(\"Cache Size\")\nax2.set_ylabel(\"Cache Hit Ratio (%)\")\nax2.set_title(\"Cache Hit Ratio vs Cache Size\")\nax2.grid(True, linestyle='--', alpha=0.7)\n\nplt.tight_layout()\nplt.show()\n\n# Print the detailed results\nprint(\"\\nDetailed Results:\")\nprint(f\"{'Cache Size':<10} | {'Time (s)':<10} | {'Hits':<6} | {'Misses':<6} | {'Hit Ratio':<8}\")\nprint(\"-\" * 50)\nfor result in benchmark_results:\n    print(f\"{result['cache_size']:<10} | {result['time']:.6f} | {result['hits']:<6} | {result['misses']:<6} | {result['hit_ratio']:.2%}\")",
   "metadata": {},
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### 2.4 Application to Pedigree Reconstruction\n\nLet's apply LRU caching to a common pedigree reconstruction function: finding the lowest common ancestor (LCA) of two individuals. This function is frequently called during pedigree inference, and would benefit from caching:",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Function to find the lowest common ancestor of two individuals\ndef find_lowest_common_ancestor(iid1, iid2, up_dct):\n    \"\"\"\n    Find the lowest common ancestor of two individuals in a pedigree.\n    \n    Args:\n        iid1: ID of the first individual\n        iid2: ID of the second individual\n        up_dct: Up-node dictionary representing the pedigree\n        \n    Returns:\n        ID of the lowest common ancestor, or None if none exists\n    \"\"\"\n    # Get all ancestors of the first individual\n    ancestors1 = set()\n    queue = [iid1]\n    while queue:\n        current = queue.pop(0)\n        if current in up_dct:\n            for parent in up_dct[current]:\n                if parent not in ancestors1:\n                    ancestors1.add(parent)\n                    queue.append(parent)\n    \n    # Check if the second individual is in the ancestors of the first\n    if iid2 in ancestors1:\n        return iid2\n    \n    # Traverse up from the second individual, checking for common ancestors\n    visited = set()\n    queue = [iid2]\n    while queue:\n        current = queue.pop(0)\n        if current in ancestors1:\n            return current\n        \n        visited.add(current)\n        if current in up_dct:\n            for parent in up_dct[current]:\n                if parent not in visited:\n                    queue.append(parent)\n    \n    # No common ancestor found\n    return None\n\n# Create an LRU-cached version\n@lru_cache(maxsize=1000)\ndef find_lowest_common_ancestor_lru(iid1, iid2, up_dct_hashable):\n    \"\"\"\n    Find the lowest common ancestor of two individuals in a pedigree (with LRU caching).\n    \n    Args:\n        iid1: ID of the first individual\n        iid2: ID of the second individual\n        up_dct_hashable: Hashable representation of the up-node dictionary\n        \n    Returns:\n        ID of the lowest common ancestor, or None if none exists\n    \"\"\"\n    # Convert the hashable representation back to a dictionary\n    up_dct = {}\n    for id_val, parent_id in up_dct_hashable:\n        if id_val not in up_dct:\n            up_dct[id_val] = set()\n        up_dct[id_val].add(parent_id)\n    \n    # Get all ancestors of the first individual\n    ancestors1 = set()\n    queue = [iid1]\n    while queue:\n        current = queue.pop(0)\n        if current in up_dct:\n            for parent in up_dct[current]:\n                if parent not in ancestors1:\n                    ancestors1.add(parent)\n                    queue.append(parent)\n    \n    # Check if the second individual is in the ancestors of the first\n    if iid2 in ancestors1:\n        return iid2\n    \n    # Traverse up from the second individual, checking for common ancestors\n    visited = set()\n    queue = [iid2]\n    while queue:\n        current = queue.pop(0)\n        if current in ancestors1:\n            return current\n        \n        visited.add(current)\n        if current in up_dct:\n            for parent in up_dct[current]:\n                if parent not in visited:\n                    queue.append(parent)\n    \n    # No common ancestor found\n    return None\n\n# Generate a larger test pedigree for benchmarking\ndef generate_complex_pedigree(individuals=100, avg_children=2, consanguinity_rate=0.1):\n    \"\"\"\n    Generate a more complex pedigree with consanguinity.\n    \n    Args:\n        individuals: Total number of individuals to generate\n        avg_children: Average number of children per pair\n        consanguinity_rate: Probability of consanguineous mating\n        \n    Returns:\n        up_dct: Up-node dictionary representing the pedigree\n    \"\"\"\n    up_dct = {}\n    available_mates = []\n    next_id = 1\n    \n    # Create founders (10% of total)\n    founders = []\n    for _ in range(max(2, int(individuals * 0.1))):\n        up_dct[next_id] = {}\n        founders.append(next_id)\n        available_mates.append(next_id)\n        next_id += 1\n    \n    # Create the rest of the pedigree\n    while next_id <= individuals:\n        # Choose parents\n        if random.random() < consanguinity_rate and len(available_mates) > 3:\n            # Consanguineous mating - choose related individuals\n            parent1 = random.choice(available_mates)\n            # Find relatives of parent1\n            relatives = []\n            for iid in available_mates:\n                if iid != parent1:\n                    lca = find_lowest_common_ancestor(parent1, iid, up_dct)\n                    if lca is not None:\n                        relatives.append(iid)\n            \n            if relatives:\n                parent2 = random.choice(relatives)\n            else:\n                # Fall back to random mating if no relatives found\n                candidates = [iid for iid in available_mates if iid != parent1]\n                parent2 = random.choice(candidates) if candidates else parent1\n        else:\n            # Random mating\n            parent1 = random.choice(available_mates)\n            candidates = [iid for iid in available_mates if iid != parent1]\n            parent2 = random.choice(candidates) if candidates else parent1\n        \n        # Create children\n        num_children = max(1, int(random.normalvariate(avg_children, 1)))\n        for _ in range(num_children):\n            if next_id > individuals:\n                break\n                \n            up_dct[next_id] = {parent1, parent2}\n            available_mates.append(next_id)\n            next_id += 1\n    \n    return up_dct\n\n# Benchmark the LCA function with and without LRU caching\ndef benchmark_lca(up_dct, num_queries=1000):\n    \"\"\"\n    Benchmark the lowest common ancestor function with and without LRU caching.\n    \n    Args:\n        up_dct: Up-node dictionary representing the pedigree\n        num_queries: Number of LCA queries to perform\n        \n    Returns:\n        Dict with benchmark results\n    \"\"\"\n    # Create a hashable representation of the up_dct for the LRU cache\n    up_dct_hashable = frozenset(\n        (id_val, parent_id) \n        for id_val, parents in up_dct.items() \n        for parent_id in parents\n    )\n    \n    # Clear the LRU cache\n    find_lowest_common_ancestor_lru.cache_clear()\n    \n    # Generate random pairs of individuals\n    all_ids = list(up_dct.keys())\n    pairs = []\n    for _ in range(num_queries):\n        iid1 = random.choice(all_ids)\n        iid2 = random.choice(all_ids)\n        if iid1 != iid2:\n            pairs.append((iid1, iid2))\n    \n    # Benchmark the regular version\n    start_time = time.time()\n    regular_results = []\n    for iid1, iid2 in pairs:\n        result = find_lowest_common_ancestor(iid1, iid2, up_dct)\n        regular_results.append(result)\n    regular_time = time.time() - start_time\n    \n    # Benchmark the LRU-cached version\n    start_time = time.time()\n    lru_results = []\n    for iid1, iid2 in pairs:\n        result = find_lowest_common_ancestor_lru(iid1, iid2, up_dct_hashable)\n        lru_results.append(result)\n    lru_time = time.time() - start_time\n    \n    # Verify results match\n    for i, (reg, lru) in enumerate(zip(regular_results, lru_results)):\n        if reg != lru:\n            print(f\"Warning: Results don't match for pair {i}!\")\n    \n    # Get cache info\n    cache_info = find_lowest_common_ancestor_lru.cache_info()\n    \n    # Calculate speedup\n    speedup = regular_time / lru_time if lru_time > 0 else float('inf')\n    \n    return {\n        \"queries\": num_queries,\n        \"regular_time\": regular_time,\n        \"lru_time\": lru_time,\n        \"speedup\": speedup,\n        \"hits\": cache_info[\"hits\"],\n        \"misses\": cache_info[\"misses\"],\n        \"hit_ratio\": cache_info[\"hit_ratio\"]\n    }\n\n# Generate a pedigree and run the benchmark\nprint(\"Generating test pedigree...\")\npedigree = generate_complex_pedigree(individuals=200, consanguinity_rate=0.2)\nprint(f\"Pedigree has {len(pedigree)} individuals\")\n\n# Run benchmark\nprint(\"\\nRunning LCA benchmark...\")\nlca_results = benchmark_lca(pedigree, num_queries=1000)\n\n# Display results\nprint(\"\\nLCA Benchmark Results:\")\nprint(f\"Number of queries: {lca_results['queries']}\")\nprint(f\"Regular time: {lca_results['regular_time']:.6f} seconds\")\nprint(f\"LRU-cached time: {lca_results['lru_time']:.6f} seconds\")\nprint(f\"Speedup: {lca_results['speedup']:.2f}x\")\nprint(f\"Cache hits: {lca_results['hits']}\")\nprint(f\"Cache misses: {lca_results['misses']}\")\nprint(f\"Hit ratio: {lca_results['hit_ratio']:.2%}\")\n\n# Create visual comparison\nplt.figure(figsize=(12, 6))\nlabels = ['Regular', 'LRU-Cached']\ntimes = [lca_results['regular_time'], lca_results['lru_time']]\ncolors = ['#ff9999', '#66b3ff']\n\nbar_plot = plt.bar(labels, times, color=colors)\nplt.ylabel('Time (seconds)')\nplt.title('LCA Calculation Performance Comparison')\nplt.grid(axis='y', linestyle='--', alpha=0.7)\n\n# Add text labels\nfor bar, time_val in zip(bar_plot, times):\n    height = bar.get_height()\n    plt.text(bar.get_x() + bar.get_width()/2., height + 0.01,\n             f'{time_val:.4f}s',\n             ha='center', va='bottom')\n\n# Add speedup annotation\nplt.annotate(f'Speedup: {lca_results[\"speedup\"]:.2f}x',\n             xy=(0.5, max(times) * 0.5),\n             xytext=(0.5, max(times) * 0.7),\n             ha='center',\n             bbox=dict(boxstyle=\"round,pad=0.3\", fc=\"#d5f5e3\", ec=\"black\", alpha=0.8))\n\nplt.tight_layout()\nplt.show()",
   "metadata": {},
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Part 3: Persistent Caching\n\nWhile memoization and LRU caching are effective for a single run, they don't persist between runs. For long-running or multi-stage pedigree reconstruction, it can be valuable to have persistent caching that saves computed results to disk.",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "### 3.1 Implementing a Simple Disk Cache\n\nLet's implement a simple persistent cache that stores results in JSON files:",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "class DiskCache:\n    \"\"\"\n    A persistent cache that stores results on disk.\n    \n    Args:\n        cache_dir: Directory to store cache files\n        encoder: Function to encode objects for storage (default: JSON serialization)\n        decoder: Function to decode stored objects (default: JSON deserialization)\n    \"\"\"\n    def __init__(self, cache_dir=None, encoder=None, decoder=None):\n        self.cache_dir = cache_dir or os.path.join(RESULTS_DIR, 'cache')\n        self.encoder = encoder or json.dumps\n        self.decoder = decoder or json.loads\n        self.hits = 0\n        self.misses = 0\n        \n        # Create the cache directory if it doesn't exist\n        os.makedirs(self.cache_dir, exist_ok=True)\n    \n    def _get_cache_path(self, key):\n        \"\"\"Convert a key to a file path\"\"\"\n        # Use a hash to create a safe filename\n        key_hash = hashlib.md5(str(key).encode()).hexdigest()\n        return os.path.join(self.cache_dir, f\"{key_hash}.json\")\n    \n    def get(self, key):\n        \"\"\"\n        Get a value from the cache.\n        \n        Args:\n            key: Cache key\n            \n        Returns:\n            Cached value or None if not found\n        \"\"\"\n        cache_path = self._get_cache_path(key)\n        \n        if os.path.exists(cache_path):\n            try:\n                with open(cache_path, 'r') as f:\n                    self.hits += 1\n                    return self.decoder(f.read())\n            except (json.JSONDecodeError, IOError) as e:\n                print(f\"Error reading cache file: {e}\")\n        \n        self.misses += 1\n        return None\n    \n    def put(self, key, value):\n        \"\"\"\n        Store a value in the cache.\n        \n        Args:\n            key: Cache key\n            value: Value to store\n        \"\"\"\n        cache_path = self._get_cache_path(key)\n        \n        try:\n            with open(cache_path, 'w') as f:\n                f.write(self.encoder(value))\n            return True\n        except (TypeError, IOError) as e:\n            print(f\"Error writing to cache file: {e}\")\n            return False\n    \n    def clear(self):\n        \"\"\"Clear all cached items.\"\"\"\n        for filename in os.listdir(self.cache_dir):\n            if filename.endswith('.json'):\n                os.remove(os.path.join(self.cache_dir, filename))\n        self.hits = 0\n        self.misses = 0\n    \n    def info(self):\n        \"\"\"Return cache statistics.\"\"\"\n        cache_files = [f for f in os.listdir(self.cache_dir) if f.endswith('.json')]\n        total_size = sum(os.path.getsize(os.path.join(self.cache_dir, f)) for f in cache_files)\n        \n        total_accesses = self.hits + self.misses\n        hit_ratio = self.hits / total_accesses if total_accesses > 0 else 0\n        \n        return {\n            \"items\": len(cache_files),\n            \"size_bytes\": total_size,\n            \"hits\": self.hits,\n            \"misses\": self.misses,\n            \"hit_ratio\": hit_ratio\n        }\n\n# Create a disk cache decorator\ndef disk_cache(cache_dir=None):\n    \"\"\"\n    Create a decorator for persistent disk caching.\n    \n    Args:\n        cache_dir: Directory to store cache files\n        \n    Returns:\n        Decorator function\n    \"\"\"\n    cache = DiskCache(cache_dir=cache_dir)\n    \n    def decorator(func):\n        @functools.wraps(func)\n        def wrapper(*args, **kwargs):\n            # Create a key from the function name and arguments\n            key = {\n                \"func\": func.__name__,\n                \"args\": args,\n                \"kwargs\": sorted(kwargs.items())\n            }\n            \n            # Try to get from cache\n            result = cache.get(key)\n            if result is not None:\n                return result\n            \n            # Compute and store the result\n            result = func(*args, **kwargs)\n            cache.put(key, result)\n            return result\n        \n        # Add methods to access cache information\n        def cache_info():\n            return cache.info()\n        \n        def cache_clear():\n            cache.clear()\n        \n        wrapper.cache_info = cache_info\n        wrapper.cache_clear = cache_clear\n        \n        return wrapper\n    \n    return decorator",
   "metadata": {},
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### 3.2 Using Persistent Caching in Pedigree Reconstruction\n\nLet's apply our disk cache to a function that simulates a time-consuming pedigree reconstruction operation, such as inferring relationships from IBD segments:",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Create a cache directory for our examples\nexample_cache_dir = os.path.join(RESULTS_DIR, 'example_cache')\nos.makedirs(example_cache_dir, exist_ok=True)\n\n# Define an expensive function that simulates relationship inference\ndef infer_relationship(ibd_segments, iid1, iid2):\n    \"\"\"\n    Infer the relationship between two individuals based on IBD segments.\n    This is a simplified simulation of a computationally expensive operation.\n    \n    Args:\n        ibd_segments: List of (start, end, cM) tuples representing IBD segments\n        iid1: ID of the first individual\n        iid2: ID of the second individual\n        \n    Returns:\n        Dict with relationship information\n    \"\"\"\n    # Simulate computation time\n    time.sleep(0.5)  # This would be much longer in reality\n    \n    # Calculate total cM shared\n    total_cm = sum(segment[2] for segment in ibd_segments)\n    \n    # Infer relationship based on total cM (simplified)\n    relationship = None\n    confidence = None\n    \n    if total_cm > 3000:\n        relationship = \"parent-child\"\n        confidence = 0.99\n    elif total_cm > 2000:\n        relationship = \"full-sibling\"\n        confidence = 0.95\n    elif total_cm > 1000:\n        relationship = \"half-sibling/grandparent\"\n        confidence = 0.9\n    elif total_cm > 500:\n        relationship = \"1st cousin\"\n        confidence = 0.85\n    elif total_cm > 250:\n        relationship = \"2nd cousin\"\n        confidence = 0.8\n    elif total_cm > 100:\n        relationship = \"3rd cousin\"\n        confidence = 0.7\n    else:\n        relationship = \"distant\"\n        confidence = 0.5\n    \n    return {\n        \"relationship\": relationship,\n        \"confidence\": confidence,\n        \"total_cm\": total_cm,\n        \"segments\": len(ibd_segments)\n    }\n\n# Create a version with disk caching\n@disk_cache(cache_dir=example_cache_dir)\ndef infer_relationship_cached(ibd_segments, iid1, iid2):\n    \"\"\"Cached version of the relationship inference function.\"\"\"\n    # Convert IBD segments to a hashable representation for the cache key\n    segments_hashable = tuple((start, end, cM) for start, end, cM in ibd_segments)\n    \n    # Call the original function\n    return infer_relationship(segments_hashable, iid1, iid2)\n\n# Generate some random IBD segments\ndef generate_random_ibd_segments(num_segments=5, relationship=\"distant\"):\n    \"\"\"Generate random IBD segments consistent with a given relationship.\"\"\"\n    segments = []\n    \n    # Set cM range based on relationship\n    if relationship == \"parent-child\":\n        cm_range = (100, 200)\n    elif relationship == \"full-sibling\":\n        cm_range = (75, 150)\n    elif relationship == \"half-sibling\":\n        cm_range = (50, 100)\n    elif relationship == \"1st cousin\":\n        cm_range = (20, 50)\n    elif relationship == \"2nd cousin\":\n        cm_range = (10, 30)\n    else:  # distant\n        cm_range = (5, 15)\n    \n    # Generate segments\n    for _ in range(num_segments):\n        start = random.randint(1, 250000000)\n        end = start + random.randint(5000000, 20000000)\n        cM = random.uniform(cm_range[0], cm_range[1])\n        segments.append((start, end, cM))\n    \n    return segments\n\n# Benchmark the cached vs non-cached version\ndef benchmark_persistent_cache(num_pairs=5, num_repeats=3):\n    \"\"\"\n    Benchmark persistent caching for relationship inference.\n    \n    Args:\n        num_pairs: Number of distinct individual pairs to test\n        num_repeats: Number of times to repeat each inference\n        \n    Returns:\n        Dict with benchmark results\n    \"\"\"\n    # Clear the cache\n    infer_relationship_cached.cache_clear()\n    \n    # Generate test data\n    test_data = []\n    for i in range(num_pairs):\n        relationship = random.choice([\"parent-child\", \"full-sibling\", \"half-sibling\", \"1st cousin\", \"2nd cousin\", \"distant\"])\n        segments = generate_random_ibd_segments(num_segments=random.randint(3, 10), relationship=relationship)\n        iid1 = f\"ind_{i*2}\"\n        iid2 = f\"ind_{i*2+1}\"\n        test_data.append((segments, iid1, iid2))\n    \n    # Test without caching\n    start_time = time.time()\n    non_cached_results = []\n    \n    for _ in range(num_repeats):\n        for segments, iid1, iid2 in test_data:\n            result = infer_relationship(segments, iid1, iid2)\n            non_cached_results.append(result)\n    \n    non_cached_time = time.time() - start_time\n    \n    # Test with caching\n    start_time = time.time()\n    cached_results = []\n    \n    for _ in range(num_repeats):\n        for segments, iid1, iid2 in test_data:\n            result = infer_relationship_cached(segments, iid1, iid2)\n            cached_results.append(result)\n    \n    cached_time = time.time() - start_time\n    \n    # Calculate speedup\n    speedup = non_cached_time / cached_time if cached_time > 0 else float('inf')\n    \n    # Get cache info\n    cache_info = infer_relationship_cached.cache_info()\n    \n    return {\n        \"num_pairs\": num_pairs,\n        \"num_repeats\": num_repeats,\n        \"non_cached_time\": non_cached_time,\n        \"cached_time\": cached_time,\n        \"speedup\": speedup,\n        \"cache_info\": cache_info\n    }\n\n# Run the benchmark and visualize results\nbenchmark_results = benchmark_persistent_cache(num_pairs=5, num_repeats=3)\n\nprint(\"Persistent Cache Benchmark Results:\")\nprint(f\"Number of pairs: {benchmark_results['num_pairs']}\")\nprint(f\"Number of repeats: {benchmark_results['num_repeats']}\")\nprint(f\"Non-cached time: {benchmark_results['non_cached_time']:.3f} seconds\")\nprint(f\"Cached time: {benchmark_results['cached_time']:.3f} seconds\")\nprint(f\"Speedup: {benchmark_results['speedup']:.2f}x\")\nprint(f\"Cache hits: {benchmark_results['cache_info']['hits']}\")\nprint(f\"Cache misses: {benchmark_results['cache_info']['misses']}\")\nprint(f\"Cache hit ratio: {benchmark_results['cache_info']['hit_ratio']:.2%}\")\nprint(f\"Cache items: {benchmark_results['cache_info']['items']}\")\nprint(f\"Cache size: {benchmark_results['cache_info']['size_bytes'] / 1024:.2f} KB\")\n\n# Create a bar chart comparing performance\nplt.figure(figsize=(10, 6))\nlabels = ['Without Cache', 'With Disk Cache']\ntimes = [benchmark_results['non_cached_time'], benchmark_results['cached_time']]\ncolors = ['#ff9999', '#66b3ff']\n\nplt.bar(labels, times, color=colors)\nplt.ylabel('Time (seconds)')\nplt.title('Persistent Caching Performance')\nplt.grid(axis='y', linestyle='--', alpha=0.7)\n\n# Add speed-up annotation\nplt.annotate(f\"Speedup: {benchmark_results['speedup']:.2f}x\",\n             xy=(0.5, max(times) * 0.7),\n             ha='center',\n             bbox=dict(boxstyle=\"round,pad=0.3\", fc=\"#d5f5e3\", ec=\"black\", alpha=0.8))\n\n# Create a small pie chart showing cache hits vs misses\npie_ax = plt.axes([0.7, 0.5, 0.2, 0.2])  # Position the pie chart inside the main figure\nhits = benchmark_results['cache_info']['hits']\nmisses = benchmark_results['cache_info']['misses']\npie_ax.pie([hits, misses], labels=['Hits', 'Misses'], colors=['#66b3ff', '#ff9999'],\n          autopct='%1.1f%%', startangle=90)\npie_ax.set_title('Cache Hits vs Misses')\n\nplt.tight_layout()\nplt.show()",
   "metadata": {},
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Part 4: Hierarchical Caching\n\nFor the most sophisticated caching strategy, Bonsai v3 uses hierarchical caching, combining multiple caching mechanisms for different types of operations. Let's implement a simple hierarchical cache:",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "class HierarchicalCache:\n    \"\"\"\n    A hierarchical cache that combines multiple cache levels.\n    \n    This cache checks multiple levels in order, starting with the fastest.\n    When a value is found, it's stored in all faster caches.\n    \n    Args:\n        caches: List of cache objects in order from fastest to slowest\n    \"\"\"\n    def __init__(self, caches):\n        self.caches = caches\n    \n    def get(self, key):\n        \"\"\"\n        Get a value from the cache, checking each level in order.\n        \n        Args:\n            key: Cache key\n            \n        Returns:\n            Cached value or None if not found\n        \"\"\"\n        # Check each cache level\n        for i, cache in enumerate(self.caches):\n            value = cache.get(key)\n            if value is not None:\n                # Store the value in all faster caches\n                for j in range(i):\n                    self.caches[j].put(key, value)\n                return value\n        \n        return None\n    \n    def put(self, key, value):\n        \"\"\"\n        Store a value in all cache levels.\n        \n        Args:\n            key: Cache key\n            value: Value to store\n        \"\"\"\n        for cache in self.caches:\n            cache.put(key, value)\n    \n    def clear(self):\n        \"\"\"Clear all cache levels.\"\"\"\n        for cache in self.caches:\n            cache.clear()\n    \n    def info(self):\n        \"\"\"Return information about all cache levels.\"\"\"\n        return {f\"level_{i}\": cache.info() for i, cache in enumerate(self.caches)}\n\n# Create a hierarchical cache decorator combining in-memory LRU and disk caching\ndef hierarchical_cache(memory_size=128, cache_dir=None):\n    \"\"\"\n    Create a decorator for hierarchical caching.\n    \n    Args:\n        memory_size: Size of the in-memory LRU cache\n        cache_dir: Directory for the disk cache\n        \n    Returns:\n        Decorator function\n    \"\"\"\n    memory_cache = LRUCache(max_size=memory_size)\n    disk_cache_obj = DiskCache(cache_dir=cache_dir)\n    hierarchical_cache_obj = HierarchicalCache([memory_cache, disk_cache_obj])\n    \n    def decorator(func):\n        @functools.wraps(func)\n        def wrapper(*args, **kwargs):\n            # Create a key from the function name and arguments\n            key = {\n                \"func\": func.__name__,\n                \"args\": args,\n                \"kwargs\": sorted(kwargs.items())\n            }\n            \n            # Try to get from cache\n            result = hierarchical_cache_obj.get(key)\n            if result is not None:\n                return result\n            \n            # Compute and store the result\n            result = func(*args, **kwargs)\n            hierarchical_cache_obj.put(key, result)\n            return result\n        \n        # Add methods to access cache information\n        def cache_info():\n            return hierarchical_cache_obj.info()\n        \n        def cache_clear():\n            hierarchical_cache_obj.clear()\n        \n        wrapper.cache_info = cache_info\n        wrapper.cache_clear = cache_clear\n        \n        return wrapper\n    \n    return decorator",
   "metadata": {},
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### 4.1 Benchmarking Hierarchical Caching\n\nLet's benchmark our hierarchical cache to see how it compares to single-level caching approaches:",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Create a directory for hierarchical cache testing\nhierarchical_cache_dir = os.path.join(RESULTS_DIR, 'hierarchical_cache')\nos.makedirs(hierarchical_cache_dir, exist_ok=True)\n\n# Create different cached versions of our relationship inference function\n@lru_cache(maxsize=50)\ndef infer_relationship_lru(ibd_segments, iid1, iid2):\n    \"\"\"LRU-cached version of relationship inference.\"\"\"\n    segments_hashable = tuple((start, end, cM) for start, end, cM in ibd_segments)\n    return infer_relationship(segments_hashable, iid1, iid2)\n\n@disk_cache(cache_dir=hierarchical_cache_dir)\ndef infer_relationship_disk(ibd_segments, iid1, iid2):\n    \"\"\"Disk-cached version of relationship inference.\"\"\"\n    segments_hashable = tuple((start, end, cM) for start, end, cM in ibd_segments)\n    return infer_relationship(segments_hashable, iid1, iid2)\n\n@hierarchical_cache(memory_size=50, cache_dir=hierarchical_cache_dir)\ndef infer_relationship_hierarchical(ibd_segments, iid1, iid2):\n    \"\"\"Hierarchically-cached version of relationship inference.\"\"\"\n    segments_hashable = tuple((start, end, cM) for start, end, cM in ibd_segments)\n    return infer_relationship(segments_hashable, iid1, iid2)\n\n# Create a benchmark function\ndef benchmark_caching_strategies(num_pairs=20, num_repeats=3):\n    \"\"\"\n    Benchmark different caching strategies.\n    \n    Args:\n        num_pairs: Number of distinct individual pairs to test\n        num_repeats: Number of times to repeat each inference\n        \n    Returns:\n        Dict with benchmark results\n    \"\"\"\n    # Clear all caches\n    infer_relationship_lru.cache_clear()\n    infer_relationship_disk.cache_clear()\n    infer_relationship_hierarchical.cache_clear()\n    \n    # Generate test data\n    test_data = []\n    for i in range(num_pairs):\n        relationship = random.choice([\"parent-child\", \"full-sibling\", \"half-sibling\", \"1st cousin\", \"2nd cousin\", \"distant\"])\n        segments = generate_random_ibd_segments(num_segments=random.randint(3, 10), relationship=relationship)\n        iid1 = f\"ind_{i*2}\"\n        iid2 = f\"ind_{i*2+1}\"\n        test_data.append((segments, iid1, iid2))\n    \n    # Function to test a specific caching strategy\n    def test_strategy(inference_func, name):\n        start_time = time.time()\n        results = []\n        \n        for _ in range(num_repeats):\n            for segments, iid1, iid2 in test_data:\n                result = inference_func(segments, iid1, iid2)\n                results.append(result)\n        \n        elapsed_time = time.time() - start_time\n        \n        return {\n            \"name\": name,\n            \"time\": elapsed_time\n        }\n    \n    # Test each strategy\n    no_cache_result = test_strategy(infer_relationship, \"No Cache\")\n    lru_result = test_strategy(infer_relationship_lru, \"LRU Cache\")\n    disk_result = test_strategy(infer_relationship_disk, \"Disk Cache\")\n    hierarchical_result = test_strategy(infer_relationship_hierarchical, \"Hierarchical Cache\")\n    \n    # Calculate speedups\n    baseline_time = no_cache_result[\"time\"]\n    lru_speedup = baseline_time / lru_result[\"time\"] if lru_result[\"time\"] > 0 else float('inf')\n    disk_speedup = baseline_time / disk_result[\"time\"] if disk_result[\"time\"] > 0 else float('inf')\n    hierarchical_speedup = baseline_time / hierarchical_result[\"time\"] if hierarchical_result[\"time\"] > 0 else float('inf')\n    \n    return {\n        \"num_pairs\": num_pairs,\n        \"num_repeats\": num_repeats,\n        \"no_cache\": no_cache_result,\n        \"lru_cache\": lru_result,\n        \"disk_cache\": disk_result,\n        \"hierarchical_cache\": hierarchical_result,\n        \"lru_speedup\": lru_speedup,\n        \"disk_speedup\": disk_speedup,\n        \"hierarchical_speedup\": hierarchical_speedup\n    }\n\n# Run the benchmark\nprint(\"Running caching strategies benchmark...\")\nbenchmark_results = benchmark_caching_strategies(num_pairs=20, num_repeats=3)\n\n# Display results\nprint(\"\\nCaching Strategies Benchmark Results:\")\nprint(f\"Number of pairs: {benchmark_results['num_pairs']}\")\nprint(f\"Number of repeats: {benchmark_results['num_repeats']}\")\nprint(\"\\nExecution times:\")\nprint(f\"No Cache: {benchmark_results['no_cache']['time']:.3f} seconds\")\nprint(f\"LRU Cache: {benchmark_results['lru_cache']['time']:.3f} seconds (speedup: {benchmark_results['lru_speedup']:.2f}x)\")\nprint(f\"Disk Cache: {benchmark_results['disk_cache']['time']:.3f} seconds (speedup: {benchmark_results['disk_speedup']:.2f}x)\")\nprint(f\"Hierarchical Cache: {benchmark_results['hierarchical_cache']['time']:.3f} seconds (speedup: {benchmark_results['hierarchical_speedup']:.2f}x)\")\n\n# Create a visualization of the results\nplt.figure(figsize=(12, 8))\n\n# Bar chart of execution times\nlabels = ['No Cache', 'LRU Cache', 'Disk Cache', 'Hierarchical Cache']\ntimes = [\n    benchmark_results['no_cache']['time'],\n    benchmark_results['lru_cache']['time'],\n    benchmark_results['disk_cache']['time'],\n    benchmark_results['hierarchical_cache']['time']\n]\ncolors = ['#ff9999', '#66b3ff', '#99ff99', '#ffcc99']\n\nbar_plot = plt.bar(labels, times, color=colors)\nplt.ylabel('Time (seconds)')\nplt.title('Caching Strategy Performance Comparison')\nplt.grid(axis='y', linestyle='--', alpha=0.7)\n\n# Add speedup annotations\nfor i, (bar, time_val, name) in enumerate(zip(bar_plot, times, labels)):\n    if i > 0:  # Skip \"No Cache\"\n        speedup = getattr(benchmark_results, f\"{name.lower().replace(' ', '_')}_speedup\")\n        plt.text(bar.get_x() + bar.get_width()/2., time_val + 0.1,\n                 f'{speedup:.2f}x',\n                 ha='center', va='bottom',\n                 bbox=dict(boxstyle=\"round,pad=0.2\", fc=\"#d5f5e3\", ec=\"black\", alpha=0.8))\n\nplt.tight_layout()\nplt.show()\n\n# Create a secondary plot showing relative performance\nplt.figure(figsize=(10, 6))\nbaseline_time = benchmark_results['no_cache']['time']\nrelative_times = [t / baseline_time * 100 for t in times]\n\nplt.bar(labels, relative_times, color=colors)\nplt.ylabel('Relative Time (%)')\nplt.title('Relative Performance of Caching Strategies')\nplt.grid(axis='y', linestyle='--', alpha=0.7)\nplt.axhline(y=100, color='r', linestyle='--')\nplt.ylim(0, 110)\n\nfor i, (rel_time, label) in enumerate(zip(relative_times, labels)):\n    plt.text(i, rel_time + 2, f'{rel_time:.1f}%', ha='center')\n\nplt.tight_layout()\nplt.show()",
   "metadata": {},
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Summary\n\nIn this lab, we explored the caching mechanisms used in Bonsai v3 to improve computational efficiency:\n\n1. **Memoization**: Simple in-memory caching that stores function results for reuse. It provides significant speedups for recursive and repetitive calculations but can lead to unbounded memory usage.\n\n2. **LRU Cache**: Fixed-size cache that evicts least recently used items when full. It balances memory usage with performance, providing good caching benefits even with limited memory.\n\n3. **Persistent Caching**: Stores computation results to disk, allowing them to persist between runs. This is especially valuable for long-running pedigree reconstruction tasks that may be split across multiple sessions.\n\n4. **Hierarchical Caching**: Combines multiple caching strategies for optimal performance. Fast in-memory caches provide quick access to recently used items, while slower persistent caches maintain a larger history.\n\nBy implementing these caching strategies effectively, Bonsai v3 can dramatically improve performance in pedigree reconstruction tasks, especially for large pedigrees with many relationship calculations.\n\n### Key Takeaways\n\n- Caching is particularly effective for recursive operations like finding ancestors or calculating relationships\n- Different caching strategies have different strengths and weaknesses:\n  - Memoization: Simple but potentially memory-hungry\n  - LRU Cache: Memory-efficient but limited capacity\n  - Disk Cache: Persistent but slower access\n  - Hierarchical Cache: Best overall performance but most complex\n- The right caching strategy depends on the specific task and available resources\n- Bonsai v3 uses all these strategies in different contexts to optimize performance\n\n### Related Concepts\n\nIn the next lab, we'll explore error handling and data validation mechanisms in Bonsai v3, which ensure robust performance even with imperfect data inputs.",
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}