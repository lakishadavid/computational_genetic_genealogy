{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 18: Optimization Techniques and Performance Enhancements\n",
    "\n",
    "## Overview\n",
    "\n",
    "In this lab, we'll explore the sophisticated optimization techniques used in Bonsai v3 to enable efficient pedigree reconstruction at scale. These techniques are essential for handling real-world genetic genealogy datasets with hundreds or thousands of individuals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard imports\n",
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import networkx as nx\n",
    "from IPython.display import display, HTML, Markdown\n",
    "import inspect\n",
    "import importlib\n",
    "import copy\n",
    "import random\n",
    "import math\n",
    "import time\n",
    "import concurrent.futures\n",
    "import contextlib\n",
    "from collections import defaultdict\n",
    "\n",
    "sys.path.append(os.path.dirname(os.getcwd()))\n",
    "\n",
    "# Cross-compatibility setup\n",
    "from scripts_support.lab_cross_compatibility import setup_environment, is_jupyterlite, save_results, save_plot\n",
    "\n",
    "# Set up environment-specific paths\n",
    "DATA_DIR, RESULTS_DIR = setup_environment()\n",
    "\n",
    "# Set visualization styles\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "sns.set_context(\"notebook\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup Bonsai module paths\n",
    "if not is_jupyterlite():\n",
    "    # In local environment, add the utils directory to system path\n",
    "    utils_dir = os.getenv('PROJECT_UTILS_DIR', os.path.join(os.path.dirname(DATA_DIR), 'utils'))\n",
    "    bonsaitree_dir = os.path.join(utils_dir, 'bonsaitree')\n",
    "    \n",
    "    # Add to path if it exists and isn't already there\n",
    "    if os.path.exists(bonsaitree_dir) and bonsaitree_dir not in sys.path:\n",
    "        sys.path.append(bonsaitree_dir)\n",
    "        print(f\"Added {bonsaitree_dir} to sys.path\")\n",
    "else:\n",
    "    # In JupyterLite, use a simplified approach\n",
    "    print(\"\u26a0\ufe0f Running in JupyterLite: Some Bonsai functionality may be limited.\")\n",
    "    print(\"This notebook is primarily designed for local execution where the Bonsai codebase is available.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions for exploring modules\n",
    "def display_module_classes(module_name):\n",
    "    \"\"\"Display classes and their docstrings from a module\"\"\"\n",
    "    try:\n",
    "        # Import the module\n",
    "        module = importlib.import_module(module_name)\n",
    "        \n",
    "        # Find all classes\n",
    "        classes = inspect.getmembers(module, inspect.isclass)\n",
    "        \n",
    "        # Filter classes defined in this module (not imported)\n",
    "        classes = [(name, cls) for name, cls in classes if cls.__module__ == module_name]\n",
    "        \n",
    "        # Print info for each class\n",
    "        for name, cls in classes:\n",
    "            print(f\"\\\n## {name}\")\n",
    "            \n",
    "            # Get docstring\n",
    "            doc = inspect.getdoc(cls)\n",
    "            if doc:\n",
    "                print(f\"Docstring: {doc}\")\n",
    "            else:\n",
    "                print(\"No docstring available\")\n",
    "            \n",
    "            # Get methods\n",
    "            methods = inspect.getmembers(cls, inspect.isfunction)\n",
    "            if methods:\n",
    "                print(\"\\\nMethods:\")\n",
    "                for method_name, method in methods:\n",
    "                    if not method_name.startswith('_'):  # Skip private methods\n",
    "                        print(f\"- {method_name}\")\n",
    "    except ImportError as e:\n",
    "        print(f\"Error importing module {module_name}: {e}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing module {module_name}: {e}\")\n",
    "\n",
    "def display_module_functions(module_name):\n",
    "    \"\"\"Display functions and their docstrings from a module\"\"\"\n",
    "    try:\n",
    "        # Import the module\n",
    "        module = importlib.import_module(module_name)\n",
    "        \n",
    "        # Find all functions\n",
    "        functions = inspect.getmembers(module, inspect.isfunction)\n",
    "        \n",
    "        # Filter functions defined in this module (not imported)\n",
    "        functions = [(name, func) for name, func in functions if func.__module__ == module_name]\n",
    "        \n",
    "        # Print info for each function\n",
    "        for name, func in functions:\n",
    "            if name.startswith('_'):  # Skip private functions\n",
    "                continue\n",
    "                \n",
    "            print(f\"\\\n## {name}\")\n",
    "            \n",
    "            # Get signature\n",
    "            sig = inspect.signature(func)\n",
    "            print(f\"Signature: {name}{sig}\")\n",
    "            \n",
    "            # Get docstring\n",
    "            doc = inspect.getdoc(func)\n",
    "            if doc:\n",
    "                print(f\"Docstring: {doc}\")\n",
    "            else:\n",
    "                print(\"No docstring available\")\n",
    "    except ImportError as e:\n",
    "        print(f\"Error importing module {module_name}: {e}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing module {module_name}: {e}\")\n",
    "\n",
    "def view_source(obj):\n",
    "    \"\"\"Display the source code of an object (function or class)\"\"\"\n",
    "    try:\n",
    "        source = inspect.getsource(obj)\n",
    "        display(Markdown(f\"```python\\\n{source}\\\n```\"))\n",
    "    except Exception as e:\n",
    "        print(f\"Error retrieving source: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check Bonsai Installation\n",
    "\n",
    "Let's verify that the Bonsai v3 module is available for import:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    from utils.bonsaitree.bonsaitree import v3\n",
    "    print(\"\u2705 Successfully imported Bonsai v3 module\")\n",
    "except ImportError as e:\n",
    "    print(f\"\u274c Failed to import Bonsai v3 module: {e}\")\n",
    "    print(\"This lab requires access to the Bonsai v3 codebase.\")\n",
    "    print(\"Make sure you've properly set up your environment with the Bonsai repository.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Lab 18: Optimization Techniques and Performance Enhancements\n\nIn this lab, we'll explore the optimization techniques that enable Bonsai v3 to efficiently handle large-scale pedigree reconstruction problems. These techniques are essential for processing real-world genetic genealogy datasets that can include hundreds or thousands of individuals.\n\nKey optimization areas we'll explore include:\n\n1. **Search Space Pruning**: Reducing the number of pedigree configurations to evaluate\n2. **Parallel Processing**: Leveraging multiple CPU cores for faster computation\n3. **Adaptive Parameter Selection**: Optimizing algorithm parameters based on dataset characteristics\n4. **Specialized Data Structures**: Using memory-efficient representations of genetic and pedigree data\n5. **Early Termination and Lazy Evaluation**: Avoiding unnecessary computation\n\nWe'll implement simplified versions of these optimization techniques to understand how they work and why they're important for large-scale pedigree reconstruction.",
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Part 1: Search Space Pruning\n\nPedigree reconstruction from genetic data involves exploring a vast combinatorial space of possible pedigree configurations. The number of possible configurations grows exponentially with the number of individuals, making exhaustive search infeasible for real-world datasets. Search space pruning is therefore essential for efficient pedigree reconstruction.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "# Import Bonsai search space pruning functions if available\nif not is_jupyterlite():\n    try:\n        from utils.bonsaitree.bonsaitree.v3.connections import prune_search_space\n        \n        # Display the source code if available\n        print(\"Source code for prune_search_space:\")\n        view_source(prune_search_space)\n    except (ImportError, AttributeError) as e:\n        print(f\"Could not import function: {e}\")\nelse:\n    print(\"Cannot display source code in JupyterLite environment.\")"
   ],
   "metadata": {},
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### 1.1 Connectivity-Based Clustering\n\nOne of the most effective search space pruning techniques in Bonsai v3 is connectivity-based clustering. This approach groups individuals into clusters based on their IBD connectivity, allowing Bonsai to focus on reconstructing each cluster separately before merging them. Let's implement a simplified version of this approach:",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "def build_connectivity_graph(id_to_shared_ibd, min_ibd_threshold=50):\n    \"\"\"\n    Build a graph representing IBD connectivity between individuals.\n    \n    Args:\n        id_to_shared_ibd: Dict mapping ID pairs to their IBD segments\n        min_ibd_threshold: Minimum IBD amount (in cM) to consider for connectivity\n        \n    Returns:\n        G: NetworkX graph with edges representing IBD connections\n    \"\"\"\n    G = nx.Graph()\n    \n    # Add edges for each pair with IBD sharing above threshold\n    for (id1, id2), segments in id_to_shared_ibd.items():\n        # Calculate total IBD\n        total_cm = sum(seg.get('length_cm', 0) for seg in segments)\n        \n        # Add edge if above threshold\n        if total_cm >= min_ibd_threshold:\n            G.add_edge(id1, id2, weight=total_cm)\n    \n    return G\n\ndef identify_related_clusters(connectivity_graph, min_size=2):\n    \"\"\"\n    Identify clusters of related individuals based on IBD connectivity.\n    \n    Args:\n        connectivity_graph: NetworkX graph with edges representing IBD connections\n        min_size: Minimum cluster size to return\n        \n    Returns:\n        clusters: List of sets containing individual IDs in each cluster\n    \"\"\"\n    # Find connected components in the graph\n    components = list(nx.connected_components(connectivity_graph))\n    \n    # Filter by minimum size\n    clusters = [comp for comp in components if len(comp) >= min_size]\n    \n    # Sort by size (descending)\n    clusters.sort(key=len, reverse=True)\n    \n    return clusters\n\ndef visualize_clusters(connectivity_graph, clusters, title=\"IBD Connectivity Clusters\"):\n    \"\"\"\n    Visualize IBD connectivity clusters.\n    \n    Args:\n        connectivity_graph: NetworkX graph with edges representing IBD connections\n        clusters: List of sets containing individual IDs in each cluster\n        title: Title for the visualization\n    \"\"\"\n    plt.figure(figsize=(12, 8))\n    plt.title(title)\n    \n    # Position nodes using force-directed layout\n    pos = nx.spring_layout(connectivity_graph, seed=42)\n    \n    # Generate colors for clusters\n    colors = plt.cm.tab10(np.linspace(0, 1, len(clusters)))\n    \n    # Draw each cluster with a different color\n    for i, cluster in enumerate(clusters):\n        nx.draw_networkx_nodes(\n            connectivity_graph, \n            pos, \n            nodelist=list(cluster),\n            node_color=[colors[i]],\n            node_size=200,\n            alpha=0.8\n        )\n    \n    # Draw edges with width proportional to weight\n    edge_weights = [connectivity_graph[u][v]['weight'] / 500 for u, v in connectivity_graph.edges()]\n    nx.draw_networkx_edges(\n        connectivity_graph, \n        pos, \n        width=edge_weights, \n        alpha=0.5\n    )\n    \n    # Draw labels\n    nx.draw_networkx_labels(connectivity_graph, pos, font_size=10)\n    \n    plt.axis('off')\n    plt.tight_layout()\n    plt.show()"
   ],
   "metadata": {},
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Demonstrate connectivity-based clustering\ndef generate_sample_ibd_data(num_individuals=30, num_families=3, family_size=10):\n    \"\"\"\n    Generate sample IBD data with a family structure.\n    \n    Args:\n        num_individuals: Total number of individuals\n        num_families: Number of distinct families\n        family_size: Approximate size of each family\n        \n    Returns:\n        id_to_shared_ibd: Dict mapping ID pairs to their IBD segments\n    \"\"\"\n    id_to_shared_ibd = {}\n    \n    # Ensure family_size * num_families <= num_individuals\n    actual_family_size = min(family_size, num_individuals // num_families)\n    \n    # Create families\n    families = []\n    remaining_ids = list(range(1, num_individuals + 1))\n    \n    for i in range(num_families):\n        # Determine this family's size\n        if i == num_families - 1:\n            # Last family gets all remaining individuals\n            size = len(remaining_ids)\n        else:\n            # Randomize family size slightly\n            size = min(actual_family_size + random.randint(-2, 2), len(remaining_ids))\n            size = max(3, size)  # Ensure at least 3 members\n        \n        # Select members for this family\n        family = random.sample(remaining_ids, size)\n        families.append(family)\n        \n        # Remove selected IDs from remaining\n        for id_val in family:\n            remaining_ids.remove(id_val)\n    \n    # Add any remaining individuals to random families\n    for id_val in remaining_ids:\n        family_idx = random.randint(0, num_families - 1)\n        families[family_idx].append(id_val)\n    \n    # Generate IBD sharing within families\n    for family in families:\n        # Create close relationships within the family\n        for i in range(len(family)):\n            for j in range(i + 1, len(family)):\n                id1, id2 = family[i], family[j]\n                \n                # Determine relationship based on position in family\n                pos_diff = abs(i - j)\n                if pos_diff == 1:\n                    # Adjacent positions - create parent-child or sibling relationship\n                    ibd_amount = random.uniform(1700, 2600)  # Parent-child or full sibling range\n                elif pos_diff == 2:\n                    # Two steps away - create grandparent or aunt/uncle relationship\n                    ibd_amount = random.uniform(700, 1600)  # Grandparent or avuncular range\n                else:\n                    # More distant relationship\n                    ibd_amount = random.uniform(200, 700) / pos_diff  # More distant = less IBD\n                \n                # Create a segment\n                segment = {\n                    'chrom': 1,\n                    'start_cm': 0,\n                    'end_cm': ibd_amount,\n                    'length_cm': ibd_amount\n                }\n                \n                # Ensure id1 < id2 for consistent keys\n                pair = (min(id1, id2), max(id1, id2))\n                if pair not in id_to_shared_ibd:\n                    id_to_shared_ibd[pair] = []\n                id_to_shared_ibd[pair].append(segment)\n    \n    # Add some cross-family relationships (more distant)\n    num_cross_relationships = num_families * 2\n    \n    for _ in range(num_cross_relationships):\n        # Select two different families\n        fam1_idx, fam2_idx = random.sample(range(num_families), 2)\n        \n        # Select one member from each family\n        id1 = random.choice(families[fam1_idx])\n        id2 = random.choice(families[fam2_idx])\n        \n        # Create a more distant relationship\n        ibd_amount = random.uniform(50, 300)  # Distant cousin range\n        \n        # Create a segment\n        segment = {\n            'chrom': 1,\n            'start_cm': 0,\n            'end_cm': ibd_amount,\n            'length_cm': ibd_amount\n        }\n        \n        # Ensure id1 < id2 for consistent keys\n        pair = (min(id1, id2), max(id1, id2))\n        if pair not in id_to_shared_ibd:\n            id_to_shared_ibd[pair] = []\n        id_to_shared_ibd[pair].append(segment)\n    \n    return id_to_shared_ibd\n\n# Set random seed for reproducibility\nrandom.seed(42)\n\n# Generate sample IBD data\nid_to_shared_ibd = generate_sample_ibd_data(\n    num_individuals=30,\n    num_families=3,\n    family_size=10\n)\n\n# Print summary of IBD data\nprint(f\"Generated IBD data with {len(id_to_shared_ibd)} pairs\")\n\n# Build connectivity graph\nconnectivity_graph = build_connectivity_graph(id_to_shared_ibd, min_ibd_threshold=100)\nprint(f\"Connectivity graph has {len(connectivity_graph.nodes())} nodes and {len(connectivity_graph.edges())} edges\")\n\n# Identify clusters\nclusters = identify_related_clusters(connectivity_graph)\nprint(f\"Identified {len(clusters)} clusters:\")\nfor i, cluster in enumerate(clusters):\n    print(f\"Cluster {i+1}: {len(cluster)} individuals - {sorted(cluster)}\")\n\n# Visualize clusters\nvisualize_clusters(connectivity_graph, clusters, \"Sample IBD Connectivity Clusters\")"
   ],
   "metadata": {},
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### 1.2 Demographic Constraints\n\nAnother important search space pruning technique is the use of demographic constraints to rule out impossible relationships. By leveraging age, sex, and other demographic information, Bonsai can eliminate many implausible pedigree configurations before evaluating them.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "def establish_demographic_constraints(id_to_info):\n    \"\"\"\n    Establish constraints based on demographic information.\n    \n    Args:\n        id_to_info: Dict with demographic information for individuals\n        \n    Returns:\n        constraints: Dict of constraints for relationship types\n    \"\"\"\n    constraints = {\n        'parent_child': [],  # (parent_id, child_id) pairs that are possible\n        'sibling': [],       # (id1, id2) pairs that could be siblings\n        'invalid': []        # (id1, id2) pairs that cannot be directly related\n    }\n    \n    # Get all pairs of individuals\n    ids = list(id_to_info.keys())\n    all_pairs = [(ids[i], ids[j]) for i in range(len(ids)) for j in range(i+1, len(ids))]\n    \n    for id1, id2 in all_pairs:\n        info1 = id_to_info.get(id1, {})\n        info2 = id_to_info.get(id2, {})\n        \n        # Extract demographic information\n        age1 = info1.get('age')\n        age2 = info2.get('age')\n        sex1 = info1.get('sex')\n        sex2 = info2.get('sex')\n        \n        # Check parent-child relationships\n        if age1 is not None and age2 is not None:\n            age_diff = abs(age1 - age2)\n            \n            if age1 > age2 and age_diff >= 15:\n                # id1 could be parent of id2\n                constraints['parent_child'].append((id1, id2))\n            \n            if age2 > age1 and age_diff >= 15:\n                # id2 could be parent of id1\n                constraints['parent_child'].append((id2, id1))\n            \n            # Check for sibling relationship\n            if abs(age_diff) < 30:\n                # Could be siblings\n                constraints['sibling'].append((id1, id2))\n            \n            # Check for impossible direct relationships\n            if age_diff < 12:\n                # Age difference too small for parent-child\n                constraints['invalid'].append(('parent_child', id1, id2))\n                constraints['invalid'].append(('parent_child', id2, id1))\n        \n        # Check sex-based constraints\n        if sex1 is not None and sex2 is not None:\n            # Two males cannot have a child together\n            if sex1 == 'M' and sex2 == 'M':\n                constraints['invalid'].append(('parent_pair', id1, id2))\n            \n            # Two females cannot have a child together\n            if sex1 == 'F' and sex2 == 'F':\n                constraints['invalid'].append(('parent_pair', id1, id2))\n    \n    return constraints\n\ndef apply_demographic_constraints(clusters, id_to_info):\n    \"\"\"\n    Apply demographic constraints to refine clusters.\n    \n    Args:\n        clusters: List of sets containing individual IDs in each cluster\n        id_to_info: Dict with demographic information for individuals\n        \n    Returns:\n        refined_clusters: List of sets with additional demographic constraints applied\n    \"\"\"\n    # Establish constraints\n    constraints = establish_demographic_constraints(id_to_info)\n    \n    # Extract invalid relationships\n    invalid_pairs = [(id1, id2) for rel_type, id1, id2 in constraints['invalid'] \n                    if rel_type == 'parent_child']\n    \n    # Create a new graph for each cluster with demographic constraints\n    refined_clusters = []\n    \n    for cluster in clusters:\n        # Skip small clusters\n        if len(cluster) < 3:\n            refined_clusters.append(cluster)\n            continue\n        \n        # Create a new graph for this cluster\n        G = nx.Graph()\n        G.add_nodes_from(cluster)\n        \n        # Add edges for possible relationships\n        for id1 in cluster:\n            for id2 in cluster:\n                if id1 == id2:\n                    continue\n                    \n                # Skip invalid pairs\n                if (id1, id2) in invalid_pairs or (id2, id1) in invalid_pairs:\n                    continue\n                \n                # Add edge for possible relationship\n                G.add_edge(id1, id2)\n        \n        # Find connected components after constraints\n        sub_clusters = list(nx.connected_components(G))\n        refined_clusters.extend(sub_clusters)\n    \n    return refined_clusters"
   ],
   "metadata": {},
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Demonstrate demographic constraints\ndef generate_sample_demographic_info(num_individuals=30):\n    \"\"\"\n    Generate sample demographic information for individuals.\n    \n    Args:\n        num_individuals: Number of individuals to generate info for\n        \n    Returns:\n        id_to_info: Dict mapping IDs to demographic information\n    \"\"\"\n    id_to_info = {}\n    \n    # Define age ranges for different generations\n    gen1_range = (60, 80)  # Grandparent generation\n    gen2_range = (35, 55)  # Parent generation\n    gen3_range = (10, 30)  # Child generation\n    \n    # Assign generations to individuals\n    gen1_count = num_individuals // 5\n    gen2_count = num_individuals // 3\n    gen3_count = num_individuals - gen1_count - gen2_count\n    \n    generations = ([1] * gen1_count) + ([2] * gen2_count) + ([3] * gen3_count)\n    random.shuffle(generations)\n    \n    for i in range(1, num_individuals + 1):\n        gen = generations[i-1]\n        \n        # Assign age based on generation\n        if gen == 1:\n            age = random.randint(*gen1_range)\n        elif gen == 2:\n            age = random.randint(*gen2_range)\n        else:\n            age = random.randint(*gen3_range)\n        \n        # Assign sex\n        sex = random.choice(['M', 'F'])\n        \n        # Store demographic information\n        id_to_info[i] = {\n            'id': i,\n            'age': age,\n            'sex': sex,\n            'generation': gen\n        }\n    \n    return id_to_info\n\n# Generate demographic information\nid_to_info = generate_sample_demographic_info(30)\n\n# Print summary of demographic information\nprint(\"Sample demographic information:\")\ngenerations = defaultdict(list)\nfor id_val, info in id_to_info.items():\n    generations[info['generation']].append(id_val)\n\nfor gen, ids in sorted(generations.items()):\n    print(f\"Generation {gen} ({len(ids)} individuals): {sorted(ids)}\")\n    \n# Establish demographic constraints\nconstraints = establish_demographic_constraints(id_to_info)\n\n# Print summary of constraints\nprint(\"\\\nDemographic constraints:\")\nprint(f\"Possible parent-child relationships: {len(constraints['parent_child'])}\")\nprint(f\"Possible sibling relationships: {len(constraints['sibling'])}\")\nprint(f\"Invalid relationships: {len(constraints['invalid'])}\")\n\n# Apply demographic constraints to clusters\nrefined_clusters = apply_demographic_constraints(clusters, id_to_info)\n\n# Print summary of refined clusters\nprint(f\"\\\nAfter applying demographic constraints: {len(refined_clusters)} clusters\")\nfor i, cluster in enumerate(refined_clusters):\n    print(f\"Refined Cluster {i+1}: {len(cluster)} individuals - {sorted(cluster)}\")"
   ],
   "metadata": {},
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### 1.3 Combining Search Space Pruning Techniques\n\nNow let's combine these pruning techniques to create a simplified version of Bonsai v3's search space pruning:",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "def prune_search_space_simplified(id_to_shared_ibd, id_to_info):\n    \"\"\"\n    Simplified implementation of Bonsai v3's search space pruning.\n    \n    Args:\n        id_to_shared_ibd: Dict mapping ID pairs to their IBD segments\n        id_to_info: Dict with demographic information for individuals\n        \n    Returns:\n        pruned_space: Dict containing the pruned search space\n    \"\"\"\n    pruned_space = {}\n    \n    # Step 1: Build connectivity graph\n    connectivity_graph = build_connectivity_graph(id_to_shared_ibd, min_ibd_threshold=100)\n    pruned_space['connectivity_graph'] = connectivity_graph\n    \n    # Step 2: Identify initial clusters\n    initial_clusters = identify_related_clusters(connectivity_graph)\n    pruned_space['initial_clusters'] = initial_clusters\n    \n    # Step 3: Apply demographic constraints\n    refined_clusters = apply_demographic_constraints(initial_clusters, id_to_info)\n    pruned_space['refined_clusters'] = refined_clusters\n    \n    # Step 4: Establish search order for clusters\n    # (Prioritize larger clusters and those with more IBD connections)\n    search_order = []\n    \n    # Calculate total IBD within each cluster\n    cluster_scores = []\n    for i, cluster in enumerate(refined_clusters):\n        # Calculate total IBD within cluster\n        total_ibd = 0\n        cluster_list = list(cluster)\n        for j in range(len(cluster_list)):\n            for k in range(j + 1, len(cluster_list)):\n                id1, id2 = cluster_list[j], cluster_list[k]\n                pair = (min(id1, id2), max(id1, id2))\n                if pair in id_to_shared_ibd:\n                    segments = id_to_shared_ibd[pair]\n                    total_ibd += sum(seg.get('length_cm', 0) for seg in segments)\n        \n        # Score is a combination of size and IBD density\n        score = len(cluster) * 100 + total_ibd\n        cluster_scores.append((i, score))\n    \n    # Sort by score (descending)\n    cluster_scores.sort(key=lambda x: x[1], reverse=True)\n    search_order = [refined_clusters[i] for i, _ in cluster_scores]\n    pruned_space['search_order'] = search_order\n    \n    # Step 5: Generate simplified search partitions\n    # (Each partition represents a subset of the search space to explore independently)\n    partitions = []\n    for cluster in search_order:\n        # Create a partition for this cluster\n        partition = {\n            'individuals': list(cluster),\n            'constraints': establish_demographic_constraints({id_val: id_to_info[id_val] for id_val in cluster if id_val in id_to_info})\n        }\n        partitions.append(partition)\n    \n    pruned_space['partitions'] = partitions\n    \n    return pruned_space\n\n# Demonstrate combined pruning\npruned_space = prune_search_space_simplified(id_to_shared_ibd, id_to_info)\n\n# Print summary of pruned search space\nprint(\"Pruned Search Space Summary:\")\nprint(f\"Initial clusters: {len(pruned_space['initial_clusters'])}\")\nprint(f\"Refined clusters after demographic constraints: {len(pruned_space['refined_clusters'])}\")\nprint(f\"Search partitions: {len(pruned_space['partitions'])}\")\n\n# Print search order\nprint(\"\\\nSearch Order:\")\nfor i, cluster in enumerate(pruned_space['search_order']):\n    print(f\"{i+1}. Cluster with {len(cluster)} individuals: {sorted(cluster)}\")\n\n# Calculate search space reduction\ntotal_individuals = len(set().union(*pruned_space['initial_clusters']))\navg_partition_size = sum(len(partition['individuals']) for partition in pruned_space['partitions']) / len(pruned_space['partitions'])\n\n# Naive search space size (exponential in number of individuals)\nnaive_space_size = 2 ** total_individuals\n\n# Pruned search space size (sum of exponentials for each partition)\npruned_space_size = sum(2 ** len(partition['individuals']) for partition in pruned_space['partitions'])\n\n# Calculate reduction\nreduction_factor = naive_space_size / pruned_space_size\n\nprint(f\"\\\nSearch Space Reduction:\")\nprint(f\"Total individuals: {total_individuals}\")\nprint(f\"Average partition size: {avg_partition_size:.1f}\")\nprint(f\"Naive search space size: 2^{total_individuals} \u2248 {naive_space_size:.2e}\")\nprint(f\"Pruned search space size: \u2248 {pruned_space_size:.2e}\")\nprint(f\"Reduction factor: \u2248 {reduction_factor:.2e}x\")"
   ],
   "metadata": {},
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Part 2: Parallel Processing\n\nAnother key optimization technique in Bonsai v3 is parallel processing, which leverages multiple CPU cores to speed up computation. This is particularly important for operations that can be naturally parallelized, such as evaluating multiple pedigree configurations or processing different chromosomes independently.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "class ParallelExecutor:\n    \"\"\"\n    Simplified implementation of Bonsai v3's parallel processing.\n    \"\"\"\n    \n    def __init__(self, max_workers=None, use_processes=False):\n        \"\"\"\n        Initialize the parallel executor.\n        \n        Args:\n            max_workers: Maximum number of worker threads/processes\n            use_processes: Whether to use processes instead of threads\n        \"\"\"\n        self.max_workers = max_workers or min(32, os.cpu_count() + 4)\n        self.use_processes = use_processes\n        self._executor = None\n    \n    def execute(self, tasks):\n        \"\"\"\n        Execute a list of tasks in parallel.\n        \n        Args:\n            tasks: List of (function, args, kwargs) tuples to execute\n            \n        Returns:\n            List of results from all tasks\n        \"\"\"\n        executor_cls = concurrent.futures.ProcessPoolExecutor if self.use_processes else concurrent.futures.ThreadPoolExecutor\n        \n        with executor_cls(max_workers=self.max_workers) as executor:\n            # Submit all tasks\n            futures = []\n            for func, args, kwargs in tasks:\n                future = executor.submit(func, *args, **kwargs)\n                futures.append(future)\n            \n            # Collect results\n            results = []\n            for future in concurrent.futures.as_completed(futures):\n                try:\n                    result = future.result()\n                    results.append(result)\n                except Exception as e:\n                    print(f\"Task failed with error: {e}\")\n                    results.append(None)\n            \n            return results\n\ndef simulate_expensive_calculation(partition, delay=0.1):\n    \"\"\"\n    Simulate an expensive calculation on a partition.\n    \n    Args:\n        partition: A partition from the pruned search space\n        delay: Simulated computation time in seconds\n        \n    Returns:\n        result: Simulated calculation result\n    \"\"\"\n    # Simulate computation time\n    time.sleep(delay)\n    \n    # Simulated calculation: For each individual in the partition,\n    # find all possible relationships with others in the partition\n    individuals = partition['individuals']\n    constraints = partition['constraints']\n    \n    # Calculate number of possible relationship configurations\n    n = len(individuals)\n    possible_configs = 0\n    \n    for i in range(n):\n        for j in range(i+1, n):\n            id1, id2 = individuals[i], individuals[j]\n            \n            # Check if relationship is allowed by constraints\n            invalid_pair = False\n            for rel_type, i1, i2 in constraints.get('invalid', []):\n                if (i1 == id1 and i2 == id2) or (i1 == id2 and i2 == id1):\n                    invalid_pair = True\n                    break\n            \n            if not invalid_pair:\n                # Assume 4 possible relationship types for each valid pair\n                possible_configs += 4\n    \n    return {\n        'partition_size': n,\n        'possible_configs': possible_configs,\n        'total_pairs': n * (n - 1) // 2,\n        'computation_time': delay\n    }"
   ],
   "metadata": {},
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Demonstrate parallel processing\ndef process_all_partitions_sequential(partitions):\n    \"\"\"\n    Process all partitions sequentially.\n    \n    Args:\n        partitions: List of partitions to process\n        \n    Returns:\n        results: List of results for each partition\n        elapsed_time: Time taken in seconds\n    \"\"\"\n    start_time = time.time()\n    \n    results = []\n    for partition in partitions:\n        result = simulate_expensive_calculation(partition)\n        results.append(result)\n    \n    elapsed_time = time.time() - start_time\n    \n    return results, elapsed_time\n\ndef process_all_partitions_parallel(partitions, max_workers=None):\n    \"\"\"\n    Process all partitions in parallel.\n    \n    Args:\n        partitions: List of partitions to process\n        max_workers: Maximum number of worker threads\n        \n    Returns:\n        results: List of results for each partition\n        elapsed_time: Time taken in seconds\n    \"\"\"\n    start_time = time.time()\n    \n    # Create tasks\n    tasks = [(simulate_expensive_calculation, (partition,), {}) for partition in partitions]\n    \n    # Execute tasks in parallel\n    executor = ParallelExecutor(max_workers=max_workers)\n    results = executor.execute(tasks)\n    \n    elapsed_time = time.time() - start_time\n    \n    return results, elapsed_time\n\n# Get partitions from our pruned search space\npartitions = pruned_space['partitions']\n\n# Add a delay to make the speed difference more noticeable\nfor partition in partitions:\n    partition['delay'] = 0.2 * len(partition['individuals']) / 10\n\n# Process sequentially\nprint(\"Processing partitions sequentially...\")\nsequential_results, sequential_time = process_all_partitions_sequential(partitions)\nprint(f\"Sequential processing took {sequential_time:.2f} seconds\")\n\n# Process in parallel\nprint(\"\\\nProcessing partitions in parallel...\")\nparallel_results, parallel_time = process_all_partitions_parallel(partitions)\nprint(f\"Parallel processing took {parallel_time:.2f} seconds\")\n\n# Calculate speedup\nspeedup = sequential_time / parallel_time\nprint(f\"Speedup: {speedup:.2f}x\")\n\n# Compare results\nprint(\"\\\nResults summary:\")\nsequential_configs = sum(result['possible_configs'] for result in sequential_results)\nparallel_configs = sum(result['possible_configs'] for result in parallel_results)\nprint(f\"Sequential processing found {sequential_configs} possible configurations\")\nprint(f\"Parallel processing found {parallel_configs} possible configurations\")\n\n# Plot results\nplt.figure(figsize=(10, 6))\nplt.title(\"Sequential vs. Parallel Processing Time\")\nplt.bar(['Sequential', 'Parallel'], [sequential_time, parallel_time], color=['blue', 'green'])\nplt.ylabel(\"Time (seconds)\")\nplt.grid(axis='y', linestyle='--', alpha=0.7)\nplt.tight_layout()\nplt.show()\n\n# Plot scaling with number of workers\ndef measure_scaling():\n    \"\"\"Measure how performance scales with number of workers\"\"\"\n    worker_counts = [1, 2, 4, 8, 16]\n    times = []\n    \n    for workers in worker_counts:\n        _, elapsed_time = process_all_partitions_parallel(partitions, max_workers=workers)\n        times.append(elapsed_time)\n    \n    return worker_counts, times\n\nworker_counts, times = measure_scaling()\n\nplt.figure(figsize=(10, 6))\nplt.title(\"Scaling with Number of Workers\")\nplt.plot(worker_counts, times, 'o-', linewidth=2, markersize=8)\nplt.xlabel(\"Number of Workers\")\nplt.ylabel(\"Time (seconds)\")\nplt.grid(True, linestyle='--', alpha=0.7)\nplt.tight_layout()\nplt.show()"
   ],
   "metadata": {},
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Part 3: Adaptive Parameter Selection\n\nBonsai v3 uses adaptive parameter selection to optimize performance based on dataset characteristics. Rather than using fixed parameters for all situations, Bonsai dynamically adjusts its parameters based on factors like dataset size, IBD density, and computational resources available.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "def optimize_parameters(id_to_shared_ibd, id_to_info, available_memory=8 * 1024 * 1024 * 1024, available_cores=None):\n    \"\"\"\n    Dynamically optimize algorithm parameters based on dataset characteristics.\n    \n    Args:\n        id_to_shared_ibd: Dict mapping ID pairs to their IBD segments\n        id_to_info: Dict with demographic information for individuals\n        available_memory: Available memory in bytes\n        available_cores: Available CPU cores\n        \n    Returns:\n        optimized_params: Dict of optimized parameter values\n    \"\"\"\n    # Use all available cores if not specified\n    available_cores = available_cores or os.cpu_count()\n    \n    # Get dataset characteristics\n    num_individuals = len(set([id for pair in id_to_shared_ibd for id in pair]))\n    \n    # Calculate IBD density and average length\n    total_pairs = num_individuals * (num_individuals - 1) // 2\n    actual_pairs = len(id_to_shared_ibd)\n    ibd_density = actual_pairs / total_pairs if total_pairs > 0 else 0\n    \n    total_length = 0\n    total_segments = 0\n    for segments in id_to_shared_ibd.values():\n        for segment in segments:\n            total_length += segment.get('length_cm', 0)\n            total_segments += 1\n    \n    avg_ibd_length = total_length / total_segments if total_segments > 0 else 0\n    \n    # Initialize parameters with default values\n    params = {\n        'max_up': 3,             # Maximum generations to extend upward\n        'n_keep': 5,             # Number of top pedigrees to keep\n        'ibd_threshold': 20,     # Minimum IBD amount to consider (cM)\n        'max_iterations': 100,   # Maximum iterations for optimization\n        'batch_size': 10,        # Batch size for parallel processing\n        'use_threading': True,   # Whether to use threading\n    }\n    \n    # Adjust max_up based on IBD density and length\n    if ibd_density > 0.5 and avg_ibd_length > 1000:\n        # Dense IBD with long segments - likely close relatives\n        params['max_up'] = 2\n    elif ibd_density < 0.1 or avg_ibd_length < 100:\n        # Sparse IBD with short segments - likely distant relatives\n        params['max_up'] = 4\n    \n    # Adjust n_keep based on available memory\n    memory_gb = available_memory / (1024 * 1024 * 1024)\n    if memory_gb < 4:  # Less than 4GB\n        params['n_keep'] = 3\n    elif memory_gb > 16:  # More than 16GB\n        params['n_keep'] = 10\n    \n    # Adjust batch_size based on available cores\n    params['batch_size'] = min(max(available_cores, 2), 32)\n    \n    # Adjust ibd_threshold based on dataset size\n    if num_individuals > 100:\n        params['ibd_threshold'] = 30\n    elif num_individuals < 20:\n        params['ibd_threshold'] = 10\n    \n    # Decide whether to use threading or processes\n    if memory_gb > 8:  # More than 8GB\n        params['use_threading'] = False  # Use processes for better parallelism\n    \n    return params"
   ],
   "metadata": {},
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Demonstrate adaptive parameter selection\ndef generate_random_ibd_data(num_individuals, ibd_density, avg_segment_length):\n    \"\"\"\n    Generate random IBD data with given characteristics.\n    \n    Args:\n        num_individuals: Number of individuals\n        ibd_density: Density of IBD connections (0-1)\n        avg_segment_length: Average segment length in cM\n        \n    Returns:\n        id_to_shared_ibd: Dict mapping ID pairs to their IBD segments\n    \"\"\"\n    id_to_shared_ibd = {}\n    \n    # Generate IDs\n    ids = list(range(1, num_individuals + 1))\n    \n    # Calculate total possible pairs\n    total_pairs = num_individuals * (num_individuals - 1) // 2\n    \n    # Determine number of pairs that share IBD\n    num_ibd_pairs = int(total_pairs * ibd_density)\n    \n    # Randomly select pairs to share IBD\n    all_pairs = [(i, j) for i in ids for j in ids if i < j]\n    ibd_pairs = random.sample(all_pairs, min(num_ibd_pairs, len(all_pairs)))\n    \n    # Generate random segments for each pair\n    for id1, id2 in ibd_pairs:\n        # Number of segments (random between 1-5)\n        num_segments = random.randint(1, 5)\n        \n        # Generate segments\n        segments = []\n        for _ in range(num_segments):\n            # Random segment length with variation around avg_segment_length\n            length = max(1, random.gauss(avg_segment_length, avg_segment_length / 4))\n            \n            segment = {\n                'chrom': random.randint(1, 22),\n                'start_cm': random.uniform(0, 200),\n                'end_cm': 0,  # Will be set based on length\n                'length_cm': length\n            }\n            segment['end_cm'] = segment['start_cm'] + length\n            \n            segments.append(segment)\n        \n        # Store segments\n        id_to_shared_ibd[(id1, id2)] = segments\n    \n    return id_to_shared_ibd\n\n# Generate different datasets with varying characteristics\nsmall_close_dataset = generate_random_ibd_data(\n    num_individuals=15,\n    ibd_density=0.7,\n    avg_segment_length=1500\n)\n\nmedium_mixed_dataset = generate_random_ibd_data(\n    num_individuals=50,\n    ibd_density=0.3,\n    avg_segment_length=500\n)\n\nlarge_distant_dataset = generate_random_ibd_data(\n    num_individuals=200,\n    ibd_density=0.05,\n    avg_segment_length=100\n)\n\n# Generate random demographic info for each dataset\ndef generate_random_demographic_info(ids):\n    \"\"\"Generate random demographic info for a list of IDs\"\"\"\n    id_to_info = {}\n    for id_val in ids:\n        id_to_info[id_val] = {\n            'id': id_val,\n            'age': random.randint(10, 80),\n            'sex': random.choice(['M', 'F'])\n        }\n    return id_to_info\n\nsmall_demo = generate_random_demographic_info(range(1, 16))\nmedium_demo = generate_random_demographic_info(range(1, 51))\nlarge_demo = generate_random_demographic_info(range(1, 201))\n\n# Optimize parameters for each dataset\nsmall_params = optimize_parameters(small_close_dataset, small_demo)\nmedium_params = optimize_parameters(medium_mixed_dataset, medium_demo)\nlarge_params = optimize_parameters(large_distant_dataset, large_demo)\n\n# Print optimized parameters\nprint(\"Parameters for small dataset with close relatives:\")\nfor param, value in small_params.items():\n    print(f\"  {param}: {value}\")\n\nprint(\"\\\nParameters for medium dataset with mixed relationships:\")\nfor param, value in medium_params.items():\n    print(f\"  {param}: {value}\")\n\nprint(\"\\\nParameters for large dataset with distant relatives:\")\nfor param, value in large_params.items():\n    print(f\"  {param}: {value}\")\n\n# Visualize parameter adaptations\nparams_to_plot = ['max_up', 'n_keep', 'ibd_threshold', 'batch_size']\ndataset_labels = ['Small, Close', 'Medium, Mixed', 'Large, Distant']\nparam_values = [\n    [small_params[p] for p in params_to_plot],\n    [medium_params[p] for p in params_to_plot],\n    [large_params[p] for p in params_to_plot]\n]\n\n# Create a grouped bar chart\nfig, ax = plt.subplots(figsize=(12, 6))\nx = np.arange(len(params_to_plot))\nwidth = 0.25\n\nfor i, (values, label) in enumerate(zip(param_values, dataset_labels)):\n    ax.bar(x + i * width, values, width, label=label)\n\nax.set_title('Adaptive Parameter Selection')\nax.set_ylabel('Parameter Value')\nax.set_xticks(x + width)\nax.set_xticklabels(params_to_plot)\nax.legend()\nplt.grid(axis='y', linestyle='--', alpha=0.7)\nplt.tight_layout()\nplt.show()"
   ],
   "metadata": {},
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Part 4: Specialized Data Structures\n\nBonsai v3 uses specialized data structures optimized for the specific requirements of pedigree reconstruction. These data structures are designed to minimize memory usage and computational overhead while still providing efficient access to the information needed for reconstruction.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "class CompactIBDStore:\n    \"\"\"\n    Memory-efficient storage for IBD segment data.\n    \n    This class provides a compact representation of IBD segments,\n    optimized for the specific access patterns used in Bonsai v3.\n    \"\"\"\n    \n    def __init__(self, id_to_shared_ibd):\n        \"\"\"\n        Initialize the compact IBD store from a standard IBD representation.\n        \n        Args:\n            id_to_shared_ibd: Dict mapping ID pairs to their IBD segments\n        \"\"\"\n        # Convert to more efficient representation\n        self.pairs = []\n        self.segments = []\n        self.pair_to_segments = {}\n        self.pair_to_idx = {}\n        \n        for (id1, id2), segs in id_to_shared_ibd.items():\n            pair_idx = len(self.pairs)\n            self.pairs.append((id1, id2))\n            self.pair_to_idx[(id1, id2)] = pair_idx\n            \n            # Store segment indices\n            seg_indices = []\n            for seg in segs:\n                seg_idx = len(self.segments)\n                # Store only essential fields\n                compact_seg = {\n                    'chrom': seg.get('chrom', 0),\n                    'start_cm': seg.get('start_cm', 0),\n                    'end_cm': seg.get('end_cm', 0),\n                    'length_cm': seg.get('length_cm', 0)\n                }\n                self.segments.append(compact_seg)\n                seg_indices.append(seg_idx)\n            \n            self.pair_to_segments[pair_idx] = seg_indices\n    \n    def get_shared_segments(self, id1, id2):\n        \"\"\"\n        Get the IBD segments shared by two individuals.\n        \n        Args:\n            id1, id2: IDs of the individuals\n            \n        Returns:\n            List of shared IBD segments\n        \"\"\"\n        pair = (min(id1, id2), max(id1, id2))\n        pair_idx = self.pair_to_idx.get(pair)\n        \n        if pair_idx is None:\n            return []\n            \n        seg_indices = self.pair_to_segments.get(pair_idx, [])\n        return [self.segments[idx] for idx in seg_indices]\n    \n    def get_total_ibd(self, id1, id2):\n        \"\"\"\n        Get the total IBD shared by two individuals.\n        \n        Args:\n            id1, id2: IDs of the individuals\n            \n        Returns:\n            Total shared IBD in centimorgans\n        \"\"\"\n        segments = self.get_shared_segments(id1, id2)\n        return sum(seg['length_cm'] for seg in segments)\n    \n    def memory_usage(self):\n        \"\"\"\n        Estimate memory usage of this data structure.\n        \n        Returns:\n            Estimated memory usage in bytes\n        \"\"\"\n        # Rough estimate based on number of objects\n        pairs_size = len(self.pairs) * 16  # Two integers per pair\n        segments_size = len(self.segments) * 32  # Four floats per segment\n        index_size = len(self.pair_to_segments) * 24  # Dict entry overhead\n        \n        return pairs_size + segments_size + index_size\n\nclass SparseRelationshipMatrix:\n    \"\"\"\n    Efficient representation of pairwise relationships.\n    \n    This class provides a memory-efficient representation of pairwise\n    relationships in a pedigree, using sparse matrix techniques.\n    \"\"\"\n    \n    def __init__(self):\n        \"\"\"\n        Initialize an empty sparse relationship matrix.\n        \"\"\"\n        self.relationships = {}  # (id1, id2) -> relationship tuple\n    \n    def set_relationship(self, id1, id2, relationship):\n        \"\"\"\n        Set the relationship between two individuals.\n        \n        Args:\n            id1, id2: IDs of the individuals\n            relationship: Tuple of (up, down, num_ancs)\n        \"\"\"\n        # Ensure id1 < id2 for consistent keys\n        if id1 > id2:\n            id1, id2 = id2, id1\n            # Swap up and down for symmetric relationship representation\n            up, down, num_ancs = relationship\n            relationship = (down, up, num_ancs)\n        \n        self.relationships[(id1, id2)] = relationship\n    \n    def get_relationship(self, id1, id2):\n        \"\"\"\n        Get the relationship between two individuals.\n        \n        Args:\n            id1, id2: IDs of the individuals\n            \n        Returns:\n            Relationship tuple (up, down, num_ancs) or None if not set\n        \"\"\"\n        # Ensure id1 < id2 for consistent keys\n        if id1 > id2:\n            id1, id2 = id2, id1\n            swap = True\n        else:\n            swap = False\n        \n        relationship = self.relationships.get((id1, id2))\n        \n        if relationship is not None and swap:\n            # Swap up and down for symmetric relationship representation\n            up, down, num_ancs = relationship\n            return (down, up, num_ancs)\n        \n        return relationship\n    \n    def get_all_relationships_for(self, id_val):\n        \"\"\"\n        Get all relationships involving a specific individual.\n        \n        Args:\n            id_val: ID of the individual\n            \n        Returns:\n            Dict mapping other IDs to relationship tuples\n        \"\"\"\n        result = {}\n        \n        # Check for relationships where id_val is first in the pair\n        for (id1, id2), relationship in self.relationships.items():\n            if id1 == id_val:\n                result[id2] = relationship\n            elif id2 == id_val:\n                # Swap up and down for symmetric relationship representation\n                up, down, num_ancs = relationship\n                result[id1] = (down, up, num_ancs)\n        \n        return result"
   ],
   "metadata": {},
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Demonstrate specialized data structures\ndef compare_ibd_storage_performance(id_to_shared_ibd):\n    \"\"\"\n    Compare performance of different IBD storage methods.\n    \n    Args:\n        id_to_shared_ibd: Dict mapping ID pairs to their IBD segments\n        \n    Returns:\n        comparison: Dict containing performance metrics\n    \"\"\"\n    # Measure standard dict storage\n    start_time = time.time()\n    queries = []\n    \n    # Create a list of all pairs to query\n    pairs = list(id_to_shared_ibd.keys())\n    \n    # Add some pairs that don't exist\n    max_id = max([max(pair) for pair in pairs])\n    for _ in range(100):\n        id1 = random.randint(1, max_id)\n        id2 = random.randint(1, max_id)\n        if id1 != id2:\n            queries.append((min(id1, id2), max(id1, id2)))\n    \n    # Add all existing pairs\n    queries.extend(pairs)\n    random.shuffle(queries)\n    \n    # Measure standard dict performance\n    standard_start = time.time()\n    standard_results = []\n    \n    for id1, id2 in queries:\n        pair = (id1, id2)\n        if pair in id_to_shared_ibd:\n            segments = id_to_shared_ibd[pair]\n            total_cm = sum(seg.get('length_cm', 0) for seg in segments)\n            standard_results.append(total_cm)\n        else:\n            standard_results.append(0)\n    \n    standard_time = time.time() - standard_start\n    \n    # Create and measure CompactIBDStore\n    compact_store = CompactIBDStore(id_to_shared_ibd)\n    \n    compact_start = time.time()\n    compact_results = []\n    \n    for id1, id2 in queries:\n        total_cm = compact_store.get_total_ibd(id1, id2)\n        compact_results.append(total_cm)\n    \n    compact_time = time.time() - compact_start\n    \n    # Compare memory usage (rough estimate)\n    standard_size = sys.getsizeof(id_to_shared_ibd)\n    for pair, segments in id_to_shared_ibd.items():\n        standard_size += sys.getsizeof(pair)\n        standard_size += sys.getsizeof(segments)\n        for segment in segments:\n            standard_size += sys.getsizeof(segment)\n    \n    compact_size = compact_store.memory_usage()\n    \n    # Verify results are the same\n    results_match = standard_results == compact_results\n    \n    return {\n        'standard_time': standard_time,\n        'compact_time': compact_time,\n        'standard_size': standard_size,\n        'compact_size': compact_size,\n        'speedup': standard_time / compact_time if compact_time > 0 else float('inf'),\n        'memory_reduction': standard_size / compact_size if compact_size > 0 else float('inf'),\n        'results_match': results_match\n    }\n\n# Create a large test dataset\nlarge_test_dataset = generate_random_ibd_data(\n    num_individuals=1000,\n    ibd_density=0.01,\n    avg_segment_length=200\n)\n\n# Compare standard and compact storage\nprint(\"Comparing IBD storage methods...\")\ncomparison = compare_ibd_storage_performance(large_test_dataset)\n\nprint(f\"Standard Dict:\")\nprint(f\"  Lookup time: {comparison['standard_time']:.6f} seconds\")\nprint(f\"  Estimated memory: {comparison['standard_size'] / (1024*1024):.2f} MB\")\n\nprint(f\"\\\nCompactIBDStore:\")\nprint(f\"  Lookup time: {comparison['compact_time']:.6f} seconds\")\nprint(f\"  Estimated memory: {comparison['compact_size'] / (1024*1024):.2f} MB\")\n\nprint(f\"\\\nPerformance Comparison:\")\nprint(f\"  Speedup: {comparison['speedup']:.2f}x\")\nprint(f\"  Memory reduction: {comparison['memory_reduction']:.2f}x\")\nprint(f\"  Results match: {comparison['results_match']}\")\n\n# Demonstrate SparseRelationshipMatrix\ndef test_sparse_relationship_matrix():\n    \"\"\"Test the SparseRelationshipMatrix class\"\"\"\n    matrix = SparseRelationshipMatrix()\n    \n    # Add some relationships\n    matrix.set_relationship(1, 2, (0, 1, 1))  # 1 is parent of 2\n    matrix.set_relationship(3, 4, (1, 1, 2))  # 3 and 4 are full siblings\n    matrix.set_relationship(5, 6, (1, 1, 1))  # 5 and 6 are half siblings\n    \n    # Test retrieval\n    print(\"Relationship between 1 and 2:\", matrix.get_relationship(1, 2))\n    print(\"Relationship between 2 and 1:\", matrix.get_relationship(2, 1))\n    print(\"Relationship between 3 and 4:\", matrix.get_relationship(3, 4))\n    print(\"Relationship between 5 and 6:\", matrix.get_relationship(5, 6))\n    \n    # Test getting all relationships for an individual\n    print(\"\\\nAll relationships for individual 1:\", matrix.get_all_relationships_for(1))\n    print(\"All relationships for individual 2:\", matrix.get_all_relationships_for(2))\n\nprint(\"\\\nTesting SparseRelationshipMatrix:\")\ntest_sparse_relationship_matrix()"
   ],
   "metadata": {},
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Part 5: Early Termination and Lazy Evaluation\n\nBonsai v3 uses early termination and lazy evaluation strategies to avoid unnecessary computation. These techniques allow Bonsai to quickly eliminate unpromising pedigree configurations without fully evaluating them.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "def evaluate_pedigree_with_early_termination(up_dct, id_to_shared_ibd, early_term_threshold=-1000.0):\n    \"\"\"\n    Evaluate the likelihood of a pedigree with early termination.\n    \n    Args:\n        up_dct: Up-node dictionary representing the pedigree\n        id_to_shared_ibd: Dict mapping ID pairs to their IBD segments\n        early_term_threshold: Threshold for early termination\n        \n    Returns:\n        log_likelihood: Log-likelihood of the pedigree, or None if terminated early\n    \"\"\"\n    # Initialize log-likelihood\n    log_like = 0.0\n    \n    # Get all pairs of individuals in the pedigree\n    all_ids = list(up_dct.keys())\n    all_pairs = [(all_ids[i], all_ids[j]) for i in range(len(all_ids)) for j in range(i+1, len(all_ids))]\n    \n    # First evaluate high-IBD pairs (more informative)\n    high_ibd_pairs = []\n    low_ibd_pairs = []\n    \n    for id1, id2 in all_pairs:\n        pair = (min(id1, id2), max(id1, id2))\n        if pair in id_to_shared_ibd:\n            total_cm = sum(seg.get('length_cm', 0) for seg in id_to_shared_ibd[pair])\n            if total_cm > 100:  # High IBD threshold\n                high_ibd_pairs.append((id1, id2, total_cm))\n            else:\n                low_ibd_pairs.append((id1, id2, total_cm))\n        else:\n            low_ibd_pairs.append((id1, id2, 0))\n    \n    # Sort high-IBD pairs by IBD amount (descending)\n    high_ibd_pairs.sort(key=lambda x: x[2], reverse=True)\n    \n    # Evaluate high-IBD pairs first\n    for id1, id2, total_cm in high_ibd_pairs:\n        # Get relationship in the pedigree (simplified for demo)\n        relationship = \"direct\"  # This would normally be determined from up_dct\n        \n        # Calculate likelihood for this pair based on relationship and IBD\n        if relationship == \"direct\":\n            # Direct relatives should have high IBD\n            if total_cm < 700:\n                pair_ll = -500  # Severe penalty for direct relatives with low IBD\n            else:\n                pair_ll = math.log(1 + total_cm)\n        else:\n            # Non-direct relatives should have lower IBD\n            if total_cm > 1500:\n                pair_ll = -300  # Penalty for non-direct relatives with high IBD\n            else:\n                pair_ll = math.log(1 + total_cm / 10)\n        \n        # Update total likelihood\n        log_like += pair_ll\n        \n        # Check for early termination\n        if log_like < early_term_threshold:\n            print(f\"Early termination after {len(high_ibd_pairs)} high-IBD pairs. Score: {log_like:.2f}\")\n            return None\n    \n    # Evaluate remaining pairs\n    for id1, id2, total_cm in low_ibd_pairs:\n        # Similar calculation as above, but for low-IBD pairs\n        relationship = \"distant\"  # Simplified\n        \n        if relationship == \"direct\":\n            pair_ll = -200  # Severe penalty for direct relatives with low IBD\n        else:\n            pair_ll = math.log(1 + total_cm / 50)\n        \n        # Update total likelihood\n        log_like += pair_ll\n        \n        # Check for early termination\n        if log_like < early_term_threshold:\n            return None\n    \n    return log_like"
   ],
   "metadata": {},
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Demonstrate early termination\ndef generate_pedigrees_with_varying_quality(id_to_shared_ibd, num_pedigrees=5):\n    \"\"\"\n    Generate pedigrees with varying quality for demonstration.\n    \n    Args:\n        id_to_shared_ibd: Dict mapping ID pairs to their IBD segments\n        num_pedigrees: Number of pedigrees to generate\n        \n    Returns:\n        pedigrees: List of pedigrees with varying quality\n    \"\"\"\n    pedigrees = []\n    \n    # Get all individuals\n    all_ids = set()\n    for id1, id2 in id_to_shared_ibd.keys():\n        all_ids.add(id1)\n        all_ids.add(id2)\n    \n    # Create a good pedigree - preserve high-IBD relationships\n    good_ped = {id_val: {} for id_val in all_ids}\n    \n    # Find pairs with high IBD\n    high_ibd_pairs = []\n    for pair, segments in id_to_shared_ibd.items():\n        total_cm = sum(seg.get('length_cm', 0) for seg in segments)\n        if total_cm > 1000:  # Parent-child or sibling level\n            high_ibd_pairs.append((pair[0], pair[1], total_cm))\n    \n    # Sort by IBD amount (descending)\n    high_ibd_pairs.sort(key=lambda x: x[2], reverse=True)\n    \n    # Add parent-child relationships for highest IBD pairs\n    for id1, id2, _ in high_ibd_pairs[:len(high_ibd_pairs)//2]:\n        # Make id1 parent of id2\n        good_ped[id2][id1] = 1\n    \n    pedigrees.append((\"Good\", good_ped))\n    \n    # Create a mediocre pedigree - some relationships preserved, some not\n    mediocre_ped = {id_val: {} for id_val in all_ids}\n    \n    # Add parent-child relationships for some high IBD pairs\n    for id1, id2, _ in high_ibd_pairs[:len(high_ibd_pairs)//4]:\n        # Make id1 parent of id2\n        mediocre_ped[id2][id1] = 1\n    \n    # Add some random relationships\n    for _ in range(5):\n        id1, id2 = random.sample(list(all_ids), 2)\n        if id1 != id2:\n            mediocre_ped[id2][id1] = 1\n    \n    pedigrees.append((\"Mediocre\", mediocre_ped))\n    \n    # Create increasingly poor pedigrees with random relationships\n    for i in range(num_pedigrees - 2):\n        poor_ped = {id_val: {} for id_val in all_ids}\n        \n        # Add random relationships\n        for _ in range(10 + i * 5):\n            id1, id2 = random.sample(list(all_ids), 2)\n            if id1 != id2:\n                poor_ped[id2][id1] = 1\n        \n        pedigrees.append((f\"Poor {i+1}\", poor_ped))\n    \n    return pedigrees\n\n# Demonstrate early termination with different pedigrees\ndef compare_evaluation_methods(pedigrees, id_to_shared_ibd):\n    \"\"\"\n    Compare standard evaluation with early termination.\n    \n    Args:\n        pedigrees: List of (name, pedigree) tuples\n        id_to_shared_ibd: Dict mapping ID pairs to their IBD segments\n    \"\"\"\n    results = []\n    \n    for name, pedigree in pedigrees:\n        # Evaluate with standard method (no early termination)\n        start_time = time.time()\n        standard_ll = evaluate_pedigree_with_early_termination(pedigree, id_to_shared_ibd, float('-inf'))\n        standard_time = time.time() - start_time\n        \n        # Evaluate with early termination\n        start_time = time.time()\n        early_term_ll = evaluate_pedigree_with_early_termination(pedigree, id_to_shared_ibd, -500)\n        early_term_time = time.time() - start_time\n        \n        terminated_early = early_term_ll is None\n        \n        results.append({\n            'name': name,\n            'standard_ll': standard_ll,\n            'standard_time': standard_time,\n            'early_term_ll': early_term_ll,\n            'early_term_time': early_term_time,\n            'terminated_early': terminated_early,\n            'speedup': standard_time / early_term_time if early_term_time > 0 else float('inf')\n        })\n    \n    # Print results\n    print(f\"{'Pedigree':<10} | {'Standard LL':<12} | {'Standard Time':<14} | {'Early Term LL':<14} | {'Early Term Time':<15} | {'Terminated':<10} | {'Speedup':<8}\")\n    print(\"-\" * 90)\n    \n    for result in results:\n        standard_ll = f\"{result['standard_ll']:.2f}\" if result['standard_ll'] is not None else \"None\"\n        early_term_ll = f\"{result['early_term_ll']:.2f}\" if result['early_term_ll'] is not None else \"None\"\n        \n        print(f\"{result['name']:<10} | {standard_ll:<12} | {result['standard_time']:.6f} s | \"\n              f\"{early_term_ll:<14} | {result['early_term_time']:.6f} s | \"\n              f\"{result['terminated_early']!s:<10} | {result['speedup']:.2f}x\")\n    \n    # Plot results\n    plt.figure(figsize=(12, 6))\n    plt.title(\"Standard vs. Early Termination Evaluation Time\")\n    \n    names = [r['name'] for r in results]\n    standard_times = [r['standard_time'] * 1000 for r in results]  # Convert to ms\n    early_term_times = [r['early_term_time'] * 1000 for r in results]  # Convert to ms\n    \n    x = np.arange(len(names))\n    width = 0.35\n    \n    plt.bar(x - width/2, standard_times, width, label='Standard')\n    plt.bar(x + width/2, early_term_times, width, label='Early Termination')\n    \n    plt.xlabel('Pedigree')\n    plt.ylabel('Evaluation Time (ms)')\n    plt.xticks(x, names)\n    plt.legend()\n    \n    plt.grid(axis='y', linestyle='--', alpha=0.7)\n    plt.tight_layout()\n    plt.show()\n\n# Generate test data\nrandom.seed(42)\ntest_ibd_data = generate_random_ibd_data(\n    num_individuals=20,\n    ibd_density=0.3,\n    avg_segment_length=500\n)\n\n# Generate pedigrees with varying quality\npedigrees = generate_pedigrees_with_varying_quality(test_ibd_data)\n\n# Compare evaluation methods\ncompare_evaluation_methods(pedigrees, test_ibd_data)"
   ],
   "metadata": {},
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Summary\n\nIn this lab, we explored the sophisticated optimization techniques used in Bonsai v3 to enable efficient pedigree reconstruction at scale. These optimizations are crucial for handling real-world genetic genealogy datasets with hundreds or thousands of individuals.\n\nKey optimization strategies we explored include:\n\n1. **Search Space Pruning**: By clustering individuals based on IBD connectivity and applying demographic constraints, Bonsai v3 can dramatically reduce the number of pedigree configurations to evaluate. We saw how this pruning can reduce the search space by many orders of magnitude.\n\n2. **Parallel Processing**: By leveraging multiple CPU cores, Bonsai v3 can significantly speed up computation for tasks that can be naturally parallelized, such as evaluating multiple pedigree configurations or processing different chromosomes independently.\n\n3. **Adaptive Parameter Selection**: Instead of using fixed parameters for all datasets, Bonsai v3 dynamically adjusts its parameters based on dataset characteristics like size, IBD density, and average segment length. This adaptive approach ensures optimal performance across a wide range of datasets.\n\n4. **Specialized Data Structures**: Memory-efficient data structures like CompactIBDStore and SparseRelationshipMatrix allow Bonsai v3 to handle larger datasets with less memory overhead and faster access times.\n\n5. **Early Termination and Lazy Evaluation**: By prioritizing evaluation of high-information pairs and terminating evaluation early for unpromising configurations, Bonsai v3 can avoid unnecessary computation and focus resources on the most promising pedigree candidates.\n\nThese optimization techniques are not just implementation details but fundamental enabling technologies that make it possible to apply pedigree reconstruction to real-world genetic genealogy datasets with hundreds or thousands of individuals.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "# Convert this notebook to PDF using poetry\n!poetry run jupyter nbconvert --to pdf Lab18_Optimization_Techniques.ipynb\n\n# Note: PDF conversion requires LaTeX to be installed on your system\n# If you encounter errors, you may need to install it:\n# On Ubuntu/Debian: sudo apt-get install texlive-xetex\n# On macOS with Homebrew: brew install texlive"
   ],
   "metadata": {},
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}