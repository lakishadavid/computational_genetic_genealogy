{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 18: Data Quality and Preprocessing in Bonsai\n",
    "\n",
    "In this lab, we'll explore how data quality and preprocessing impact pedigree reconstruction with Bonsai. Building upon our understanding of Bonsai's architecture and likelihood calculations from previous labs, we'll focus on techniques to identify and address data quality issues, implement effective filtering strategies, and build robust preprocessing pipelines.\n",
    "\n",
    "## Why This Matters\n",
    "\n",
    "The quality of input data significantly affects the accuracy of pedigree reconstruction. Even with excellent algorithms, poor quality data leads to unreliable results. Understanding how to detect and mitigate data quality issues is essential for:\n",
    "- Improving the accuracy of relationship inference\n",
    "- Reducing false positives and false negatives in IBD detection\n",
    "- Handling missing or incomplete data effectively\n",
    "- Adapting to population-specific challenges\n",
    "- Building confidence in the reconstructed pedigrees\n",
    "\n",
    "**Learning Objectives**:\n",
    "- Identify common data quality issues affecting pedigree reconstruction\n",
    "- Implement advanced filtering techniques for IBD segment data\n",
    "- Develop strategies for handling missing data and incomplete pedigrees\n",
    "- Build effective data preprocessing pipelines for Bonsai\n",
    "- Detect and mitigate errors in IBD detection\n",
    "- Apply quality control metrics to evaluate input data reliability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!poetry install --no-root"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import logging\n",
    "import sys\n",
    "import re\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "import subprocess\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from IPython.display import display, HTML\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "from scipy import stats\n",
    "from collections import defaultdict, Counter\n",
    "import random\n",
    "import time\n",
    "import json\n",
    "from tqdm.notebook import tqdm\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Environment setup for cross-compatibility\n",
    "from scripts_support.lab_cross_compatibility import setup_environment, is_jupyterlite\n\n",
    "# Set up environment-specific paths\n",
    "DATA_DIR, RESULTS_DIR = setup_environment()\n\n",
    "# Now you can use DATA_DIR and RESULTS_DIR consistently across environments\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Common Data Quality Issues in Genetic Genealogy\n",
    "\n",
    "Before diving into specific preprocessing techniques, let's explore common data quality issues that affect pedigree reconstruction. Understanding these issues is the first step toward developing effective solutions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 IBD Detection Errors\n",
    "\n",
    "IBD detection algorithms are not perfect and can produce both false positives and false negatives:\n",
    "\n",
    "- **False positives**: Segments incorrectly identified as IBD\n",
    "- **False negatives**: True IBD segments that are missed\n",
    "- **Boundary errors**: Imprecise determination of segment start/end positions\n",
    "\n",
    "Let's load some IBD segments and examine potential quality issues:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load IBD segments from a sample file\n",
    "seg_file = os.path.join(data_directory, \"class_data/ped_sim_run2.seg\")\n",
    "seg_df = pd.read_csv(seg_file, sep=\"\\t\", header=None)\n",
    "seg_df.columns = [\"sample1\", \"sample2\", \"chrom\", \"phys_start\", \"phys_end\", \"ibd_type\", \"gen_start\", \"gen_end\", \"gen_seg_len\"]\n",
    "\n",
    "# Display basic information about the segments\n",
    "print(f\"Loaded {len(seg_df)} IBD segments\")\n",
    "print(f\"Number of unique pairs: {seg_df[['sample1', 'sample2']].drop_duplicates().shape[0]}\")\n",
    "print(f\"Number of unique individuals: {len(set(seg_df['sample1']).union(set(seg_df['sample2'])))}\")\n",
    "\n",
    "# Check for potential issues\n",
    "print(\"\\nPotential data quality issues:\")\n",
    "\n",
    "# Check for very short segments (potential false positives)\n",
    "short_segments = seg_df[seg_df['gen_seg_len'] < 4]\n",
    "print(f\"Very short segments (< 4 cM): {len(short_segments)} ({len(short_segments)/len(seg_df)*100:.2f}%)\")\n",
    "\n",
    "# Check for suspiciously long segments (potential errors)\n",
    "long_segments = seg_df[seg_df['gen_seg_len'] > 200]\n",
    "print(f\"Very long segments (> 200 cM): {len(long_segments)} ({len(long_segments)/len(seg_df)*100:.2f}%)\")\n",
    "\n",
    "# Check for segments with unusual IBD type\n",
    "unusual_ibd_type = seg_df[~seg_df['ibd_type'].isin(['IBD1', 'IBD2'])]\n",
    "print(f\"Segments with unusual IBD type: {len(unusual_ibd_type)}\")\n",
    "\n",
    "# Display a sample of potentially problematic segments\n",
    "print(\"\\nSample of short segments (potential false positives):\")\n",
    "display(short_segments.head())\n",
    "\n",
    "print(\"\\nSample of long segments (potential errors):\")\n",
    "display(long_segments.head())\n",
    "\n",
    "# Visualize segment length distribution to identify potential issues\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.hist(seg_df['gen_seg_len'], bins=50, alpha=0.7)\n",
    "plt.axvline(x=4, color='red', linestyle='--', label='4 cM threshold')\n",
    "plt.axvline(x=7, color='green', linestyle='--', label='7 cM threshold')\n",
    "plt.axvline(x=20, color='purple', linestyle='--', label='20 cM threshold')\n",
    "plt.xlabel('Segment Length (cM)')\n",
    "plt.ylabel('Count')\n",
    "plt.title('IBD Segment Length Distribution')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.xlim(0, 100)  # Focus on the relevant range\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Phasing and Genotyping Errors\n",
    "\n",
    "Errors in the underlying genetic data can propagate to IBD detection and pedigree reconstruction:\n",
    "\n",
    "- **Phasing errors**: Incorrect assignment of alleles to parental chromosomes\n",
    "- **Genotyping errors**: Incorrect genotype calls in the raw data\n",
    "- **Missing genotypes**: Gaps in genetic data\n",
    "\n",
    "Let's examine how these errors might manifest in segment data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to simulate the effect of phasing and genotyping errors on IBD detection\n",
    "def simulate_errors(base_segments, error_rate=0.05, seg_length_effect=0.2, num_segments_effect=0.1):\n",
    "    \"\"\"Simulate the effect of phasing and genotyping errors on IBD segments.\n",
    "    \n",
    "    Args:\n",
    "        base_segments: DataFrame of IBD segments without errors\n",
    "        error_rate: Rate of segments affected by errors\n",
    "        seg_length_effect: Factor by which segment lengths are affected\n",
    "        num_segments_effect: Factor affecting the number of segments\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame with simulated errors\n",
    "    \"\"\"\n",
    "    # Create a copy of the base segments\n",
    "    error_segments = base_segments.copy()\n",
    "    \n",
    "    # Randomly select segments to affect with errors\n",
    "    num_affected = int(len(error_segments) * error_rate)\n",
    "    affected_indices = np.random.choice(len(error_segments), size=num_affected, replace=False)\n",
    "    \n",
    "    # Apply effects to selected segments\n",
    "    for idx in affected_indices:\n",
    "        # Randomly shorten segments (phasing errors often break segments)\n",
    "        if np.random.random() < 0.7:  # 70% chance of segment shortening\n",
    "            orig_length = error_segments.loc[idx, 'gen_seg_len']\n",
    "            new_length = orig_length * (1 - seg_length_effect * np.random.random())\n",
    "            error_segments.loc[idx, 'gen_seg_len'] = new_length\n",
    "            \n",
    "            # Adjust genetic positions accordingly\n",
    "            orig_range = error_segments.loc[idx, 'gen_end'] - error_segments.loc[idx, 'gen_start']\n",
    "            new_range = new_length\n",
    "            scale_factor = new_range / orig_range if orig_range > 0 else 1\n",
    "            \n",
    "            error_segments.loc[idx, 'gen_end'] = error_segments.loc[idx, 'gen_start'] + new_range\n",
    "            \n",
    "            # Adjust physical positions proportionally\n",
    "            phys_range = error_segments.loc[idx, 'phys_end'] - error_segments.loc[idx, 'phys_start']\n",
    "            error_segments.loc[idx, 'phys_end'] = error_segments.loc[idx, 'phys_start'] + int(phys_range * scale_factor)\n",
    "    \n",
    "    # Drop some segments entirely (false negatives)\n",
    "    drop_indices = np.random.choice(len(error_segments), size=int(len(error_segments) * num_segments_effect), replace=False)\n",
    "    error_segments = error_segments.drop(drop_indices)\n",
    "    \n",
    "    # Add some false positive segments (typically shorter)\n",
    "    num_false_pos = int(len(base_segments) * num_segments_effect * 0.8)  # 80% of dropped segments\n",
    "    \n",
    "    false_pos_segments = []\n",
    "    for _ in range(num_false_pos):\n",
    "        # Random pair of individuals\n",
    "        all_individuals = list(set(base_segments['sample1']).union(set(base_segments['sample2'])))\n",
    "        sample1, sample2 = np.random.choice(all_individuals, size=2, replace=False)\n",
    "        \n",
    "        # Random chromosome\n",
    "        chrom = np.random.randint(1, 23)\n",
    "        \n",
    "        # Generate a short segment (false positives tend to be short)\n",
    "        gen_start = np.random.uniform(0, 250)\n",
    "        gen_seg_len = np.random.uniform(1, 5)  # Short segments\n",
    "        gen_end = gen_start + gen_seg_len\n",
    "        \n",
    "        # Generate physical positions (simplified)\n",
    "        phys_start = np.random.randint(1000000, 200000000)\n",
    "        phys_end = phys_start + np.random.randint(1000000, 10000000)\n",
    "        \n",
    "        # Always IBD1 for simplicity\n",
    "        ibd_type = 'IBD1'\n",
    "        \n",
    "        false_pos_segments.append({\n",
    "            'sample1': sample1,\n",
    "            'sample2': sample2,\n",
    "            'chrom': chrom,\n",
    "            'phys_start': phys_start,\n",
    "            'phys_end': phys_end,\n",
    "            'ibd_type': ibd_type,\n",
    "            'gen_start': gen_start,\n",
    "            'gen_end': gen_end,\n",
    "            'gen_seg_len': gen_seg_len\n",
    "        })\n",
    "    \n",
    "    # Combine original segments (with errors) and false positives\n",
    "    error_segments = pd.concat([error_segments, pd.DataFrame(false_pos_segments)], ignore_index=True)\n",
    "    \n",
    "    return error_segments\n",
    "\n",
    "# Simulate some errors to demonstrate their effects\n",
    "# Select a subset of the data for demonstration\n",
    "subset_df = seg_df.sample(n=min(1000, len(seg_df)), random_state=42)\n",
    "\n",
    "# Simulate errors with different rates\n",
    "low_error_df = simulate_errors(subset_df, error_rate=0.05)\n",
    "high_error_df = simulate_errors(subset_df, error_rate=0.2)\n",
    "\n",
    "# Compare the distributions\n",
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.hist(subset_df['gen_seg_len'], bins=30, alpha=0.7)\n",
    "plt.xlabel('Segment Length (cM)')\n",
    "plt.ylabel('Count')\n",
    "plt.title('Original Segments')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.xlim(0, 50)\n",
    "\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.hist(low_error_df['gen_seg_len'], bins=30, alpha=0.7)\n",
    "plt.xlabel('Segment Length (cM)')\n",
    "plt.ylabel('Count')\n",
    "plt.title('Low Error Rate (5%)')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.xlim(0, 50)\n",
    "\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.hist(high_error_df['gen_seg_len'], bins=30, alpha=0.7)\n",
    "plt.xlabel('Segment Length (cM)')\n",
    "plt.ylabel('Count')\n",
    "plt.title('High Error Rate (20%)')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.xlim(0, 50)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Compare summary statistics\n",
    "def print_summary_stats(df, name):\n",
    "    print(f\"Summary for {name}:\")\n",
    "    print(f\"  Number of segments: {len(df)}\")\n",
    "    print(f\"  Mean segment length: {df['gen_seg_len'].mean():.2f} cM\")\n",
    "    print(f\"  Median segment length: {df['gen_seg_len'].median():.2f} cM\")\n",
    "    print(f\"  Segments < 5 cM: {(df['gen_seg_len'] < 5).sum()} ({(df['gen_seg_len'] < 5).sum() / len(df) * 100:.1f}%)\")\n",
    "    print(f\"  Segments > 50 cM: {(df['gen_seg_len'] > 50).sum()} ({(df['gen_seg_len'] > 50).sum() / len(df) * 100:.1f}%)\")\n",
    "\n",
    "print_summary_stats(subset_df, \"Original Data\")\n",
    "print_summary_stats(low_error_df, \"Low Error Data\")\n",
    "print_summary_stats(high_error_df, \"High Error Data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Population-Specific Issues\n",
    "\n",
    "Different populations present unique challenges for IBD detection and pedigree reconstruction:\n",
    "\n",
    "- **Endogamy**: Increased background IBD due to historical intermarriage\n",
    "- **Population bottlenecks**: Reduced genetic diversity leading to more identical segments\n",
    "- **Recent admixture**: Complex patterns of ancestry that can complicate IBD detection\n",
    "\n",
    "Let's examine how these issues might manifest in the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to simulate different population scenarios\n",
    "def simulate_population_effects(base_segments, population_type, num_individuals=50):\n",
    "    \"\"\"Simulate the effect of different population structures on IBD sharing.\n",
    "    \n",
    "    Args:\n",
    "        base_segments: DataFrame to use as template for segment structure\n",
    "        population_type: One of 'outbred', 'endogamous', 'bottleneck', or 'admixed'\n",
    "        num_individuals: Number of individuals to simulate\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame with simulated segments\n",
    "    \"\"\"\n",
    "    # Create random individual IDs\n",
    "    individuals = [f\"ind_{i}\" for i in range(num_individuals)]\n",
    "    \n",
    "    # Generate all possible pairs\n",
    "    pairs = [(i, j) for i in range(num_individuals) for j in range(i+1, num_individuals)]\n",
    "    \n",
    "    # Initialize parameters based on population type\n",
    "    if population_type == 'outbred':\n",
    "        # Standard outbred population\n",
    "        bg_ibd_rate = 0.01  # Background IBD rate (proportion of genome shared on average between unrelated individuals)\n",
    "        related_pair_rate = 0.05  # Proportion of pairs that are related\n",
    "        related_icor_factor = 0.5  # \"Inbreeding coefficient of relationship\" for related pairs (r=0.5 for parent-child and siblings)\n",
    "        segment_count_factor = 1.0\n",
    "        \n",
    "    elif population_type == 'endogamous':\n",
    "        # Endogamous population with elevated background IBD\n",
    "        bg_ibd_rate = 0.05  # Higher background IBD\n",
    "        related_pair_rate = 0.1  # More related pairs\n",
    "        related_icor_factor = 0.6  # Higher sharing due to multiple relationships\n",
    "        segment_count_factor = 1.5  # More segments due to endogamy\n",
    "        \n",
    "    elif population_type == 'bottleneck':\n",
    "        # Population that went through a bottleneck\n",
    "        bg_ibd_rate = 0.03  # Elevated background IBD\n",
    "        related_pair_rate = 0.08  # More related pairs\n",
    "        related_icor_factor = 0.55  # Slightly higher sharing\n",
    "        segment_count_factor = 1.2  # More segments\n",
    "        \n",
    "    elif population_type == 'admixed':\n",
    "        # Recently admixed population\n",
    "        bg_ibd_rate = 0.02  # Moderate background IBD\n",
    "        related_pair_rate = 0.05  # Standard related pair rate\n",
    "        related_icor_factor = 0.5  # Standard relatedness\n",
    "        segment_count_factor = 0.8  # Fewer segments due to more heterozygosity\n",
    "        \n",
    "    else:\n",
    "        raise ValueError(f\"Unknown population type: {population_type}\")\n",
    "    \n",
    "    # Generate segments for each pair\n",
    "    simulated_segments = []\n",
    "    \n",
    "    for i, j in pairs:\n",
    "        # Determine if this pair is closely related\n",
    "        is_related = np.random.random() < related_pair_rate\n",
    "        \n",
    "        # Determine amount of sharing\n",
    "        if is_related:\n",
    "            # Related pairs (siblings, parent-child, etc.)\n",
    "            icor = related_icor_factor\n",
    "        else:\n",
    "            # Unrelated pairs - still have some background IBD\n",
    "            icor = bg_ibd_rate\n",
    "        \n",
    "        # Generate segments based on icor\n",
    "        # Simplified model: expected total sharing is icor * 3500 cM (total genome length)\n",
    "        expected_total_cm = icor * 3500\n",
    "        \n",
    "        # For closely related pairs, generate structured segments\n",
    "        if is_related and icor > 0.1:\n",
    "            # Parent-child or sibling-like pattern\n",
    "            if np.random.random() < 0.5:  # Parent-child\n",
    "                # Generate ~22 segments (one per chromosome) of substantial length\n",
    "                num_segments = np.random.randint(20, 25)\n",
    "                segment_lengths = np.random.normal(expected_total_cm / num_segments, \n",
    "                                                 expected_total_cm / num_segments * 0.2, \n",
    "                                                 num_segments)\n",
    "                ibd_types = ['IBD1'] * num_segments  # Parent-child is always IBD1\n",
    "            else:  # Sibling-like\n",
    "                # More segments with a mix of IBD1 and IBD2\n",
    "                num_segments = np.random.randint(30, 40)\n",
    "                segment_lengths = np.random.normal(expected_total_cm / num_segments, \n",
    "                                                 expected_total_cm / num_segments * 0.3, \n",
    "                                                 num_segments)\n",
    "                # About 25% of sibling segments are IBD2\n",
    "                ibd_types = ['IBD1'] * num_segments\n",
    "                for k in range(int(num_segments * 0.25)):\n",
    "                    ibd_types[k] = 'IBD2'\n",
    "        else:\n",
    "            # For distant relationships, exponential distribution of segment lengths\n",
    "            # Average segment length decreases with more distant relationships\n",
    "            avg_segment_len = max(5, 30 * icor)  # in cM\n",
    "            num_segments = int(max(1, expected_total_cm / avg_segment_len * segment_count_factor))\n",
    "            \n",
    "            # Generate segment lengths from exponential distribution\n",
    "            segment_lengths = np.random.exponential(avg_segment_len, num_segments)\n",
    "            \n",
    "            # Trim very short segments and very long segments\n",
    "            segment_lengths = np.clip(segment_lengths, 3, 100)\n",
    "            \n",
    "            # All IBD1 for distant relationships\n",
    "            ibd_types = ['IBD1'] * num_segments\n",
    "        \n",
    "        # Create segment records\n",
    "        for k in range(num_segments):\n",
    "            # Skip if segment is too short\n",
    "            if segment_lengths[k] < 1:\n",
    "                continue\n",
    "                \n",
    "            # Random chromosome\n",
    "            chrom = np.random.randint(1, 23)\n",
    "            \n",
    "            # Random genetic positions\n",
    "            genetic_length = min(segment_lengths[k], 280)  # Cap at chromosome length\n",
    "            gen_start = np.random.uniform(0, 280 - genetic_length)\n",
    "            gen_end = gen_start + genetic_length\n",
    "            \n",
    "            # Random physical positions (simplified)\n",
    "            phys_start = np.random.randint(1000000, 200000000)\n",
    "            phys_end = phys_start + np.random.randint(1000000, 50000000)\n",
    "            \n",
    "            # Add segment\n",
    "            simulated_segments.append({\n",
    "                'sample1': f\"ind_{i}\",\n",
    "                'sample2': f\"ind_{j}\",\n",
    "                'chrom': chrom,\n",
    "                'phys_start': phys_start,\n",
    "                'phys_end': phys_end,\n",
    "                'ibd_type': ibd_types[k],\n",
    "                'gen_start': gen_start,\n",
    "                'gen_end': gen_end,\n",
    "                'gen_seg_len': genetic_length\n",
    "            })\n",
    "    \n",
    "    return pd.DataFrame(simulated_segments)\n",
    "\n",
    "# Simulate different population scenarios\n",
    "np.random.seed(42)  # For reproducibility\n",
    "outbred_df = simulate_population_effects(seg_df, 'outbred', num_individuals=30)\n",
    "endogamous_df = simulate_population_effects(seg_df, 'endogamous', num_individuals=30)\n",
    "bottleneck_df = simulate_population_effects(seg_df, 'bottleneck', num_individuals=30)\n",
    "admixed_df = simulate_population_effects(seg_df, 'admixed', num_individuals=30)\n",
    "\n",
    "# Compare summary statistics\n",
    "print_summary_stats(outbred_df, \"Outbred Population\")\n",
    "print_summary_stats(endogamous_df, \"Endogamous Population\")\n",
    "print_summary_stats(bottleneck_df, \"Bottleneck Population\")\n",
    "print_summary_stats(admixed_df, \"Admixed Population\")\n",
    "\n",
    "# Compare segment length distributions\n",
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "plt.subplot(1, 4, 1)\n",
    "plt.hist(outbred_df['gen_seg_len'], bins=30, alpha=0.7)\n",
    "plt.xlabel('Segment Length (cM)')\n",
    "plt.ylabel('Count')\n",
    "plt.title('Outbred Population')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.xlim(0, 50)\n",
    "\n",
    "plt.subplot(1, 4, 2)\n",
    "plt.hist(endogamous_df['gen_seg_len'], bins=30, alpha=0.7)\n",
    "plt.xlabel('Segment Length (cM)')\n",
    "plt.ylabel('Count')\n",
    "plt.title('Endogamous Population')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.xlim(0, 50)\n",
    "\n",
    "plt.subplot(1, 4, 3)\n",
    "plt.hist(bottleneck_df['gen_seg_len'], bins=30, alpha=0.7)\n",
    "plt.xlabel('Segment Length (cM)')\n",
    "plt.ylabel('Count')\n",
    "plt.title('Bottleneck Population')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.xlim(0, 50)\n",
    "\n",
    "plt.subplot(1, 4, 4)\n",
    "plt.hist(admixed_df['gen_seg_len'], bins=30, alpha=0.7)\n",
    "plt.xlabel('Segment Length (cM)')\n",
    "plt.ylabel('Count')\n",
    "plt.title('Admixed Population')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.xlim(0, 50)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Compare pairwise IBD sharing distributions\n",
    "def plot_pairwise_sharing_distributions(dfs, names):\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    \n",
    "    for i, (df, name) in enumerate(zip(dfs, names)):\n",
    "        # Calculate total IBD sharing for each pair\n",
    "        pair_sharing = df.groupby(['sample1', 'sample2'])['gen_seg_len'].sum().reset_index()\n",
    "        \n",
    "        # Plot as CDF for comparison\n",
    "        x = np.sort(pair_sharing['gen_seg_len'])\n",
    "        y = np.arange(1, len(x)+1) / len(x)\n",
    "        plt.plot(x, y, label=name)\n",
    "    \n",
    "    plt.xscale('log')\n",
    "    plt.xlabel('Total IBD Sharing (cM)')\n",
    "    plt.ylabel('Cumulative Probability')\n",
    "    plt.title('Pairwise IBD Sharing Distributions')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "# Plot pairwise sharing distributions\n",
    "plot_pairwise_sharing_distributions(\n",
    "    [outbred_df, endogamous_df, bottleneck_df, admixed_df],\n",
    "    ['Outbred', 'Endogamous', 'Bottleneck', 'Admixed']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### 2.1 Length-Based Filtering Strategies\n\nThe most common and straightforward filtering technique is to remove segments below a certain genetic length threshold. Let's implement this and other more sophisticated length-based filters:",
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Let's implement various filtering techniques for IBD segments\n# First, create a simple class to handle IBD filtering operations\n\nclass IBDSegmentFilter:\n    \"\"\"A class for filtering IBD segments based on various criteria.\"\"\"\n    \n    def __init__(self, segments_df):\n        \"\"\"Initialize with a DataFrame of IBD segments.\"\"\"\n        self.original_segments = segments_df.copy()\n        self.filtered_segments = segments_df.copy()\n        self.filter_history = []\n    \n    def reset(self):\n        \"\"\"Reset to original segments.\"\"\"\n        self.filtered_segments = self.original_segments.copy()\n        self.filter_history = []\n        return self\n    \n    def filter_by_length(self, min_length=7, max_length=None):\n        \"\"\"Filter segments by genetic length.\"\"\"\n        n_before = len(self.filtered_segments)\n        \n        # Apply minimum length filter\n        self.filtered_segments = self.filtered_segments[self.filtered_segments['gen_seg_len'] >= min_length]\n        \n        # Apply maximum length filter if specified\n        if max_length is not None:\n            self.filtered_segments = self.filtered_segments[self.filtered_segments['gen_seg_len'] <= max_length]\n        \n        n_after = len(self.filtered_segments)\n        self.filter_history.append({\n            'filter': 'length',\n            'params': {'min_length': min_length, 'max_length': max_length},\n            'removed': n_before - n_after\n        })\n        \n        return self\n    \n    def get_filtered_segments(self):\n        \"\"\"Return the filtered segments DataFrame.\"\"\"\n        return self.filtered_segments\n    \n    def get_filter_summary(self):\n        \"\"\"Return a summary of applied filters.\"\"\"\n        orig_count = len(self.original_segments)\n        final_count = len(self.filtered_segments)\n        \n        summary = {\n            'original_count': orig_count,\n            'filtered_count': final_count,\n            'total_removed': orig_count - final_count,\n            'percent_removed': (orig_count - final_count) / orig_count * 100,\n            'filter_history': self.filter_history\n        }\n        \n        return summary\n\n# Apply length-based filtering with different thresholds\nfilter_thresholds = [3, 5, 7, 10, 15]\nresults = {}\n\nfor threshold in filter_thresholds:\n    filter_obj = IBDSegmentFilter(seg_df.copy())\n    filtered_df = filter_obj.filter_by_length(min_length=threshold).get_filtered_segments()\n    results[threshold] = {\n        'filtered_count': len(filtered_df),\n        'remaining_percent': len(filtered_df) / len(seg_df) * 100\n    }\n\n# Visualize the effect of different length thresholds\nthresholds = list(results.keys())\nremaining = [results[t]['remaining_percent'] for t in thresholds]\n\nplt.figure(figsize=(10, 6))\nplt.plot(thresholds, remaining, 'o-', linewidth=2)\nplt.xlabel('Minimum Segment Length Threshold (cM)')\nplt.ylabel('Remaining Segments (%)')\nplt.title('Effect of Length Filtering on Segment Retention')\nplt.grid(True, alpha=0.3)\nplt.xticks(thresholds)\nplt.ylim(0, 100)\n\n# Add text annotations\nfor i, threshold in enumerate(thresholds):\n    plt.annotate(f\"{remaining[i]:.1f}%\", \n                 (threshold, remaining[i]),\n                 textcoords=\"offset points\",\n                 xytext=(0,10), \n                 ha='center')\n\nplt.show()\n\n# Show detailed results\nprint(\"Effect of different length thresholds:\")\nprint(\"-\" * 50)\nfor threshold in filter_thresholds:\n    print(f\"Threshold: {threshold} cM\")\n    print(f\"  Remaining segments: {results[threshold]['filtered_count']} ({results[threshold]['remaining_percent']:.1f}%)\")\n    print(f\"  Removed segments: {len(seg_df) - results[threshold]['filtered_count']} ({100 - results[threshold]['remaining_percent']:.1f}%)\")\n    print()\n\n# Let's see what the filtered data looks like with a common threshold of 7 cM\nfilter_obj = IBDSegmentFilter(seg_df.copy())\nfiltered_7cm = filter_obj.filter_by_length(min_length=7).get_filtered_segments()\n\nplt.figure(figsize=(12, 5))\n\nplt.subplot(1, 2, 1)\nplt.hist(seg_df['gen_seg_len'], bins=30, alpha=0.7)\nplt.axvline(x=7, color='red', linestyle='--', label='7 cM threshold')\nplt.xlabel('Segment Length (cM)')\nplt.ylabel('Count')\nplt.title('Original Segments')\nplt.legend()\nplt.grid(True, alpha=0.3)\nplt.xlim(0, 50)\n\nplt.subplot(1, 2, 2)\nplt.hist(filtered_7cm['gen_seg_len'], bins=30, alpha=0.7)\nplt.axvline(x=7, color='red', linestyle='--', label='7 cM threshold')\nplt.xlabel('Segment Length (cM)')\nplt.ylabel('Count')\nplt.title('Filtered Segments (\u2265 7 cM)')\nplt.legend()\nplt.grid(True, alpha=0.3)\nplt.xlim(0, 50)\n\nplt.tight_layout()\nplt.show()"
   ],
   "metadata": {},
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### 2.2 Confidence-Based Filtering",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "# Extend our IBD filter class with confidence-based filtering\nclass ConfidenceFilter(IBDSegmentFilter):\n    \"\"\"Extended IBD filter with confidence-based filtering.\"\"\"\n    \n    def __init__(self, segments_df):\n        super().__init__(segments_df)\n        \n        # Add a simulated confidence score if it doesn't exist\n        if 'confidence' not in self.filtered_segments.columns:\n            self._add_simulated_confidence()\n    \n    def _add_simulated_confidence(self):\n        \"\"\"Add simulated confidence scores based on segment characteristics.\n        \n        In a real-world scenario, these scores would come from the IBD detection algorithm.\n        Here we simulate them based on segment length and other features.\n        \"\"\"\n        # Base confidence on segment length (longer segments are more reliable)\n        # Use a sigmoid function scaled between 0.5 and 1.0\n        length_conf = 0.5 + 0.5 / (1 + np.exp(-(self.filtered_segments['gen_seg_len'] - 7)))\n        \n        # Add some noise\n        noise = np.random.normal(0, 0.05, size=len(self.filtered_segments))\n        \n        # Combine and clip to [0, 1]\n        confidence = np.clip(length_conf + noise, 0, 1)\n        \n        self.filtered_segments['confidence'] = confidence\n        self.original_segments['confidence'] = confidence.copy()\n    \n    def filter_by_confidence(self, min_confidence=0.8):\n        \"\"\"Filter segments by their confidence score.\"\"\"\n        n_before = len(self.filtered_segments)\n        \n        self.filtered_segments = self.filtered_segments[self.filtered_segments['confidence'] >= min_confidence]\n        \n        n_after = len(self.filtered_segments)\n        self.filter_history.append({\n            'filter': 'confidence',\n            'params': {'min_confidence': min_confidence},\n            'removed': n_before - n_after\n        })\n        \n        return self\n    \n    def filter_by_percentile(self, keep_percentile=80):\n        \"\"\"Keep only the top X percentile of segments by confidence.\"\"\"\n        n_before = len(self.filtered_segments)\n        \n        threshold = np.percentile(self.filtered_segments['confidence'], 100 - keep_percentile)\n        self.filtered_segments = self.filtered_segments[self.filtered_segments['confidence'] >= threshold]\n        \n        n_after = len(self.filtered_segments)\n        self.filter_history.append({\n            'filter': 'percentile',\n            'params': {'keep_percentile': keep_percentile, 'threshold': threshold},\n            'removed': n_before - n_after\n        })\n        \n        return self\n\n# Apply confidence-based filtering\nconfidence_filter = ConfidenceFilter(seg_df.copy())\n\n# Show the distribution of confidence scores\nplt.figure(figsize=(10, 6))\nplt.hist(confidence_filter.filtered_segments['confidence'], bins=30, alpha=0.7)\nplt.axvline(x=0.8, color='red', linestyle='--', label='0.8 threshold')\nplt.axvline(x=0.9, color='green', linestyle='--', label='0.9 threshold')\nplt.xlabel('Confidence Score')\nplt.ylabel('Count')\nplt.title('Distribution of Segment Confidence Scores')\nplt.legend()\nplt.grid(True, alpha=0.3)\nplt.show()\n\n# Try different confidence thresholds\nconfidence_thresholds = [0.6, 0.7, 0.8, 0.9]\nconf_results = {}\n\nfor threshold in confidence_thresholds:\n    cf = ConfidenceFilter(seg_df.copy())\n    filtered = cf.filter_by_confidence(min_confidence=threshold).get_filtered_segments()\n    conf_results[threshold] = {\n        'filtered_count': len(filtered),\n        'remaining_percent': len(filtered) / len(seg_df) * 100\n    }\n\n# Plot results\nthresholds = list(conf_results.keys())\nremaining = [conf_results[t]['remaining_percent'] for t in thresholds]\n\nplt.figure(figsize=(10, 6))\nplt.plot(thresholds, remaining, 'o-', linewidth=2)\nplt.xlabel('Minimum Confidence Threshold')\nplt.ylabel('Remaining Segments (%)')\nplt.title('Effect of Confidence Filtering on Segment Retention')\nplt.grid(True, alpha=0.3)\nplt.xticks(thresholds)\nplt.ylim(0, 100)\n\n# Add text annotations\nfor i, threshold in enumerate(thresholds):\n    plt.annotate(f\"{remaining[i]:.1f}%\", \n                 (threshold, remaining[i]),\n                 textcoords=\"offset points\",\n                 xytext=(0,10), \n                 ha='center')\n\nplt.show()\n\n# Compare length-based vs. confidence-based filtering\nprint(\"Comparison of filtering methods:\")\nprint(\"-\" * 50)\nlf = IBDSegmentFilter(seg_df.copy())\ncf = ConfidenceFilter(seg_df.copy())\n\nlength_filtered = lf.filter_by_length(min_length=7).get_filtered_segments()\nconfidence_filtered = cf.filter_by_confidence(min_confidence=0.8).get_filtered_segments()\n\nprint(f\"Original segments: {len(seg_df)}\")\nprint(f\"After length filtering (\u2265 7 cM): {len(length_filtered)} ({len(length_filtered)/len(seg_df)*100:.1f}%)\")\nprint(f\"After confidence filtering (\u2265 0.8): {len(confidence_filtered)} ({len(confidence_filtered)/len(seg_df)*100:.1f}%)\")\n\n# Visualize the relationship between segment length and confidence\nplt.figure(figsize=(10, 6))\nplt.scatter(seg_df['gen_seg_len'], cf.original_segments['confidence'], alpha=0.3)\nplt.axhline(y=0.8, color='red', linestyle='--', label='0.8 confidence threshold')\nplt.axvline(x=7, color='green', linestyle='--', label='7 cM threshold')\nplt.xlabel('Segment Length (cM)')\nplt.ylabel('Confidence Score')\nplt.title('Relationship Between Segment Length and Confidence')\nplt.legend()\nplt.grid(True, alpha=0.3)\nplt.xlim(0, 50)\nplt.ylim(0, 1)\nplt.show()"
   ],
   "metadata": {},
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### 2.3 Consistency-Based Filtering",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "# Extend our filter with consistency-based methods\nclass ConsistencyFilter(IBDSegmentFilter):\n    \"\"\"Filter IBD segments based on consistency metrics.\"\"\"\n    \n    def __init__(self, segments_df):\n        super().__init__(segments_df)\n        # Precompute useful metrics\n        self._compute_pair_metrics()\n    \n    def _compute_pair_metrics(self):\n        \"\"\"Compute metrics for each pair of individuals.\"\"\"\n        # Number of segments per pair\n        self.pair_counts = self.filtered_segments.groupby(['sample1', 'sample2']).size().reset_index(name='seg_count')\n        \n        # Total shared cM per pair\n        self.pair_totals = self.filtered_segments.groupby(['sample1', 'sample2'])['gen_seg_len'].sum().reset_index(name='total_cm')\n        \n        # Merge these metrics\n        self.pair_metrics = pd.merge(self.pair_counts, self.pair_totals, on=['sample1', 'sample2'])\n    \n    def filter_by_segment_count(self, min_segments=2):\n        \"\"\"Filter pairs with too few segments.\"\"\"\n        n_before = len(self.filtered_segments)\n        \n        # Get valid pairs\n        valid_pairs = self.pair_counts[self.pair_counts['seg_count'] >= min_segments]\n        \n        # Filter segments\n        valid_pairs_tuples = list(zip(valid_pairs['sample1'], valid_pairs['sample2']))\n        pair_tuples = list(zip(self.filtered_segments['sample1'], self.filtered_segments['sample2']))\n        self.filtered_segments = self.filtered_segments[\n            [p in valid_pairs_tuples for p in pair_tuples]\n        ]\n        \n        n_after = len(self.filtered_segments)\n        self.filter_history.append({\n            'filter': 'segment_count',\n            'params': {'min_segments': min_segments},\n            'removed': n_before - n_after\n        })\n        \n        # Recompute metrics\n        self._compute_pair_metrics()\n        \n        return self\n    \n    def filter_by_total_sharing(self, min_total_cm=20):\n        \"\"\"Filter pairs with too little total sharing.\"\"\"\n        n_before = len(self.filtered_segments)\n        \n        # Get valid pairs\n        valid_pairs = self.pair_totals[self.pair_totals['total_cm'] >= min_total_cm]\n        \n        # Filter segments\n        valid_pairs_tuples = list(zip(valid_pairs['sample1'], valid_pairs['sample2']))\n        pair_tuples = list(zip(self.filtered_segments['sample1'], self.filtered_segments['sample2']))\n        self.filtered_segments = self.filtered_segments[\n            [p in valid_pairs_tuples for p in pair_tuples]\n        ]\n        \n        n_after = len(self.filtered_segments)\n        self.filter_history.append({\n            'filter': 'total_sharing',\n            'params': {'min_total_cm': min_total_cm},\n            'removed': n_before - n_after\n        })\n        \n        # Recompute metrics\n        self._compute_pair_metrics()\n        \n        return self\n    \n    def filter_by_cross_chromosome_consistency(self, min_chromosomes=2):\n        \"\"\"Keep only pairs that share segments on multiple chromosomes.\"\"\"\n        n_before = len(self.filtered_segments)\n        \n        # Count unique chromosomes per pair\n        chrom_counts = self.filtered_segments.groupby(['sample1', 'sample2'])['chrom'].nunique().reset_index(name='chrom_count')\n        \n        # Get valid pairs\n        valid_pairs = chrom_counts[chrom_counts['chrom_count'] >= min_chromosomes]\n        \n        # Filter segments\n        valid_pairs_tuples = list(zip(valid_pairs['sample1'], valid_pairs['sample2']))\n        pair_tuples = list(zip(self.filtered_segments['sample1'], self.filtered_segments['sample2']))\n        self.filtered_segments = self.filtered_segments[\n            [p in valid_pairs_tuples for p in pair_tuples]\n        ]\n        \n        n_after = len(self.filtered_segments)\n        self.filter_history.append({\n            'filter': 'cross_chromosome',\n            'params': {'min_chromosomes': min_chromosomes},\n            'removed': n_before - n_after\n        })\n        \n        # Recompute metrics\n        self._compute_pair_metrics()\n        \n        return self\n\n# Apply consistency-based filtering\nconsistency_filter = ConsistencyFilter(seg_df.copy())\n\n# Explore initial pair metrics\nprint(\"Initial pair statistics:\")\nprint(f\"Total pairs: {len(consistency_filter.pair_metrics)}\")\nprint(f\"Average segments per pair: {consistency_filter.pair_metrics['seg_count'].mean():.2f}\")\nprint(f\"Average total cM per pair: {consistency_filter.pair_metrics['total_cm'].mean():.2f}\")\nprint(f\"Pairs with only 1 segment: {(consistency_filter.pair_metrics['seg_count'] == 1).sum()} ({(consistency_filter.pair_metrics['seg_count'] == 1).sum() / len(consistency_filter.pair_metrics) * 100:.1f}%)\")\n\n# Visualize segment count distribution\nplt.figure(figsize=(10, 6))\nplt.hist(consistency_filter.pair_metrics['seg_count'], bins=range(1, 20), alpha=0.7)\nplt.axvline(x=2, color='red', linestyle='--', label='min 2 segments')\nplt.axvline(x=3, color='green', linestyle='--', label='min 3 segments')\nplt.xlabel('Number of Segments per Pair')\nplt.ylabel('Count of Pairs')\nplt.title('Distribution of Segment Counts per Pair')\nplt.legend()\nplt.grid(True, alpha=0.3)\nplt.xticks(range(1, 20))\nplt.show()\n\n# Apply segment count filtering and see the effect\nfiltered_by_count = consistency_filter.filter_by_segment_count(min_segments=2).get_filtered_segments()\nprint(f\"\\nAfter filtering pairs with fewer than 2 segments:\")\nprint(f\"Remaining segments: {len(filtered_by_count)} ({len(filtered_by_count) / len(seg_df) * 100:.1f}%)\")\nprint(f\"Remaining pairs: {len(consistency_filter.pair_metrics)}\")\n\n# Apply chromosome consistency filtering\nconsistency_filter = ConsistencyFilter(seg_df.copy())\nchrom_counts = consistency_filter.filtered_segments.groupby(['sample1', 'sample2'])['chrom'].nunique().reset_index(name='chrom_count')\n\n# Visualize chromosome count distribution\nplt.figure(figsize=(10, 6))\nplt.hist(chrom_counts['chrom_count'], bins=range(1, 15), alpha=0.7)\nplt.axvline(x=2, color='red', linestyle='--', label='min 2 chromosomes')\nplt.xlabel('Number of Unique Chromosomes per Pair')\nplt.ylabel('Count of Pairs')\nplt.title('Distribution of Chromosome Counts per Pair')\nplt.legend()\nplt.grid(True, alpha=0.3)\nplt.xticks(range(1, 15))\nplt.show()\n\n# Apply chromosome consistency filtering\nfiltered_by_chrom = consistency_filter.filter_by_cross_chromosome_consistency(min_chromosomes=2).get_filtered_segments()\nprint(f\"\\nAfter filtering pairs with segments on fewer than 2 chromosomes:\")\nprint(f\"Remaining segments: {len(filtered_by_chrom)} ({len(filtered_by_chrom) / len(seg_df) * 100:.1f}%)\")\nprint(f\"Remaining pairs: {len(consistency_filter.pair_metrics)}\")\n\n# Compare the effects of different consistency filters\nprint(\"\\nComparison of consistency filtering methods:\")\nprint(\"-\" * 50)\ncf1 = ConsistencyFilter(seg_df.copy())\ncf2 = ConsistencyFilter(seg_df.copy())\ncf3 = ConsistencyFilter(seg_df.copy())\n\nfiltered1 = cf1.filter_by_segment_count(min_segments=2).get_filtered_segments()\nfiltered2 = cf2.filter_by_total_sharing(min_total_cm=20).get_filtered_segments()\nfiltered3 = cf3.filter_by_cross_chromosome_consistency(min_chromosomes=2).get_filtered_segments()\n\nprint(f\"Original segments: {len(seg_df)}\")\nprint(f\"After segment count filtering (\u2265 2): {len(filtered1)} ({len(filtered1)/len(seg_df)*100:.1f}%)\")\nprint(f\"After total sharing filtering (\u2265 20 cM): {len(filtered2)} ({len(filtered2)/len(seg_df)*100:.1f}%)\")\nprint(f\"After chromosome consistency filtering (\u2265 2 chroms): {len(filtered3)} ({len(filtered3)/len(seg_df)*100:.1f}%)\")"
   ],
   "metadata": {},
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### 2.4 Population-Specific Filtering Strategies",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "# Implement population-specific filtering\nclass PopulationAwareFilter(IBDSegmentFilter):\n    \"\"\"Filter IBD segments with awareness of population structure.\"\"\"\n    \n    def __init__(self, segments_df, population_type='outbred'):\n        \"\"\"Initialize the filter with population type.\"\"\"\n        super().__init__(segments_df)\n        self.population_type = population_type\n    \n    def apply_population_specific_filters(self):\n        \"\"\"Apply filters appropriate for the specified population.\"\"\"\n        n_before = len(self.filtered_segments)\n        \n        if self.population_type == 'outbred':\n            # Standard thresholds for outbred populations\n            self.filter_by_length(min_length=7)  # Standard threshold\n            \n        elif self.population_type == 'endogamous':\n            # More stringent thresholds for endogamous populations\n            # Endogamous populations have elevated background IBD\n            self.filter_by_length(min_length=10)  # Higher threshold to reduce false positives\n            \n            # Add additional constraints - implement a simple consistency filter\n            # Count segments per pair\n            pair_counts = self.filtered_segments.groupby(['sample1', 'sample2']).size().reset_index(name='count')\n            valid_pairs = pair_counts[pair_counts['count'] >= 2]  # Require at least 2 segments\n            \n            # Filter segments\n            valid_pairs_tuples = list(zip(valid_pairs['sample1'], valid_pairs['sample2']))\n            pair_tuples = list(zip(self.filtered_segments['sample1'], self.filtered_segments['sample2']))\n            self.filtered_segments = self.filtered_segments[\n                [p in valid_pairs_tuples for p in pair_tuples]\n            ]\n            \n        elif self.population_type == 'bottleneck':\n            # Intermediate thresholds for bottleneck populations\n            self.filter_by_length(min_length=8)  # Slightly higher threshold\n            \n        elif self.population_type == 'admixed':\n            # Different strategy for admixed populations\n            self.filter_by_length(min_length=6)  # Slightly lower threshold\n            \n            # Apply an outlier filter instead\n            mean_length = self.filtered_segments['gen_seg_len'].mean()\n            std_length = self.filtered_segments['gen_seg_len'].std()\n            self.filtered_segments = self.filtered_segments[\n                abs(self.filtered_segments['gen_seg_len'] - mean_length) <= 3 * std_length\n            ]\n            \n        else:\n            raise ValueError(f\"Unknown population type: {self.population_type}\")\n        \n        n_after = len(self.filtered_segments)\n        self.filter_history.append({\n            'filter': f'population_{self.population_type}',\n            'params': {},\n            'removed': n_before - n_after\n        })\n        \n        return self\n\n# Compare filtering results for different population types\n# First, recreate our simulated population data\nnp.random.seed(42)  # For reproducibility\noutbred_df = simulate_population_effects(seg_df, 'outbred', num_individuals=30)\nendogamous_df = simulate_population_effects(seg_df, 'endogamous', num_individuals=30)\nbottleneck_df = simulate_population_effects(seg_df, 'bottleneck', num_individuals=30)\nadmixed_df = simulate_population_effects(seg_df, 'admixed', num_individuals=30)\n\n# Apply population-aware filtering\noutbred_filter = PopulationAwareFilter(outbred_df, 'outbred')\nendogamous_filter = PopulationAwareFilter(endogamous_df, 'endogamous')\nbottleneck_filter = PopulationAwareFilter(bottleneck_df, 'bottleneck')\nadmixed_filter = PopulationAwareFilter(admixed_df, 'admixed')\n\n# Apply filters\noutbred_filtered = outbred_filter.apply_population_specific_filters().get_filtered_segments()\nendogamous_filtered = endogamous_filter.apply_population_specific_filters().get_filtered_segments()\nbottleneck_filtered = bottleneck_filter.apply_population_specific_filters().get_filtered_segments()\nadmixed_filtered = admixed_filter.apply_population_specific_filters().get_filtered_segments()\n\n# Compare results\nprint(\"Population-specific filtering results:\")\nprint(\"-\" * 50)\nprint(f\"Outbred population:\")\nprint(f\"  Original segments: {len(outbred_df)}\")\nprint(f\"  After filtering: {len(outbred_filtered)} ({len(outbred_filtered)/len(outbred_df)*100:.1f}%)\")\nprint(f\"  Filter summary: {outbred_filter.get_filter_summary()['filter_history']}\")\nprint()\n\nprint(f\"Endogamous population:\")\nprint(f\"  Original segments: {len(endogamous_df)}\")\nprint(f\"  After filtering: {len(endogamous_filtered)} ({len(endogamous_filtered)/len(endogamous_df)*100:.1f}%)\")\nprint(f\"  Filter summary: {endogamous_filter.get_filter_summary()['filter_history']}\")\nprint()\n\nprint(f\"Bottleneck population:\")\nprint(f\"  Original segments: {len(bottleneck_df)}\")\nprint(f\"  After filtering: {len(bottleneck_filtered)} ({len(bottleneck_filtered)/len(bottleneck_df)*100:.1f}%)\")\nprint(f\"  Filter summary: {bottleneck_filter.get_filter_summary()['filter_history']}\")\nprint()\n\nprint(f\"Admixed population:\")\nprint(f\"  Original segments: {len(admixed_df)}\")\nprint(f\"  After filtering: {len(admixed_filtered)} ({len(admixed_filtered)/len(admixed_df)*100:.1f}%)\")\nprint(f\"  Filter summary: {admixed_filter.get_filter_summary()['filter_history']}\")\nprint()\n\n# Visualize the differences in segment length distributions after filtering\nplt.figure(figsize=(20, 10))\n\nplt.subplot(2, 4, 1)\nplt.hist(outbred_df['gen_seg_len'], bins=30, alpha=0.7)\nplt.xlabel('Segment Length (cM)')\nplt.ylabel('Count')\nplt.title('Outbred - Before Filtering')\nplt.grid(True, alpha=0.3)\nplt.xlim(0, 50)\n\nplt.subplot(2, 4, 2)\nplt.hist(endogamous_df['gen_seg_len'], bins=30, alpha=0.7)\nplt.xlabel('Segment Length (cM)')\nplt.ylabel('Count')\nplt.title('Endogamous - Before Filtering')\nplt.grid(True, alpha=0.3)\nplt.xlim(0, 50)\n\nplt.subplot(2, 4, 3)\nplt.hist(bottleneck_df['gen_seg_len'], bins=30, alpha=0.7)\nplt.xlabel('Segment Length (cM)')\nplt.ylabel('Count')\nplt.title('Bottleneck - Before Filtering')\nplt.grid(True, alpha=0.3)\nplt.xlim(0, 50)\n\nplt.subplot(2, 4, 4)\nplt.hist(admixed_df['gen_seg_len'], bins=30, alpha=0.7)\nplt.xlabel('Segment Length (cM)')\nplt.ylabel('Count')\nplt.title('Admixed - Before Filtering')\nplt.grid(True, alpha=0.3)\nplt.xlim(0, 50)\n\nplt.subplot(2, 4, 5)\nplt.hist(outbred_filtered['gen_seg_len'], bins=30, alpha=0.7)\nplt.xlabel('Segment Length (cM)')\nplt.ylabel('Count')\nplt.title('Outbred - After Filtering')\nplt.grid(True, alpha=0.3)\nplt.xlim(0, 50)\n\nplt.subplot(2, 4, 6)\nplt.hist(endogamous_filtered['gen_seg_len'], bins=30, alpha=0.7)\nplt.xlabel('Segment Length (cM)')\nplt.ylabel('Count')\nplt.title('Endogamous - After Filtering')\nplt.grid(True, alpha=0.3)\nplt.xlim(0, 50)\n\nplt.subplot(2, 4, 7)\nplt.hist(bottleneck_filtered['gen_seg_len'], bins=30, alpha=0.7)\nplt.xlabel('Segment Length (cM)')\nplt.ylabel('Count')\nplt.title('Bottleneck - After Filtering')\nplt.grid(True, alpha=0.3)\nplt.xlim(0, 50)\n\nplt.subplot(2, 4, 8)\nplt.hist(admixed_filtered['gen_seg_len'], bins=30, alpha=0.7)\nplt.xlabel('Segment Length (cM)')\nplt.ylabel('Count')\nplt.title('Admixed - After Filtering')\nplt.grid(True, alpha=0.3)\nplt.xlim(0, 50)\n\nplt.tight_layout()\nplt.show()\n\n# Function to compare pair-wise sharing distributions before and after filtering\ndef plot_pairwise_sharing_comparison(before_dfs, after_dfs, names):\n    plt.figure(figsize=(15, 6))\n    \n    plt.subplot(1, 2, 1)\n    for i, (df, name) in enumerate(zip(before_dfs, names)):\n        # Calculate total IBD sharing for each pair\n        pair_sharing = df.groupby(['sample1', 'sample2'])['gen_seg_len'].sum().reset_index()\n        \n        # Plot as CDF for comparison\n        x = np.sort(pair_sharing['gen_seg_len'])\n        y = np.arange(1, len(x)+1) / len(x)\n        plt.plot(x, y, label=name)\n    \n    plt.xscale('log')\n    plt.xlabel('Total IBD Sharing (cM)')\n    plt.ylabel('Cumulative Probability')\n    plt.title('Before Filtering')\n    plt.grid(True, alpha=0.3)\n    plt.legend()\n    \n    plt.subplot(1, 2, 2)\n    for i, (df, name) in enumerate(zip(after_dfs, names)):\n        # Calculate total IBD sharing for each pair\n        pair_sharing = df.groupby(['sample1', 'sample2'])['gen_seg_len'].sum().reset_index()\n        \n        # Plot as CDF for comparison\n        if len(pair_sharing) > 0:  # Check if there are any pairs left\n            x = np.sort(pair_sharing['gen_seg_len'])\n            y = np.arange(1, len(x)+1) / len(x)\n            plt.plot(x, y, label=name)\n    \n    plt.xscale('log')\n    plt.xlabel('Total IBD Sharing (cM)')\n    plt.ylabel('Cumulative Probability')\n    plt.title('After Filtering')\n    plt.grid(True, alpha=0.3)\n    plt.legend()\n    \n    plt.tight_layout()\n    plt.show()\n\n# Compare pairwise sharing distributions before and after filtering\nplot_pairwise_sharing_comparison(\n    [outbred_df, endogamous_df, bottleneck_df, admixed_df],\n    [outbred_filtered, endogamous_filtered, bottleneck_filtered, admixed_filtered],\n    ['Outbred', 'Endogamous', 'Bottleneck', 'Admixed']\n)"
   ],
   "metadata": {},
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## 3. Handling Missing Data in Pedigree Reconstruction",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "### 3.1 Types of Missing Data\n\nIn genetic genealogy, missing data can take various forms:\n\n- **Missing individuals**: Key ancestors or relatives who connect branches of a family may not have genetic data available\n- **Missing segments**: Due to sparse testing coverage, IBD detection errors, or filtering\n- **Missing chromosomes**: Some testing platforms may not cover certain chromosomes (e.g., Y or X)\n- **Missing metadata**: Important context like birth years, locations, or relationships\n\nLet's explore how missing data impacts pedigree reconstruction and strategies to address these issues.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "# Create a simulated pedigree network with missing data\ndef create_simulated_pedigree(num_individuals=30, missing_rate=0.2):\n    \"\"\"\n    Create a simulated pedigree with some missing individuals.\n    \n    Args:\n        num_individuals: Number of individuals in the complete pedigree\n        missing_rate: Fraction of individuals to mark as missing\n        \n    Returns:\n        G: NetworkX graph of the pedigree\n        missing_ids: IDs of individuals marked as missing\n    \"\"\"\n    # Create a directed graph\n    G = nx.DiGraph()\n    \n    # Add individuals\n    for i in range(num_individuals):\n        birth_year = np.random.randint(1900, 2000)\n        G.add_node(i, id=i, birth_year=birth_year, has_genetic_data=True)\n    \n    # Add family relationships - simplified model\n    # Each person has up to 2 parents and some number of children\n    # Start from the oldest generation\n    individuals = list(range(num_individuals))\n    individuals.sort(key=lambda x: G.nodes[x]['birth_year'])\n    \n    for i, person_id in enumerate(individuals):\n        birth_year = G.nodes[person_id]['birth_year']\n        \n        # Add parents for everyone except the oldest generation\n        if i >= num_individuals // 3:  # Skip oldest ~1/3 of individuals\n            # Find potential parents (at least 20 years older)\n            potential_parents = [\n                p for p in individuals[:i] \n                if G.nodes[p]['birth_year'] <= birth_year - 20\n            ]\n            \n            # Add one or two parents if available\n            if len(potential_parents) >= 2:\n                # Add father and mother\n                father = np.random.choice(potential_parents)\n                potential_parents.remove(father)\n                mother = np.random.choice(potential_parents)\n                \n                G.add_edge(father, person_id, relationship='parent-child')\n                G.add_edge(mother, person_id, relationship='parent-child')\n            elif len(potential_parents) == 1:\n                # Add one parent\n                parent = potential_parents[0]\n                G.add_edge(parent, person_id, relationship='parent-child')\n    \n    # Mark some individuals as missing (no genetic data)\n    num_missing = int(num_individuals * missing_rate)\n    missing_ids = np.random.choice(individuals, size=num_missing, replace=False)\n    \n    for person_id in missing_ids:\n        G.nodes[person_id]['has_genetic_data'] = False\n    \n    return G, missing_ids\n\n# Create a pedigree with some missing individuals\nnp.random.seed(42)\npedigree, missing_ids = create_simulated_pedigree(num_individuals=30, missing_rate=0.2)\n\n# Analyze the impact of missing individuals\nprint(f\"Total individuals in pedigree: {len(pedigree.nodes())}\")\nprint(f\"Missing individuals: {len(missing_ids)} ({len(missing_ids)/len(pedigree.nodes())*100:.1f}%)\")\n\n# Check how many connections are affected by missing individuals\ntotal_edges = len(pedigree.edges())\naffected_edges = 0\n\nfor u, v in pedigree.edges():\n    if u in missing_ids or v in missing_ids:\n        affected_edges += 1\n\nprint(f\"Total relationships: {total_edges}\")\nprint(f\"Relationships affected by missing individuals: {affected_edges} ({affected_edges/total_edges*100:.1f}%)\")\n\n# Visualize the pedigree with missing individuals highlighted\nplt.figure(figsize=(12, 10))\n\n# Create a spring layout positioned by birth year (older individuals at the top)\npos = nx.spring_layout(pedigree, seed=42)\n\n# Adjust y-coordinate based on birth year\nfor node in pedigree.nodes():\n    birth_year = pedigree.nodes[node]['birth_year']\n    # Normalize to 0-1 range\n    normalized_year = (birth_year - 1900) / 100\n    # Invert so older people are at the top\n    pos[node][1] = 1 - normalized_year\n\n# Draw nodes\nnode_colors = ['red' if node in missing_ids else 'lightblue' for node in pedigree.nodes()]\nnx.draw_networkx_nodes(pedigree, pos, node_color=node_colors, node_size=500, alpha=0.8)\n\n# Draw edges\nedge_colors = ['red' if u in missing_ids or v in missing_ids else 'black' for u, v in pedigree.edges()]\nnx.draw_networkx_edges(pedigree, pos, edge_color=edge_colors, width=1.5, alpha=0.7)\n\n# Add labels\nlabels = {node: f\"{node}\\n({pedigree.nodes[node]['birth_year']})\" for node in pedigree.nodes()}\nnx.draw_networkx_labels(pedigree, pos, labels=labels, font_size=10)\n\nplt.title('Simulated Pedigree with Missing Individuals (in red)')\nplt.axis('off')\nplt.tight_layout()\nplt.show()\n\n# Function to simulate the impact of missing data on IBD segment detection\ndef simulate_missing_data_impact(pedigree, missing_ids, num_segments_per_relationship=10):\n    \"\"\"\n    Simulate IBD segments for all genetic relationships and measure the impact of missing individuals.\n    \n    Args:\n        pedigree: NetworkX graph of the pedigree\n        missing_ids: IDs of individuals marked as missing\n        num_segments_per_relationship: Number of segments to simulate per relationship\n        \n    Returns:\n        segments_df: DataFrame of simulated IBD segments\n        missing_segments_df: DataFrame showing segments lost due to missing individuals\n    \"\"\"\n    segments = []\n    missing_segments = []\n    \n    # Get all pairs with genetic relationships (up to 3 degrees of separation)\n    for start_node in pedigree.nodes():\n        if start_node in missing_ids:\n            continue  # Skip missing individuals\n            \n        # Find all related individuals within 3 steps\n        for length in range(1, 4):\n            for path in nx.all_simple_paths(pedigree, start_node, cutoff=length):\n                if len(path) > 1:\n                    end_node = path[-1]\n                    \n                    # Skip if end node is missing\n                    if end_node in missing_ids:\n                        continue\n                    \n                    # Calculate a relationship coefficient (simplified)\n                    # For parent-child: 0.5, grandparent-grandchild: 0.25, etc.\n                    relationship_coef = 0.5 ** (len(path) - 1)\n                    \n                    # Check if any intermediary is missing\n                    has_missing_intermediary = any(node in missing_ids for node in path[1:-1])\n                    \n                    # Simulate segments for this relationship\n                    for _ in range(num_segments_per_relationship):\n                        # Segment length depends on relationship (closer = longer)\n                        avg_length = max(5, 50 * relationship_coef)\n                        length_cm = np.random.exponential(avg_length)\n                        \n                        # Skip very short segments\n                        if length_cm < 3:\n                            continue\n                            \n                        segment = {\n                            'sample1': f\"ind_{start_node}\",\n                            'sample2': f\"ind_{end_node}\",\n                            'chrom': np.random.randint(1, 23),\n                            'gen_start': np.random.uniform(0, 250),\n                            'gen_seg_len': length_cm,\n                            'relationship_type': f\"{len(path) - 1}_degrees\",\n                            'relationship_coef': relationship_coef\n                        }\n                        segment['gen_end'] = segment['gen_start'] + segment['gen_seg_len']\n                        \n                        # Record if this segment is lost due to missing intermediaries\n                        if has_missing_intermediary:\n                            missing_segments.append(segment)\n                        else:\n                            segments.append(segment)\n    \n    # Convert to DataFrames\n    segments_df = pd.DataFrame(segments)\n    missing_segments_df = pd.DataFrame(missing_segments)\n    \n    return segments_df, missing_segments_df\n\n# Simulate IBD segments and measure impact of missing individuals\nsegments_df, missing_segments_df = simulate_missing_data_impact(pedigree, missing_ids)\n\nprint(f\"Detected segments: {len(segments_df)}\")\nprint(f\"Missing segments (due to missing intermediaries): {len(missing_segments_df)}\")\nprint(f\"Percentage of segments lost: {len(missing_segments_df)/(len(segments_df) + len(missing_segments_df))*100:.1f}%\")\n\n# Compare segment length distributions for detected vs. missing segments\nplt.figure(figsize=(12, 5))\n\nplt.subplot(1, 2, 1)\nplt.hist(segments_df['gen_seg_len'], bins=30, alpha=0.7, label='Detected')\nplt.hist(missing_segments_df['gen_seg_len'], bins=30, alpha=0.7, label='Missing')\nplt.xlabel('Segment Length (cM)')\nplt.ylabel('Count')\nplt.title('Segment Length Distribution')\nplt.legend()\nplt.grid(True, alpha=0.3)\nplt.xlim(0, 100)\n\nplt.subplot(1, 2, 2)\nplt.hist(segments_df['relationship_coef'], bins=[0, 0.125, 0.25, 0.5, 1.0], alpha=0.7, label='Detected')\nplt.hist(missing_segments_df['relationship_coef'], bins=[0, 0.125, 0.25, 0.5, 1.0], alpha=0.7, label='Missing')\nplt.xlabel('Relationship Coefficient')\nplt.ylabel('Count')\nplt.title('Relationship Distribution')\nplt.legend()\nplt.grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n\n# Calculate the percentage of segments lost by relationship degree\nrelationship_impact = pd.DataFrame({\n    'detected': segments_df.groupby('relationship_type').size(),\n    'missing': missing_segments_df.groupby('relationship_type').size()\n}).fillna(0)\n\nrelationship_impact['total'] = relationship_impact['detected'] + relationship_impact['missing']\nrelationship_impact['percent_lost'] = relationship_impact['missing'] / relationship_impact['total'] * 100\n\nprint(\"Impact by relationship degree:\")\nprint(relationship_impact)"
   ],
   "metadata": {},
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### 3.2 Strategies for Handling Missing Data",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "# Implement strategies for handling missing data in pedigree reconstruction\n\nclass MissingDataHandler:\n    \"\"\"\n    Class for implementing strategies to handle missing data in pedigree reconstruction.\n    \"\"\"\n    \n    def __init__(self, pedigree, segments_df):\n        \"\"\"\n        Initialize with a pedigree and IBD segments.\n        \n        Args:\n            pedigree: NetworkX graph representing the pedigree structure\n            segments_df: DataFrame of IBD segments\n        \"\"\"\n        self.pedigree = pedigree\n        self.segments_df = segments_df\n        \n        # Extract individual IDs from segment data\n        self.individuals = set()\n        for _, row in segments_df.iterrows():\n            self.individuals.add(row['sample1'])\n            self.individuals.add(row['sample2'])\n    \n    def identify_missing_connections(self):\n        \"\"\"\n        Identify potential missing connections based on transitive relationships.\n        \n        Returns:\n            DataFrame of potential missing connections\n        \"\"\"\n        # Create an undirected graph of observed IBD connections\n        G = nx.Graph()\n        \n        # Add all individuals as nodes\n        for ind in self.individuals:\n            G.add_node(ind)\n        \n        # Add edges for observed segments\n        for _, row in self.segments_df.iterrows():\n            G.add_edge(row['sample1'], row['sample2'], \n                      length=row['gen_seg_len'], \n                      count=1)\n        \n        # Find potential missing connections through transitive relationships\n        missing_connections = []\n        \n        # Check all pairs of individuals with distance 2 (common connection)\n        for ind1 in G.nodes():\n            for ind2 in G.nodes():\n                if ind1 >= ind2:  # Skip self-connections and duplicates\n                    continue\n                    \n                # Skip if direct connection already exists\n                if G.has_edge(ind1, ind2):\n                    continue\n                \n                # Check if there's a path of length 2\n                paths = list(nx.all_simple_paths(G, ind1, ind2, cutoff=2))\n                if len(paths) > 0:\n                    # Found at least one common connection\n                    for path in paths:\n                        if len(path) == 3:  # Path of length 2 (3 nodes)\n                            intermediary = path[1]\n                            \n                            # Get segment lengths for both connections\n                            len1 = G[ind1][intermediary]['length']\n                            len2 = G[intermediary][ind2]['length']\n                            \n                            # Estimate expected segment length for missing connection\n                            # This is a simplification - in reality it depends on the relationship\n                            expected_length = min(len1, len2) / 2\n                            \n                            missing_connections.append({\n                                'sample1': ind1,\n                                'sample2': ind2,\n                                'intermediary': intermediary,\n                                'expected_length': expected_length\n                            })\n        \n        return pd.DataFrame(missing_connections)\n    \n    def infer_phantom_ancestors(self):\n        \"\"\"\n        Infer the existence of \"phantom\" ancestors who might explain observed IBD patterns.\n        \n        Returns:\n            List of inferred phantom ancestors with their likely descendants\n        \"\"\"\n        # Create a graph of observed IBD connections\n        G = nx.Graph()\n        \n        # Add edges for observed segments\n        for _, row in self.segments_df.iterrows():\n            if G.has_edge(row['sample1'], row['sample2']):\n                # Update existing edge\n                G[row['sample1']][row['sample2']]['segments'].append({\n                    'length': row['gen_seg_len'],\n                    'chrom': row['chrom'],\n                    'start': row['gen_start'],\n                    'end': row['gen_end']\n                })\n                G[row['sample1']][row['sample2']]['total_cm'] += row['gen_seg_len']\n                G[row['sample1']][row['sample2']]['count'] += 1\n            else:\n                # Add new edge\n                G.add_edge(row['sample1'], row['sample2'], \n                          segments=[{\n                              'length': row['gen_seg_len'],\n                              'chrom': row['chrom'],\n                              'start': row['gen_start'],\n                              'end': row['gen_end']\n                          }],\n                          total_cm=row['gen_seg_len'],\n                          count=1)\n        \n        # Use community detection to find clusters that might share a common ancestor\n        communities = list(nx.community.greedy_modularity_communities(G))\n        \n        phantom_ancestors = []\n        \n        for i, community in enumerate(communities):\n            if len(community) < 3:\n                continue  # Need at least 3 individuals to infer a common ancestor\n                \n            # Check if all pairs share IBD\n            is_fully_connected = True\n            for ind1 in community:\n                for ind2 in community:\n                    if ind1 != ind2 and not G.has_edge(ind1, ind2):\n                        is_fully_connected = False\n                        break\n                if not is_fully_connected:\n                    break\n            \n            # If not fully connected, create a phantom ancestor\n            if not is_fully_connected:\n                # Calculate average IBD sharing within community\n                total_cm = 0\n                pair_count = 0\n                for ind1 in community:\n                    for ind2 in community:\n                        if ind1 < ind2 and G.has_edge(ind1, ind2):\n                            total_cm += G[ind1][ind2]['total_cm']\n                            pair_count += 1\n                \n                if pair_count > 0:\n                    avg_cm = total_cm / pair_count\n                    \n                    # Infer ancestor generation based on average sharing\n                    if avg_cm > 100:\n                        gen_distance = 1  # Parent\n                    elif avg_cm > 50:\n                        gen_distance = 2  # Grandparent\n                    else:\n                        gen_distance = 3  # Great-grandparent\n                    \n                    phantom_ancestors.append({\n                        'id': f\"phantom_{i}\",\n                        'descendants': list(community),\n                        'avg_sharing_cm': avg_cm,\n                        'generation_distance': gen_distance\n                    })\n        \n        return phantom_ancestors\n    \n    def predict_missing_segments(self, min_confidence=0.7):\n        \"\"\"\n        Predict IBD segments that might exist but weren't detected.\n        \n        Args:\n            min_confidence: Minimum confidence threshold for predictions\n            \n        Returns:\n            DataFrame of predicted missing segments\n        \"\"\"\n        # Group segments by pair\n        pair_segments = self.segments_df.groupby(['sample1', 'sample2'])\n        \n        predicted_segments = []\n        \n        for (sample1, sample2), group in pair_segments:\n            # Skip pairs with only one segment\n            if len(group) < 2:\n                continue\n                \n            # Count segments per chromosome\n            chrom_counts = group['chrom'].value_counts()\n            \n            # Identify chromosomes with missing segments\n            for chrom in range(1, 23):\n                if chrom not in chrom_counts.index:\n                    # This chromosome has no segments\n                    # Calculate probability of missing segment based on pair's other segments\n                    other_chroms_avg = group['gen_seg_len'].mean()\n                    \n                    # Simple confidence model: higher if pair has many segments\n                    confidence = 1 - (1 / (0.5 * len(group)))\n                    \n                    # Only include predictions with sufficient confidence\n                    if confidence >= min_confidence:\n                        predicted_segments.append({\n                            'sample1': sample1,\n                            'sample2': sample2,\n                            'chrom': chrom,\n                            'predicted_length': other_chroms_avg,\n                            'confidence': confidence\n                        })\n        \n        return pd.DataFrame(predicted_segments)\n\n# Apply missing data strategies to our simulated data\nhandler = MissingDataHandler(pedigree, segments_df)\n\n# 1. Identify potentially missing connections\nmissing_connections = handler.identify_missing_connections()\nprint(f\"Identified {len(missing_connections)} potentially missing connections\")\ndisplay(missing_connections.head(10))\n\n# 2. Infer phantom ancestors\nphantom_ancestors = handler.infer_phantom_ancestors()\nprint(f\"\\nInferred {len(phantom_ancestors)} phantom ancestors\")\nfor i, ancestor in enumerate(phantom_ancestors[:3]):  # Show first 3\n    print(f\"\\nPhantom ancestor {i+1}:\")\n    print(f\"  ID: {ancestor['id']}\")\n    print(f\"  Generation distance: {ancestor['generation_distance']}\")\n    print(f\"  Average sharing: {ancestor['avg_sharing_cm']:.2f} cM\")\n    print(f\"  Number of descendants: {len(ancestor['descendants'])}\")\n    print(f\"  Descendants: {', '.join(ancestor['descendants'][:5])}{'...' if len(ancestor['descendants']) > 5 else ''}\")\n\n# 3. Predict missing segments\npredicted_segments = handler.predict_missing_segments()\nprint(f\"\\nPredicted {len(predicted_segments)} potentially missing segments\")\ndisplay(predicted_segments.head(10))\n\n# Visualize predicted segments by chromosome\nplt.figure(figsize=(12, 6))\nchrom_counts = predicted_segments['chrom'].value_counts().sort_index()\nplt.bar(chrom_counts.index, chrom_counts.values)\nplt.xlabel('Chromosome')\nplt.ylabel('Number of Predicted Segments')\nplt.title('Distribution of Predicted Missing Segments by Chromosome')\nplt.xticks(range(1, 23))\nplt.grid(True, alpha=0.3)\nplt.show()\n\n# Visualize relationship between prediction confidence and predicted length\nplt.figure(figsize=(10, 6))\nplt.scatter(predicted_segments['confidence'], predicted_segments['predicted_length'], alpha=0.5)\nplt.xlabel('Prediction Confidence')\nplt.ylabel('Predicted Segment Length (cM)')\nplt.title('Relationship Between Prediction Confidence and Segment Length')\nplt.grid(True, alpha=0.3)\nplt.xlim(0.7, 1.0)\nplt.ylim(0, predicted_segments['predicted_length'].max() * 1.1)\nplt.show()"
   ],
   "metadata": {},
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## 4. Building Data Preprocessing Pipelines for Bonsai",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "### 4.1 Designing an End-to-End Preprocessing Pipeline\n\nNow that we've explored various filtering techniques and strategies for handling missing data, let's design a comprehensive preprocessing pipeline for Bonsai. This pipeline will integrate the different components we've developed and prepare IBD data for pedigree reconstruction.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "# Implement an end-to-end preprocessing pipeline for Bonsai\n\nclass BonsaiPreprocessingPipeline:\n    \"\"\"\n    End-to-end pipeline for preprocessing IBD data for Bonsai.\n    \n    This pipeline handles:\n    1. Data loading and validation\n    2. Quality control and filtering\n    3. Missing data handling\n    4. Data transformation for Bonsai input\n    \"\"\"\n    \n    def __init__(self, population_type='outbred'):\n        \"\"\"\n        Initialize the pipeline.\n        \n        Args:\n            population_type: Type of population ('outbred', 'endogamous', 'bottleneck', 'admixed')\n        \"\"\"\n        self.population_type = population_type\n        self.logs = []\n        self.original_segments = None\n        self.filtered_segments = None\n        self.predicted_segments = None\n        self.pedigree_data = None\n    \n    def log(self, message):\n        \"\"\"Add message to log and print it.\"\"\"\n        self.logs.append(message)\n        print(message)\n    \n    def load_data(self, segments_file, metadata_file=None):\n        \"\"\"\n        Load IBD segment data and optional metadata.\n        \n        Args:\n            segments_file: Path to the IBD segments file\n            metadata_file: Optional path to metadata file\n            \n        Returns:\n            self for method chaining\n        \"\"\"\n        self.log(f\"Loading IBD segments from {segments_file}\")\n        \n        # Load segments\n        if segments_file.endswith('.seg'):\n            # Load in the format from previous IBD detection steps\n            self.original_segments = pd.read_csv(segments_file, sep=\"\\t\", header=None)\n            self.original_segments.columns = [\"sample1\", \"sample2\", \"chrom\", \"phys_start\", \n                                              \"phys_end\", \"ibd_type\", \"gen_start\", \"gen_end\", \"gen_seg_len\"]\n        elif segments_file.endswith('.csv'):\n            # Assume standard CSV format\n            self.original_segments = pd.read_csv(segments_file)\n        else:\n            # Try to infer format\n            try:\n                self.original_segments = pd.read_csv(segments_file, sep=\"\\t\")\n            except:\n                self.original_segments = pd.read_csv(segments_file)\n        \n        # Load metadata if provided\n        self.metadata = None\n        if metadata_file:\n            self.log(f\"Loading metadata from {metadata_file}\")\n            try:\n                self.metadata = pd.read_csv(metadata_file)\n            except:\n                self.log(f\"Warning: Could not load metadata from {metadata_file}\")\n        \n        self.log(f\"Loaded {len(self.original_segments)} IBD segments involving \"\n                f\"{len(set(self.original_segments['sample1']).union(set(self.original_segments['sample2'])))} individuals\")\n        \n        # Make a copy for filtering\n        self.filtered_segments = self.original_segments.copy()\n        \n        return self\n    \n    def validate_data(self):\n        \"\"\"\n        Validate the loaded data for required columns and formats.\n        \n        Returns:\n            self for method chaining\n        \"\"\"\n        self.log(\"Validating data formats...\")\n        \n        # Check required columns\n        required_columns = [\n            \"sample1\", \"sample2\", \"chrom\", \"gen_seg_len\"\n        ]\n        \n        missing_columns = [col for col in required_columns if col not in self.original_segments.columns]\n        if missing_columns:\n            self.log(f\"Warning: Missing required columns: {', '.join(missing_columns)}\")\n            \n            # Try to infer or rename columns\n            if \"gen_seg_len\" not in self.original_segments.columns and \"cM\" in self.original_segments.columns:\n                self.original_segments[\"gen_seg_len\"] = self.original_segments[\"cM\"]\n                self.log(\"Inferred 'gen_seg_len' from 'cM' column\")\n        \n        # Check data types\n        try:\n            self.original_segments[\"gen_seg_len\"] = self.original_segments[\"gen_seg_len\"].astype(float)\n            self.original_segments[\"chrom\"] = self.original_segments[\"chrom\"].astype(int)\n        except:\n            self.log(\"Warning: Could not convert segment length or chromosome to numeric values\")\n        \n        # Check for duplicates\n        duplicates = self.original_segments.duplicated().sum()\n        if duplicates > 0:\n            self.log(f\"Warning: Found {duplicates} duplicate rows\")\n        \n        # Standardize individual IDs\n        sample_cols = [\"sample1\", \"sample2\"]\n        for col in sample_cols:\n            if col in self.original_segments.columns:\n                self.original_segments[col] = self.original_segments[col].astype(str)\n        \n        # Ensure sample1 < sample2 for consistency\n        swap_mask = self.original_segments[\"sample1\"] > self.original_segments[\"sample2\"]\n        if swap_mask.sum() > 0:\n            self.log(f\"Standardizing {swap_mask.sum()} rows to ensure sample1 < sample2\")\n            self.original_segments.loc[swap_mask, sample_cols] = self.original_segments.loc[swap_mask, sample_cols[::-1]].values\n        \n        # Update filtered segments\n        self.filtered_segments = self.original_segments.copy()\n        \n        return self\n    \n    def apply_qc_and_filtering(self, min_length=7, min_confidence=None, consistency_filter=True):\n        \"\"\"\n        Apply quality control and filtering steps.\n        \n        Args:\n            min_length: Minimum segment length (cM)\n            min_confidence: Minimum confidence score (if available)\n            consistency_filter: Whether to apply consistency-based filtering\n            \n        Returns:\n            self for method chaining\n        \"\"\"\n        self.log(f\"Applying quality control and filtering (population type: {self.population_type})...\")\n        \n        # Create a filter based on population type\n        if self.population_type == 'endogamous':\n            filter_obj = PopulationAwareFilter(self.filtered_segments, 'endogamous')\n            self.filtered_segments = filter_obj.apply_population_specific_filters().get_filtered_segments()\n        elif self.population_type == 'bottleneck':\n            filter_obj = PopulationAwareFilter(self.filtered_segments, 'bottleneck')\n            self.filtered_segments = filter_obj.apply_population_specific_filters().get_filtered_segments()\n        elif self.population_type == 'admixed':\n            filter_obj = PopulationAwareFilter(self.filtered_segments, 'admixed')\n            self.filtered_segments = filter_obj.apply_population_specific_filters().get_filtered_segments()\n        else:  # outbred\n            # Apply standard filtering for outbred populations\n            filter_obj = IBDSegmentFilter(self.filtered_segments)\n            self.filtered_segments = filter_obj.filter_by_length(min_length=min_length).get_filtered_segments()\n        \n        # Apply confidence filtering if specified\n        if min_confidence is not None and 'confidence' in self.filtered_segments.columns:\n            self.filtered_segments = self.filtered_segments[self.filtered_segments['confidence'] >= min_confidence]\n            self.log(f\"Applied confidence threshold of {min_confidence}\")\n        \n        # Apply consistency filtering if specified\n        if consistency_filter:\n            consistency_obj = ConsistencyFilter(self.filtered_segments)\n            self.filtered_segments = consistency_obj.filter_by_segment_count(min_segments=2).get_filtered_segments()\n            self.log(\"Applied consistency filtering (min 2 segments per pair)\")\n        \n        self.log(f\"After filtering: {len(self.filtered_segments)} segments remaining \"\n                f\"({len(self.filtered_segments)/len(self.original_segments)*100:.1f}% of original)\")\n        \n        return self\n    \n    def handle_missing_data(self, predict_segments=True, infer_ancestors=True):\n        \"\"\"\n        Apply strategies to handle missing data.\n        \n        Args:\n            predict_segments: Whether to predict potentially missing segments\n            infer_ancestors: Whether to infer phantom ancestors\n            \n        Returns:\n            self for method chaining\n        \"\"\"\n        self.log(\"Handling missing data...\")\n        \n        # Create a simple pedigree graph from the filtered segments\n        G = nx.Graph()\n        \n        # Add all individuals as nodes\n        individuals = set()\n        for _, row in self.filtered_segments.iterrows():\n            individuals.add(row['sample1'])\n            individuals.add(row['sample2'])\n        \n        for ind in individuals:\n            G.add_node(ind)\n        \n        # Add edges for IBD connections\n        for _, row in self.filtered_segments.iterrows():\n            G.add_edge(row['sample1'], row['sample2'], \n                      length=row['gen_seg_len'], \n                      count=1)\n        \n        # Create a handler\n        handler = MissingDataHandler(G, self.filtered_segments)\n        \n        # Predict missing segments if specified\n        if predict_segments:\n            self.predicted_segments = handler.predict_missing_segments(min_confidence=0.8)\n            self.log(f\"Predicted {len(self.predicted_segments)} potentially missing segments\")\n        \n        # Infer phantom ancestors if specified\n        if infer_ancestors:\n            self.phantom_ancestors = handler.infer_phantom_ancestors()\n            self.log(f\"Inferred {len(self.phantom_ancestors)} phantom ancestors\")\n        \n        return self\n    \n    def prepare_bonsai_input(self, output_file=None, include_predicted=True):\n        \"\"\"\n        Prepare data in the format required by Bonsai.\n        \n        Args:\n            output_file: Optional path to save the Bonsai input file\n            include_predicted: Whether to include predicted segments\n            \n        Returns:\n            Dict with Bonsai input data\n        \"\"\"\n        self.log(\"Preparing Bonsai input data...\")\n        \n        # Combine filtered segments with predicted segments if specified\n        segments_for_bonsai = self.filtered_segments.copy()\n        \n        if include_predicted and self.predicted_segments is not None and len(self.predicted_segments) > 0:\n            # Convert predicted segments to the same format\n            predicted_for_bonsai = self.predicted_segments.copy()\n            predicted_for_bonsai['gen_start'] = 0  # Placeholder\n            predicted_for_bonsai['gen_end'] = predicted_for_bonsai['predicted_length']\n            predicted_for_bonsai['phys_start'] = 0  # Placeholder\n            predicted_for_bonsai['phys_end'] = 0  # Placeholder\n            predicted_for_bonsai['ibd_type'] = 'IBD1'  # Assuming IBD1\n            predicted_for_bonsai['is_predicted'] = True\n            \n            # Rename columns to match\n            predicted_for_bonsai = predicted_for_bonsai.rename(\n                columns={'predicted_length': 'gen_seg_len'}\n            )\n            \n            # Add required columns\n            required_cols = set(segments_for_bonsai.columns) - set(predicted_for_bonsai.columns)\n            for col in required_cols:\n                if col != 'is_predicted':\n                    predicted_for_bonsai[col] = 0  # Placeholder\n            \n            # Mark original segments\n            segments_for_bonsai['is_predicted'] = False\n            \n            # Combine\n            segments_for_bonsai = pd.concat([segments_for_bonsai, predicted_for_bonsai[segments_for_bonsai.columns]])\n            \n            self.log(f\"Added {len(predicted_for_bonsai)} predicted segments\")\n        \n        # Create a mapping of individuals to metadata if available\n        individuals = {}\n        if self.metadata is not None:\n            for ind in set(segments_for_bonsai['sample1']).union(set(segments_for_bonsai['sample2'])):\n                # Try to find this individual in metadata\n                ind_metadata = self.metadata[self.metadata['id'] == ind]\n                if len(ind_metadata) > 0:\n                    individuals[ind] = ind_metadata.iloc[0].to_dict()\n                else:\n                    individuals[ind] = {'id': ind}\n        else:\n            # Create basic entries for each individual\n            for ind in set(segments_for_bonsai['sample1']).union(set(segments_for_bonsai['sample2'])):\n                individuals[ind] = {'id': ind}\n        \n        # Prepare Bonsai input format\n        bonsai_input = {\n            'individuals': individuals,\n            'segments': segments_for_bonsai.to_dict(orient='records'),\n            'phantom_ancestors': self.phantom_ancestors if hasattr(self, 'phantom_ancestors') else [],\n            'metadata': {\n                'pipeline_version': '1.0.0',\n                'population_type': self.population_type,\n                'filtering_info': {\n                    'original_count': len(self.original_segments),\n                    'filtered_count': len(self.filtered_segments),\n                    'predicted_count': len(self.predicted_segments) if hasattr(self, 'predicted_segments') else 0\n                }\n            }\n        }\n        \n        # Save to file if specified\n        if output_file:\n            try:\n                with open(output_file, 'w') as f:\n                    json.dump(bonsai_input, f, indent=2)\n                self.log(f\"Saved Bonsai input to {output_file}\")\n            except Exception as e:\n                self.log(f\"Error saving to {output_file}: {e}\")\n        \n        self.pedigree_data = bonsai_input\n        return bonsai_input\n    \n    def run_pipeline(self, segments_file, output_file=None, metadata_file=None, min_length=7):\n        \"\"\"\n        Run the complete pipeline in one go.\n        \n        Args:\n            segments_file: Path to the IBD segments file\n            output_file: Optional path to save the Bonsai input file\n            metadata_file: Optional path to metadata file\n            min_length: Minimum segment length (cM)\n            \n        Returns:\n            Dict with Bonsai input data\n        \"\"\"\n        return (self\n                .load_data(segments_file, metadata_file)\n                .validate_data()\n                .apply_qc_and_filtering(min_length=min_length)\n                .handle_missing_data()\n                .prepare_bonsai_input(output_file))\n\n# Demonstrate the complete pipeline\npipeline = BonsaiPreprocessingPipeline(population_type='outbred')\nbonsai_input = pipeline.run_pipeline(\n    segments_file=os.path.join(data_directory, \"class_data/ped_sim_run2.seg\"),\n    min_length=7\n)\n\n# Display summary statistics\nindividuals = set(pipeline.filtered_segments['sample1']).union(set(pipeline.filtered_segments['sample2']))\nprint(f\"\\nSummary:\")\nprint(f\"- {len(individuals)} individuals\")\nprint(f\"- {len(pipeline.filtered_segments)} filtered segments\")\nprint(f\"- {len(pipeline.predicted_segments) if pipeline.predicted_segments is not None else 0} predicted segments\")\nprint(f\"- {len(pipeline.phantom_ancestors) if hasattr(pipeline, 'phantom_ancestors') else 0} inferred phantom ancestors\")\n\n# Plot the distribution of segments by chromosome\nplt.figure(figsize=(12, 6))\nchrom_counts = pipeline.filtered_segments['chrom'].value_counts().sort_index()\nplt.bar(chrom_counts.index, chrom_counts.values)\nplt.xlabel('Chromosome')\nplt.ylabel('Number of Segments')\nplt.title('Distribution of IBD Segments by Chromosome')\nplt.xticks(range(1, 23))\nplt.grid(True, alpha=0.3)\nplt.show()\n\n# Plot the distribution of segment lengths\nplt.figure(figsize=(12, 6))\nplt.hist(pipeline.filtered_segments['gen_seg_len'], bins=30, alpha=0.7)\nplt.axvline(x=7, color='red', linestyle='--', label='7 cM threshold')\nplt.xlabel('Segment Length (cM)')\nplt.ylabel('Count')\nplt.title('Distribution of IBD Segment Lengths')\nplt.legend()\nplt.grid(True, alpha=0.3)\nplt.xlim(0, 50)\nplt.show()"
   ],
   "metadata": {},
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## 5. Exercises",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "Complete the following exercises to test your understanding of data quality and preprocessing for Bonsai:\n\n### Exercise 1: Modify the Population-Aware Filter\nExtend the `PopulationAwareFilter` class to handle a new population type called \"island\" that represents an isolated population with high levels of endogamy but also some distinctive features.\n\n### Exercise 2: Evaluate Filter Performance\nDesign a function that evaluates the performance of different filtering strategies by comparing the filtered segments to a \"ground truth\" set of known relationships.\n\n### Exercise 3: Create a Custom Preprocessing Pipeline\nCreate a custom preprocessing pipeline for a specific use case (e.g., ancient DNA analysis, multi-ethnic dataset, or a specific family history question).",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "# Example solution for Exercise 1: Extend the PopulationAwareFilter for \"island\" populations\n\nclass ExtendedPopulationFilter(PopulationAwareFilter):\n    \"\"\"Extended filter that handles additional population types including island populations.\"\"\"\n    \n    def apply_population_specific_filters(self):\n        \"\"\"Apply filters appropriate for the specified population.\"\"\"\n        n_before = len(self.filtered_segments)\n        \n        if self.population_type == 'island':\n            # Island populations tend to have:\n            # 1. High background IBD due to isolation\n            # 2. Distinct patterns of sharing across specific chromosomes due to founder effects\n            # 3. Need for careful consistency checks to distinguish true from background IBD\n            \n            # Step 1: Apply a higher length threshold to reduce false positives from background IBD\n            self.filter_by_length(min_length=12)  # More stringent than endogamous\n            \n            # Step 2: Identify \"common segments\" that appear frequently in the population\n            # (in a real scenario, these would be pre-identified based on population-wide analysis)\n            # For this example, we'll simulate by counting the most frequent segment regions\n            if len(self.filtered_segments) > 0:\n                # Group segments by chromosome and divide into regions\n                region_counts = {}\n                bin_size = 10  # in cM\n                \n                for _, row in self.filtered_segments.iterrows():\n                    chrom = row['chrom']\n                    start_bin = int(row['gen_start'] / bin_size)\n                    end_bin = int(row['gen_end'] / bin_size)\n                    \n                    # Count each affected bin\n                    for bin_idx in range(start_bin, end_bin + 1):\n                        region_key = f\"{chrom}_{bin_idx}\"\n                        if region_key in region_counts:\n                            region_counts[region_key] += 1\n                        else:\n                            region_counts[region_key] = 1\n                \n                # Identify common regions (top 10% most frequent)\n                sorted_regions = sorted(region_counts.items(), key=lambda x: x[1], reverse=True)\n                common_regions = []\n                if len(sorted_regions) > 10:\n                    threshold_idx = max(1, int(len(sorted_regions) * 0.1))\n                    common_regions = [r[0] for r in sorted_regions[:threshold_idx]]\n                \n                # Filter out segments that are entirely in common regions\n                if common_regions:\n                    filtered_idx = []\n                    for i, row in self.filtered_segments.iterrows():\n                        chrom = row['chrom']\n                        start_bin = int(row['gen_start'] / bin_size)\n                        end_bin = int(row['gen_end'] / bin_size)\n                        \n                        # Check if all bins are in common regions\n                        all_common = True\n                        for bin_idx in range(start_bin, end_bin + 1):\n                            region_key = f\"{chrom}_{bin_idx}\"\n                            if region_key not in common_regions:\n                                all_common = False\n                                break\n                        \n                        # Keep if not all bins are common\n                        if not all_common:\n                            filtered_idx.append(i)\n                    \n                    # Apply the filter\n                    self.filtered_segments = self.filtered_segments.loc[filtered_idx]\n            \n            # Step 3: Apply consistency filtering - pairs should have multiple segments\n            # Count segments per pair\n            pair_counts = self.filtered_segments.groupby(['sample1', 'sample2']).size().reset_index(name='count')\n            valid_pairs = pair_counts[pair_counts['count'] >= 3]  # Higher than standard\n            \n            # Filter segments\n            valid_pairs_tuples = list(zip(valid_pairs['sample1'], valid_pairs['sample2']))\n            pair_tuples = list(zip(self.filtered_segments['sample1'], self.filtered_segments['sample2']))\n            \n            self.filtered_segments = self.filtered_segments[\n                [p in valid_pairs_tuples for p in pair_tuples]\n            ]\n            \n        else:\n            # Use the parent class implementation for other population types\n            super().apply_population_specific_filters()\n        \n        n_after = len(self.filtered_segments)\n        self.filter_history.append({\n            'filter': f'population_{self.population_type}',\n            'params': {},\n            'removed': n_before - n_after\n        })\n        \n        return self\n\n# Test the extended filter with an island population\nnp.random.seed(42)\n# Create a simulated island population - extreme version of endogamous\nisland_df = simulate_population_effects(seg_df, 'endogamous', num_individuals=30)\n# Add some common segments that appear in many pairs (simulating founder effect)\ncommon_segments = []\nfor _ in range(50):  # Add 50 common segments\n    # Randomly select 30-60% of pairs to have this common segment\n    all_pairs = [(i, j) for i in range(30) for j in range(i+1, 30)]\n    num_pairs = int(len(all_pairs) * np.random.uniform(0.3, 0.6))\n    pairs = np.random.choice(range(len(all_pairs)), size=num_pairs, replace=False)\n    \n    # Create a segment on the same region\n    chrom = np.random.randint(1, 23)\n    start = np.random.uniform(0, 200)\n    length = np.random.uniform(15, 30)  # Longer segments\n    \n    for idx in pairs:\n        i, j = all_pairs[idx]\n        common_segments.append({\n            'sample1': f\"ind_{i}\",\n            'sample2': f\"ind_{j}\",\n            'chrom': chrom,\n            'phys_start': np.random.randint(1000000, 200000000),\n            'phys_end': np.random.randint(200000000, 300000000),\n            'ibd_type': 'IBD1',\n            'gen_start': start,\n            'gen_end': start + length,\n            'gen_seg_len': length\n        })\n\n# Add common segments to the dataset\nisland_df = pd.concat([island_df, pd.DataFrame(common_segments)], ignore_index=True)\n\n# Apply both standard and extended filter for comparison\nstandard_filter = PopulationAwareFilter(island_df.copy(), 'endogamous')\nextended_filter = ExtendedPopulationFilter(island_df.copy(), 'island')\n\n# Apply filters\nstandard_filtered = standard_filter.apply_population_specific_filters().get_filtered_segments()\nextended_filtered = extended_filter.apply_population_specific_filters().get_filtered_segments()\n\n# Compare results\nprint(f\"Original island population segments: {len(island_df)}\")\nprint(f\"After standard endogamous filtering: {len(standard_filtered)} ({len(standard_filtered)/len(island_df)*100:.1f}%)\")\nprint(f\"After island-specific filtering: {len(extended_filtered)} ({len(extended_filtered)/len(island_df)*100:.1f}%)\")\n\n# Visualize the distributions\nplt.figure(figsize=(15, 5))\n\nplt.subplot(1, 3, 1)\nplt.hist(island_df['gen_seg_len'], bins=30, alpha=0.7)\nplt.xlabel('Segment Length (cM)')\nplt.ylabel('Count')\nplt.title('Original Island Population')\nplt.grid(True, alpha=0.3)\nplt.xlim(0, 50)\n\nplt.subplot(1, 3, 2)\nplt.hist(standard_filtered['gen_seg_len'], bins=30, alpha=0.7)\nplt.xlabel('Segment Length (cM)')\nplt.ylabel('Count')\nplt.title('Standard Endogamous Filtering')\nplt.grid(True, alpha=0.3)\nplt.xlim(0, 50)\n\nplt.subplot(1, 3, 3)\nplt.hist(extended_filtered['gen_seg_len'], bins=30, alpha=0.7)\nplt.xlabel('Segment Length (cM)')\nplt.ylabel('Count')\nplt.title('Island-Specific Filtering')\nplt.grid(True, alpha=0.3)\nplt.xlim(0, 50)\n\nplt.tight_layout()\nplt.show()\n\n# Visualize the distribution of segments by chromosome\nplt.figure(figsize=(15, 5))\n\nplt.subplot(1, 3, 1)\nisland_chrom_counts = island_df['chrom'].value_counts().sort_index()\nplt.bar(island_chrom_counts.index, island_chrom_counts.values)\nplt.xlabel('Chromosome')\nplt.ylabel('Number of Segments')\nplt.title('Original Island Population')\nplt.xticks(range(1, 23))\nplt.grid(True, alpha=0.3)\n\nplt.subplot(1, 3, 2)\nstandard_chrom_counts = standard_filtered['chrom'].value_counts().sort_index()\nplt.bar(standard_chrom_counts.index, standard_chrom_counts.values)\nplt.xlabel('Chromosome')\nplt.ylabel('Number of Segments')\nplt.title('Standard Endogamous Filtering')\nplt.xticks(range(1, 23))\nplt.grid(True, alpha=0.3)\n\nplt.subplot(1, 3, 3)\nextended_chrom_counts = extended_filtered['chrom'].value_counts().sort_index()\nplt.bar(extended_chrom_counts.index, extended_chrom_counts.values)\nplt.xlabel('Chromosome')\nplt.ylabel('Number of Segments')\nplt.title('Island-Specific Filtering')\nplt.xticks(range(1, 23))\nplt.grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()"
   ],
   "metadata": {},
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Example solution for Exercise 2: Evaluate filter performance against ground truth\n\ndef evaluate_filter_performance(true_relationships, filtered_segments, relationship_thresholds=None):\n    \"\"\"\n    Evaluate the performance of filtering strategies by comparing to known relationships.\n    \n    Args:\n        true_relationships: DataFrame with columns 'sample1', 'sample2', 'relationship_degree'\n        filtered_segments: DataFrame of filtered IBD segments\n        relationship_thresholds: Dict mapping relationship degrees to min total cM thresholds\n        \n    Returns:\n        Dict with performance metrics\n    \"\"\"\n    if relationship_thresholds is None:\n        # Default thresholds: minimum total cM expected for each relationship degree\n        relationship_thresholds = {\n            1: 1500,  # Parent-child: ~1500+ cM\n            2: 500,   # Sibling, grandparent: ~500+ cM\n            3: 200,   # Aunt/uncle, half-sibling: ~200+ cM\n            4: 90,    # First cousin: ~90+ cM\n            5: 45,    # First cousin once removed: ~45+ cM\n            6: 20     # Second cousin: ~20+ cM\n        }\n    \n    # Calculate total sharing between each pair in the filtered segments\n    pair_sharing = filtered_segments.groupby(['sample1', 'sample2'])['gen_seg_len'].sum().reset_index()\n    \n    # Create a lookup dictionary for quick access\n    sharing_dict = {(row['sample1'], row['sample2']): row['gen_seg_len'] for _, row in pair_sharing.iterrows()}\n    \n    # Initialize counters for evaluation\n    true_positives = 0\n    false_positives = 0\n    false_negatives = 0\n    true_negatives_unknown = 0  # We can't know true negatives without all possible pairs\n    \n    # Check each true relationship\n    for _, rel in true_relationships.iterrows():\n        sample1, sample2 = rel['sample1'], rel['sample2']\n        degree = rel['relationship_degree']\n        threshold = relationship_thresholds.get(degree, 0)\n        \n        # Standardize pair order for lookup\n        if sample1 > sample2:\n            sample1, sample2 = sample2, sample1\n        \n        # Check if this pair has sufficient sharing\n        pair_key = (sample1, sample2)\n        if pair_key in sharing_dict:\n            total_sharing = sharing_dict[pair_key]\n            if total_sharing >= threshold:\n                # Correctly identified relationship\n                true_positives += 1\n            else:\n                # Relationship detected but below threshold\n                false_negatives += 1\n        else:\n            # Relationship not detected at all\n            false_negatives += 1\n    \n    # Check for false positives\n    for pair_key, total_sharing in sharing_dict.items():\n        sample1, sample2 = pair_key\n        # Find this pair in true relationships\n        rel_row = true_relationships[\n            ((true_relationships['sample1'] == sample1) & (true_relationships['sample2'] == sample2)) |\n            ((true_relationships['sample1'] == sample2) & (true_relationships['sample2'] == sample1))\n        ]\n        \n        if len(rel_row) == 0:\n            # This pair isn't in the true relationships - check if it exceeds any threshold\n            min_threshold = min(relationship_thresholds.values())\n            if total_sharing >= min_threshold:\n                false_positives += 1\n            else:\n                true_negatives_unknown += 1\n    \n    # Calculate metrics\n    total_predictions = true_positives + false_positives + true_negatives_unknown\n    total_true = true_positives + false_negatives\n    \n    precision = true_positives / (true_positives + false_positives) if (true_positives + false_positives) > 0 else 0\n    recall = true_positives / total_true if total_true > 0 else 0\n    f1_score = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0\n    \n    # Prepare results\n    results = {\n        'true_positives': true_positives,\n        'false_positives': false_positives,\n        'false_negatives': false_negatives,\n        'precision': precision,\n        'recall': recall,\n        'f1_score': f1_score\n    }\n    \n    return results\n\n# Create a set of true relationships for evaluation\ndef generate_true_relationships(pedigree):\n    \"\"\"Generate true relationships from a pedigree graph.\"\"\"\n    true_relationships = []\n    \n    # Convert to undirected graph for path finding\n    undirected = pedigree.to_undirected()\n    \n    # Find paths between all pairs\n    for source in pedigree.nodes():\n        for target in pedigree.nodes():\n            if source >= target:\n                continue  # Skip self and duplicates\n                \n            # Find shortest path between these individuals\n            try:\n                path = nx.shortest_path(undirected, source, target)\n                # Path length - 1 = relationship degree\n                degree = len(path) - 1\n                \n                if degree <= 6:  # Only consider up to 6th degree relationships\n                    true_relationships.append({\n                        'sample1': f\"ind_{source}\",\n                        'sample2': f\"ind_{target}\",\n                        'relationship_degree': degree,\n                        'path': path\n                    })\n            except nx.NetworkXNoPath:\n                # No path between these individuals\n                pass\n    \n    return pd.DataFrame(true_relationships)\n\n# Generate true relationships from our simulated pedigree\ntrue_relationships = generate_true_relationships(pedigree)\nprint(f\"Generated {len(true_relationships)} true relationships\")\n\n# Simulate segments for these relationships\nsegments_df_for_eval, _ = simulate_missing_data_impact(pedigree, missing_ids, num_segments_per_relationship=20)\n\n# Apply different filtering strategies\nfilters_to_evaluate = {\n    'minimal': IBDSegmentFilter(segments_df_for_eval.copy()).filter_by_length(min_length=3).get_filtered_segments(),\n    'standard': IBDSegmentFilter(segments_df_for_eval.copy()).filter_by_length(min_length=7).get_filtered_segments(),\n    'strict': IBDSegmentFilter(segments_df_for_eval.copy()).filter_by_length(min_length=15).get_filtered_segments(),\n    'consistency': ConsistencyFilter(segments_df_for_eval.copy()).filter_by_segment_count(min_segments=2).get_filtered_segments(),\n    'combined': ConsistencyFilter(segments_df_for_eval.copy()).filter_by_length(min_length=7).filter_by_segment_count(min_segments=2).get_filtered_segments()\n}\n\n# Evaluate each filter\nresults = {}\nfor filter_name, filtered_segments in filters_to_evaluate.items():\n    results[filter_name] = evaluate_filter_performance(true_relationships, filtered_segments)\n    \n# Display results\nresults_df = pd.DataFrame.from_dict(results, orient='index')\nprint(\"Filter Performance Metrics:\")\ndisplay(results_df)\n\n# Visualize precision, recall, and F1 score\nplt.figure(figsize=(12, 6))\nmetrics = ['precision', 'recall', 'f1_score']\nx = np.arange(len(filters_to_evaluate))\nwidth = 0.25\n\nfor i, metric in enumerate(metrics):\n    values = [results[filter_name][metric] for filter_name in filters_to_evaluate]\n    plt.bar(x + i*width - width, values, width, label=metric)\n\nplt.xlabel('Filtering Strategy')\nplt.ylabel('Score')\nplt.title('Performance Metrics by Filtering Strategy')\nplt.xticks(x, list(filters_to_evaluate.keys()), rotation=45)\nplt.legend()\nplt.grid(True, alpha=0.3)\nplt.tight_layout()\nplt.show()\n\n# Analyze performance by relationship degree\ndef evaluate_by_degree(true_relationships, filtered_segments, relationship_thresholds=None):\n    \"\"\"Evaluate filter performance broken down by relationship degree.\"\"\"\n    if relationship_thresholds is None:\n        # Default thresholds (as above)\n        relationship_thresholds = {\n            1: 1500, 2: 500, 3: 200, 4: 90, 5: 45, 6: 20\n        }\n    \n    # Calculate total sharing between each pair\n    pair_sharing = filtered_segments.groupby(['sample1', 'sample2'])['gen_seg_len'].sum().reset_index()\n    sharing_dict = {(row['sample1'], row['sample2']): row['gen_seg_len'] for _, row in pair_sharing.iterrows()}\n    \n    # Evaluate each degree separately\n    degree_results = {}\n    for degree in range(1, 7):\n        # Filter relationships to this degree\n        degree_rels = true_relationships[true_relationships['relationship_degree'] == degree]\n        \n        if len(degree_rels) == 0:\n            continue  # Skip if no relationships of this degree\n            \n        threshold = relationship_thresholds[degree]\n        \n        # Count true positives and false negatives\n        tp = 0\n        fn = 0\n        \n        for _, rel in degree_rels.iterrows():\n            sample1, sample2 = rel['sample1'], rel['sample2']\n            \n            # Standardize pair order\n            if sample1 > sample2:\n                sample1, sample2 = sample2, sample1\n            \n            # Check if this pair has sufficient sharing\n            pair_key = (sample1, sample2)\n            if pair_key in sharing_dict:\n                total_sharing = sharing_dict[pair_key]\n                if total_sharing >= threshold:\n                    tp += 1\n                else:\n                    fn += 1\n            else:\n                fn += 1\n        \n        # Calculate recall for this degree\n        recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n        \n        degree_results[degree] = {\n            'true_positives': tp,\n            'false_negatives': fn,\n            'recall': recall,\n            'total_relationships': len(degree_rels)\n        }\n    \n    return degree_results\n\n# Evaluate breakdown by degree for each filter\ndegree_results = {}\nfor filter_name, filtered_segments in filters_to_evaluate.items():\n    degree_results[filter_name] = evaluate_by_degree(true_relationships, filtered_segments)\n\n# Visualize recall by relationship degree\nplt.figure(figsize=(12, 6))\nx = np.arange(1, 7)\nwidth = 0.15\ni = 0\n\nfor filter_name, results in degree_results.items():\n    recall_values = [results.get(degree, {}).get('recall', 0) for degree in range(1, 7)]\n    plt.bar(x + i*width - 2*width, recall_values, width, label=filter_name)\n    i += 1\n\nplt.xlabel('Relationship Degree')\nplt.ylabel('Recall')\nplt.title('Recall by Relationship Degree and Filtering Strategy')\nplt.xticks(x)\nplt.legend()\nplt.grid(True, alpha=0.3)\nplt.tight_layout()\nplt.show()"
   ],
   "metadata": {},
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## 6. Summary and Next Steps",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "In this lab, we explored data quality issues and preprocessing techniques for Bonsai. Let's summarize what we've learned:\n\n### Key Concepts Covered\n\n1. **Common Data Quality Issues**\n   - IBD detection errors (false positives, false negatives, boundary errors)\n   - Phasing and genotyping errors\n   - Population-specific challenges (endogamy, bottlenecks, admixture)\n\n2. **Filtering Techniques**\n   - Length-based filtering\n   - Confidence-based filtering\n   - Consistency-based filtering\n   - Population-aware filtering\n\n3. **Handling Missing Data**\n   - Understanding the impact of missing individuals\n   - Predicting missing segments\n   - Inferring phantom ancestors\n\n4. **Building Preprocessing Pipelines**\n   - End-to-end data processing\n   - Data validation and transformation\n   - Integration of multiple filtering strategies\n\n### Next Steps\n\n1. **Advanced Pipeline Customization**\n   - Create domain-specific preprocessing pipelines\n   - Integrate demographic and historical information\n\n2. **Performance Analysis**\n   - Conduct systematic evaluations of filtering strategies\n   - Benchmark against known pedigrees\n   - Measure impact on Bonsai accuracy\n\n3. **Further Learning**\n   - Explore specialized techniques for challenging populations\n   - Study phasing improvements and their impact on IBD detection\n   - Investigate machine learning approaches for segment filtering\n\nIn the next lab, we'll explore how to use Bonsai for multi-sample relationship inference and examine strategies for building larger pedigrees from IBD data.",
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}