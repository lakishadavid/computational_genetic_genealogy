{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from collections import Counter\n",
    "import logging\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import subprocess\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from IPython.display import display, HTML\n",
    "import IPython\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "from dotenv import load_dotenv\n",
    "import numpy as np\n",
    "from sklearn.metrics import precision_recall_curve, average_precision_score, roc_curve, auc\n",
    "from intervaltree import IntervalTree\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_comp_gen_dir():\n",
    "    \"\"\"Find the computational_genetic_genealogy directory by searching up from current directory.\"\"\"\n",
    "    current = Path.cwd()\n",
    "    \n",
    "    # Search up through parent directories\n",
    "    while current != current.parent:\n",
    "        # Check if target directory exists in current path\n",
    "        target = current / 'computational_genetic_genealogy'\n",
    "        if target.is_dir():\n",
    "            return target\n",
    "        # Move up one directory\n",
    "        current = current.parent\n",
    "    \n",
    "    raise FileNotFoundError(\"Could not find computational_genetic_genealogy directory\")\n",
    "\n",
    "def load_env_file():\n",
    "    \"\"\"Find and load the .env file from the computational_genetic_genealogy directory.\"\"\"\n",
    "    try:\n",
    "        # Find the computational_genetic_genealogy directory\n",
    "        comp_gen_dir = find_comp_gen_dir()\n",
    "        \n",
    "        # Look for .env file\n",
    "        env_path = comp_gen_dir / '.env'\n",
    "        if not env_path.exists():\n",
    "            print(f\"Warning: No .env file found in {comp_gen_dir}\")\n",
    "            return None\n",
    "        \n",
    "        # Load the .env file\n",
    "        load_dotenv(env_path, override=True)\n",
    "        print(f\"Loaded environment variables from: {env_path}\")\n",
    "        return env_path\n",
    "        \n",
    "    except FileNotFoundError as e:\n",
    "        print(f\"Error: {e}\")\n",
    "        return None\n",
    "\n",
    "# Use the function\n",
    "env_path = load_env_file()\n",
    "\n",
    "working_directory = os.getenv('PROJECT_WORKING_DIR', default=None)\n",
    "data_directory = os.getenv('PROJECT_DATA_DIR', default=None)\n",
    "references_directory = os.getenv('PROJECT_REFERENCES_DIR', default=None)\n",
    "results_directory = os.getenv('PROJECT_RESULTS_DIR', default=None)\n",
    "utils_directory = os.getenv('PROJECT_UTILS_DIR', default=None)\n",
    "\n",
    "os.environ[\"WORKING_DIRECTORY\"] = working_directory\n",
    "os.environ[\"DATA_DIRECTORY\"] = data_directory\n",
    "os.environ[\"REFERENCES_DIRECTORY\"] = references_directory\n",
    "os.environ[\"RESULTS_DIRECTORY\"] = results_directory\n",
    "os.environ[\"UTILS_DIRECTORY\"] = utils_directory\n",
    "\n",
    "print(f\"Working Directory: {working_directory}\")\n",
    "print(f\"Data Directory: {data_directory}\")\n",
    "print(f\"References Directory: {references_directory}\")\n",
    "print(f\"Results Directory: {results_directory}\")\n",
    "print(f\"Utils Directory: {utils_directory}\")\n",
    "\n",
    "os.chdir(working_directory)\n",
    "print(f\"The current directory is {os.getcwd()}\")"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": "def configure_logging(log_filename, log_file_debug_level=\"INFO\", console_debug_level=\"INFO\"):\n    \"\"\"\n    Configure logging for both file and console handlers.\n\n    Args:\n        log_filename (str): Path to the log file where logs will be written.\n        log_file_debug_level (str): Logging level for the file handler.\n        console_debug_level (str): Logging level for the console handler.\n    \"\"\"\n    # Create a root logger\n    logger = logging.getLogger()\n    logger.setLevel(logging.DEBUG)  # Capture all messages at the root level\n\n    # Convert level names to numeric levels\n    file_level = getattr(logging, log_file_debug_level.upper(), logging.INFO)\n    console_level = getattr(logging, console_debug_level.upper(), logging.INFO)\n\n    # File handler: Logs messages at file_level and above to the file\n    file_handler = logging.FileHandler(log_filename)\n    file_handler.setLevel(file_level)\n    file_formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')\n    file_handler.setFormatter(file_formatter)\n\n    # Console handler: Logs messages at console_level and above to the console\n    console_handler = logging.StreamHandler(sys.stdout)\n    console_handler.setLevel(console_level)\n    console_formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')\n    console_handler.setFormatter(console_formatter)\n\n    # Add handlers to the root logger\n    logger.addHandler(file_handler)\n    logger.addHandler(console_handler)\n    \ndef clear_logger():\n    \"\"\"Remove all handlers from the root logger.\"\"\"\n    logger = logging.getLogger()\n    for handler in logger.handlers[:]:\n        logger.removeHandler(handler)\n        \nlog_filename = os.path.join(results_directory, \"lab8_log.txt\")\nprint(f\"The Lab 8 log file is located at {log_filename}.\")\n\n# Ensure the results_directory exists\nif not os.path.exists(results_directory):\n    os.makedirs(results_directory)\n\n# Check if the file exists; if not, create it\nif not os.path.exists(log_filename):\n    with open(log_filename, 'w') as file:\n        pass  # The file is now created.\n    \nclear_logger() # Clear the logger before reconfiguring it\nconfigure_logging(log_filename, log_file_debug_level=\"INFO\", console_debug_level=\"INFO\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To run this notebook, you need to use the simulated data VCF file with Lab4 (skip the `Prepare Supplemental Data` section), Lab5, Lab6, and Lab7 first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prefix = \"ped_sim_run2_autosomes\"\n",
    "prefix_revised = prefix.replace(\"_autosomes\", \"\")\n",
    "\n",
    "source_path = Path(results_directory) / f\"{prefix_revised}.seg\"\n",
    "destination_path = Path(results_directory) / prefix / \"segments\" / f\"{prefix}.seg\"\n",
    "\n",
    "shutil.copy(source_path, destination_path)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": "segments_pedsim = Path(results_directory) / prefix / \"segments\" / f\"{prefix}.seg\"\nsegments_refinedibd = Path(results_directory) / prefix / \"segments\" / f\"{prefix}_refinedibd.seg\"\nsegments_hapibd = Path(results_directory) / prefix / \"segments\" / f\"{prefix}_hapibd.seg\"\nsegments_ibis = Path(results_directory) / prefix / \"segments\" / f\"{prefix}_ibis.seg\"\n\n# Check for alternative naming patterns\nif not segments_refinedibd.exists():\n    segments_refinedibd_alt = Path(results_directory) / prefix / \"segments\" / f\"{prefix}_autosomes_refinedibd.seg\"\n    if segments_refinedibd_alt.exists():\n        segments_refinedibd = segments_refinedibd_alt\n        print(f\"Using alternative path for Refined-IBD: {segments_refinedibd}\")\n        \nif not segments_hapibd.exists():\n    segments_hapibd_alt = Path(results_directory) / prefix / \"segments\" / f\"{prefix}_autosomes_hapibd.seg\"\n    if segments_hapibd_alt.exists():\n        segments_hapibd = segments_hapibd_alt\n        print(f\"Using alternative path for Hap-IBD: {segments_hapibd}\")\n        \nif not segments_ibis.exists():\n    segments_ibis_alt = Path(results_directory) / prefix / \"segments\" / f\"{prefix}_autosomes_ibis.seg\"\n    if segments_ibis_alt.exists():\n        segments_ibis = segments_ibis_alt\n        print(f\"Using alternative path for IBIS: {segments_ibis}\")\n\nsegment_files = [segments_pedsim, segments_refinedibd, segments_hapibd, segments_ibis]\n\n# Check if each file exists\nmissing_files = []\nfor file_path in segment_files:\n    if not file_path.exists():\n        print(f\"Warning: File not found - {file_path}\")\n        missing_files.append(file_path.name)\n    else:\n        print(f\"File exists: {file_path}\")\n\n# Provide instructions if files are missing\nif missing_files:\n    print(\"\\nMissing segment files. To fix this issue:\")\n    print(\"1. Make sure you've run the corresponding IBD detection labs (Lab4-Lab7) first\")\n    print(\"2. Check the output directories in your results folder\")\n    print(\"3. For missing PedSim ground truth, try running cell 4 to copy it from the expected location\")\n    print(\"\\nMissing files: \" + \", \".join(missing_files))"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_refined_ibd(filepath):\n",
    "    \"\"\"Load Refined IBD output file\"\"\"\n",
    "    try:\n",
    "        df = pd.read_csv(filepath, sep=\"\\t\", header=None)\n",
    "        df.columns = [\"sample1\", \"sample1_haplotype\", \"sample2\", \n",
    "                      \"sample2_haplotype\", \"chrom\", \"start\", \"end\", \"LOD\", \"cM\"]\n",
    "        # Create a unique segment ID for each segment\n",
    "        df['segment_id'] = range(len(df))\n",
    "        df['tool'] = 'RefinedIBD'\n",
    "        df['length'] = df['end'] - df['start']\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading Refined IBD file: {e}\")\n",
    "        cols = [\"sample1\", \"sample1_haplotype\", \"sample2\", \"sample2_haplotype\", \n",
    "                \"chrom\", \"start\", \"end\", \"LOD\", \"cM\"]\n",
    "        return pd.DataFrame(columns=cols + ['segment_id', 'tool', 'length'])\n",
    "\n",
    "def load_hap_ibd(filepath):\n",
    "   \"\"\"Load Hap IBD output file\"\"\"\n",
    "   try:\n",
    "       df = pd.read_csv(filepath, sep=\"\\t\", header=None)\n",
    "       df.columns = [\"sample1\", \"sample1_haplotype\", \"sample2\", \n",
    "                     \"sample2_haplotype\", \"chrom\", \"start\", \"end\", \"cM\"]\n",
    "       df['segment_id'] = range(len(df))\n",
    "       df['tool'] = 'HapIBD'\n",
    "       df['length'] = df['end'] - df['start']\n",
    "       # Since LOD isn't in the HapIBD output, create a placeholder or use cM as proxy\n",
    "       df['LOD'] = df['cM']  # Using cM as a proxy for confidence\n",
    "       return df\n",
    "   except Exception as e:\n",
    "       print(f\"Error loading Hap IBD file: {e}\")\n",
    "       cols = [\"sample1\", \"sample1_haplotype\", \"sample2\", \"sample2_haplotype\", \n",
    "               \"chrom\", \"start\", \"end\", \"cM\"]\n",
    "       return pd.DataFrame(columns=cols + ['segment_id', 'tool', 'length', 'LOD'])\n",
    "\n",
    "def load_ibis(filepath):\n",
    "    \"\"\"Load IBIS output file\"\"\"\n",
    "    try:\n",
    "        df = pd.read_csv(filepath, sep=\"\\t\", header=None)\n",
    "        df.columns = [\"sample1\", \"sample2\", \"chrom\", \n",
    "                    \"phys_start_pos\", \"phys_end_pos\", \n",
    "                    \"IBD_type\", \"genetic_start_pos\", \n",
    "                    \"genetic_end_pos\", \"genetic_seg_length\", \n",
    "                    \"marker_count\", \"error_count\", \"error_density\"]\n",
    "        \n",
    "        # Map IBIS columns to standardized column names for evaluation\n",
    "        df['start'] = df['phys_start_pos']\n",
    "        df['end'] = df['phys_end_pos']\n",
    "        df['cM'] = df['genetic_seg_length']\n",
    "        df['segment_id'] = range(len(df))\n",
    "        df['tool'] = 'IBIS'\n",
    "        df['length'] = df['end'] - df['start']\n",
    "        \n",
    "        # Create LOD-like score based on error density (lower error = higher score)\n",
    "        # Invert error_density to make higher values better\n",
    "        df['LOD'] = 1.0 / (df['error_density'] + 0.001)  # Add small constant to avoid division by zero\n",
    "        \n",
    "        # Add haplotype columns if needed for consistent evaluation\n",
    "        df['sample1_haplotype'] = 0  # Placeholder if IBIS doesn't specify haplotypes\n",
    "        df['sample2_haplotype'] = 0\n",
    "        \n",
    "        return df\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading IBIS file: {e}\")\n",
    "        cols = [\"sample1\", \"sample2\", \"chrom\", \"phys_start_pos\", \"phys_end_pos\", \n",
    "                \"IBD_type\", \"genetic_start_pos\", \"genetic_end_pos\", \"genetic_seg_length\", \n",
    "                \"marker_count\", \"error_count\", \"error_density\"]\n",
    "        return pd.DataFrame(columns=cols + ['start', 'end', 'segment_id', 'tool', 'length', 'LOD', \n",
    "                                           'sample1_haplotype', 'sample2_haplotype'])\n",
    "\n",
    "def load_pedsim_truth(filepath):\n",
    "   \"\"\"Load ground truth IBD segments from ped-sim\"\"\"\n",
    "   try:\n",
    "       df = pd.read_csv(filepath, sep=\"\\t\", header=None)\n",
    "       df.columns = [\"id1\", \"id2\", \"chromosome\", \"physical_position_start\", \n",
    "                    \"physical_position_end\", \"IBD_type\", \"genetic_position_start\", \n",
    "                    \"genetic_position_end\", \"genetic_length\"]\n",
    "       \n",
    "       # Map to standardized column names used in evaluation\n",
    "       df['sample1'] = df['id1']\n",
    "       df['sample2'] = df['id2']\n",
    "       df['chrom'] = df['chromosome']\n",
    "       df['start'] = df['physical_position_start']\n",
    "       df['end'] = df['physical_position_end']\n",
    "       df['cM'] = df['genetic_length']\n",
    "       df['sample1_haplotype'] = 0  # Add placeholder haplotypes if needed\n",
    "       df['sample2_haplotype'] = 0  # These can be updated if actual haplotype info exists\n",
    "       \n",
    "       df['segment_id'] = range(len(df))\n",
    "       df['tool'] = 'Truth'\n",
    "       df['length'] = df['end'] - df['start']\n",
    "       return df\n",
    "   except Exception as e:\n",
    "       print(f\"Error loading PedSim truth file: {e}\")\n",
    "       cols = [\"id1\", \"id2\", \"chromosome\", \"physical_position_start\", \n",
    "              \"physical_position_end\", \"IBD_type\", \"genetic_position_start\", \n",
    "              \"genetic_position_end\", \"genetic_length\"]\n",
    "       return pd.DataFrame(columns=cols + ['sample1', 'sample2', 'chrom', 'start', 'end', \n",
    "                                         'cM', 'segment_id', 'tool', 'length',\n",
    "                                         'sample1_haplotype', 'sample2_haplotype'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "refined_df = load_refined_ibd(segments_refinedibd)\n",
    "hap_df = load_hap_ibd(segments_hapibd)\n",
    "ibis_df = load_ibis(segments_ibis)\n",
    "truth_df = load_pedsim_truth(segments_pedsim)\n",
    "\n",
    "# Print data summaries\n",
    "print(f\"Loaded {len(refined_df)} Refined IBD segments\")\n",
    "print(f\"Loaded {len(hap_df)} Hap IBD segments\")\n",
    "print(f\"Loaded {len(ibis_df)} IBIS segments\")\n",
    "print(f\"Loaded {len(truth_df)} truth segments from ped-sim\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_interval_tree(truth_df):\n",
    "    \"\"\"Create an interval tree from truth segments for efficient overlap checking\"\"\"\n",
    "    trees = {}\n",
    "    \n",
    "    print(\"Building interval trees from truth data...\")\n",
    "    \n",
    "    # First try with standard column names\n",
    "    if all(col in truth_df.columns for col in ['sample1', 'sample2', 'chrom', 'start', 'end']):\n",
    "        sample1_col, sample2_col = 'sample1', 'sample2'\n",
    "    # Then try with ped-sim column names\n",
    "    elif all(col in truth_df.columns for col in ['id1', 'id2', 'chromosome', 'physical_position_start', 'physical_position_end']):\n",
    "        sample1_col, sample2_col = 'id1', 'id2'\n",
    "        # Map columns for consistency\n",
    "        truth_df['sample1'] = truth_df['id1']\n",
    "        truth_df['sample2'] = truth_df['id2']\n",
    "        truth_df['chrom'] = truth_df['chromosome']\n",
    "        truth_df['start'] = truth_df['physical_position_start']\n",
    "        truth_df['end'] = truth_df['physical_position_end']\n",
    "    else:\n",
    "        print(\"Error: Could not identify required columns in truth data\")\n",
    "        return trees\n",
    "    \n",
    "    # Add haplotype columns if they don't exist\n",
    "    if 'sample1_haplotype' not in truth_df.columns:\n",
    "        truth_df['sample1_haplotype'] = 0\n",
    "    if 'sample2_haplotype' not in truth_df.columns:\n",
    "        truth_df['sample2_haplotype'] = 0\n",
    "    \n",
    "    # Create trees with and without haplotype information\n",
    "    for _, row in truth_df.iterrows():\n",
    "        # Create keys in both orders to handle either order in tool data\n",
    "        pairs = [\n",
    "            # With haplotypes\n",
    "            ((row['sample1'], row['sample1_haplotype']), \n",
    "             (row['sample2'], row['sample2_haplotype']), \n",
    "             row['chrom']),\n",
    "            ((row['sample2'], row['sample2_haplotype']), \n",
    "             (row['sample1'], row['sample1_haplotype']), \n",
    "             row['chrom']),\n",
    "            # Without haplotypes\n",
    "            ((row['sample1'], None), (row['sample2'], None), row['chrom']),\n",
    "            ((row['sample2'], None), (row['sample1'], None), row['chrom'])\n",
    "        ]\n",
    "        \n",
    "        for sample1, sample2, chrom in pairs:\n",
    "            pair_key = (sample1, sample2)\n",
    "            if (pair_key, chrom) not in trees:\n",
    "                trees[(pair_key, chrom)] = IntervalTree()\n",
    "            \n",
    "            trees[(pair_key, chrom)].addi(row['start'], row['end'], row['segment_id'])\n",
    "    \n",
    "    print(f\"Created {len(trees)} interval trees\")\n",
    "    return trees\n",
    "\n",
    "def calculate_overlap(segment, tree):\n",
    "    \"\"\"Calculate overlap between a segment and truth segments in the tree\"\"\"\n",
    "    overlaps = tree.overlap(segment['start'], segment['end'])\n",
    "    if not overlaps:\n",
    "        return 0, None\n",
    "    \n",
    "    # Find the best overlapping segment\n",
    "    best_overlap = 0\n",
    "    best_truth_id = None\n",
    "    \n",
    "    for interval in overlaps:\n",
    "        overlap_start = max(segment['start'], interval.begin)\n",
    "        overlap_end = min(segment['end'], interval.end)\n",
    "        overlap_length = overlap_end - overlap_start\n",
    "        \n",
    "        if overlap_length > best_overlap:\n",
    "            best_overlap = overlap_length\n",
    "            best_truth_id = interval.data\n",
    "    \n",
    "    return best_overlap / (segment['end'] - segment['start']), best_truth_id\n",
    "\n",
    "def evaluate_tool(tool_df, truth_trees):\n",
    "    \"\"\"Evaluate IBD detection performance for a specific tool\"\"\"\n",
    "    # If the DataFrame is empty, return it without processing\n",
    "    if len(tool_df) == 0:\n",
    "        print(f\"Tool: {tool_df.name if hasattr(tool_df, 'name') else 'Unknown'} - No segments to evaluate\")\n",
    "        return tool_df\n",
    "        \n",
    "    # Add columns for evaluation metrics\n",
    "    tool_df['detected_truth'] = False\n",
    "    tool_df['overlap_pct'] = 0.0\n",
    "    tool_df['truth_id'] = None\n",
    "    \n",
    "    # Debug counters\n",
    "    total_segments = len(tool_df)\n",
    "    matched_segments = 0\n",
    "    \n",
    "    for idx, row in tool_df.iterrows():\n",
    "        # Check if we have both sample pair in regular and reversed order\n",
    "        sample_pairs = [\n",
    "            # Regular order\n",
    "            ((row['sample1'], row.get('sample1_haplotype', 0)), \n",
    "             (row['sample2'], row.get('sample2_haplotype', 0)),\n",
    "             row['chrom']),\n",
    "            # Reversed order\n",
    "            ((row['sample2'], row.get('sample2_haplotype', 0)), \n",
    "             (row['sample1'], row.get('sample1_haplotype', 0)),\n",
    "             row['chrom'])\n",
    "        ]\n",
    "        \n",
    "        found_match = False\n",
    "        for sample1, sample2, chrom in sample_pairs:\n",
    "            pair_key = (sample1, sample2)\n",
    "            if (pair_key, chrom) in truth_trees:\n",
    "                overlap_pct, truth_id = calculate_overlap(row, truth_trees[(pair_key, chrom)])\n",
    "                if overlap_pct > 0:\n",
    "                    tool_df.at[idx, 'overlap_pct'] = overlap_pct\n",
    "                    tool_df.at[idx, 'truth_id'] = truth_id\n",
    "                    tool_df.at[idx, 'detected_truth'] = (overlap_pct >= 0.5)  # Consider >=50% overlap a true positive\n",
    "                    matched_segments += 1 if overlap_pct >= 0.5 else 0\n",
    "                    found_match = True\n",
    "                    break\n",
    "        \n",
    "        # Try without haplotypes if no match found and haplotypes present\n",
    "        if not found_match and 'sample1_haplotype' in row:\n",
    "            # Create keys without haplotype information\n",
    "            sample_pairs_no_hap = [\n",
    "                ((row['sample1'], None), (row['sample2'], None), row['chrom']),\n",
    "                ((row['sample2'], None), (row['sample1'], None), row['chrom'])\n",
    "            ]\n",
    "            \n",
    "            for sample1, sample2, chrom in sample_pairs_no_hap:\n",
    "                pair_key = (sample1, sample2)\n",
    "                if (pair_key, chrom) in truth_trees:\n",
    "                    overlap_pct, truth_id = calculate_overlap(row, truth_trees[(pair_key, chrom)])\n",
    "                    if overlap_pct > 0:\n",
    "                        tool_df.at[idx, 'overlap_pct'] = overlap_pct\n",
    "                        tool_df.at[idx, 'truth_id'] = truth_id\n",
    "                        tool_df.at[idx, 'detected_truth'] = (overlap_pct >= 0.5)\n",
    "                        matched_segments += 1 if overlap_pct >= 0.5 else 0\n",
    "                        break\n",
    "    \n",
    "    # Get the tool name safely\n",
    "    tool_name = tool_df['tool'].iloc[0] if len(tool_df) > 0 else \"Unknown\"\n",
    "    \n",
    "    # Calculate percentage safely\n",
    "    percentage = (matched_segments/total_segments*100) if total_segments > 0 else 0\n",
    "    \n",
    "    print(f\"Tool: {tool_name} - Matched {matched_segments} of {total_segments} segments ({percentage:.2f}%)\")\n",
    "    return tool_df\n",
    "\n",
    "def evaluate_all_tools(refined_df, hap_df, ibis_df, truth_df):\n",
    "    \"\"\"Evaluate all IBD detection tools\"\"\"\n",
    "    # Create interval trees for truth segments\n",
    "    truth_trees = create_interval_tree(truth_df)\n",
    "    \n",
    "    # Evaluate each tool\n",
    "    refined_eval = evaluate_tool(refined_df, truth_trees)\n",
    "    \n",
    "    # Only evaluate tools with data\n",
    "    if len(hap_df) > 0:\n",
    "        hap_eval = evaluate_tool(hap_df, truth_trees)\n",
    "    else:\n",
    "        print(\"Tool: HapIBD - No segments to evaluate\")\n",
    "        hap_eval = hap_df.copy()\n",
    "        hap_eval['tool'] = 'HapIBD'  # Ensure tool column exists\n",
    "    \n",
    "    ibis_eval = evaluate_tool(ibis_df, truth_trees)\n",
    "    \n",
    "    # Combine results\n",
    "    all_results = pd.concat([refined_eval, hap_eval, ibis_eval], ignore_index=True)\n",
    "    \n",
    "    return all_results, truth_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate tools\n",
    "print(\"Evaluating IBD detection tools...\")\n",
    "all_results, truth_df = evaluate_all_tools(refined_df, hap_df, ibis_df, truth_df)\n",
    "print(\"Done!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def diagnose_data_format_issues(all_results, truth_df):\n",
    "    \"\"\"Diagnose potential format inconsistencies between tool outputs and truth data\"\"\"\n",
    "    print(\"\\n=== Data Format Diagnosis ===\")\n",
    "    \n",
    "    # Check chromosome formats\n",
    "    truth_chroms = set(truth_df['chrom'].unique())\n",
    "    print(f\"Truth data chromosome formats (sample of 5): {list(truth_chroms)[:5]}\")\n",
    "    \n",
    "    for tool_name in ['RefinedIBD', 'HapIBD', 'IBIS']:\n",
    "        tool_df = all_results[all_results['tool'] == tool_name]\n",
    "        if len(tool_df) == 0:\n",
    "            continue\n",
    "            \n",
    "        tool_chroms = set(tool_df['chrom'].unique())\n",
    "        print(f\"{tool_name} chromosome formats (sample of 5): {list(tool_chroms)[:5]}\")\n",
    "        \n",
    "        # Check for chromosome format mismatches\n",
    "        if not truth_chroms.intersection(tool_chroms):\n",
    "            print(f\"⚠️ WARNING: No matching chromosome formats between truth and {tool_name}!\")\n",
    "            print(f\"  Consider standardizing chromosome formats (e.g., '1' vs 'chr1')\")\n",
    "    \n",
    "    # Check sample ID formats\n",
    "    if 'sample1' in truth_df.columns and 'sample2' in truth_df.columns:\n",
    "        truth_samples = set(truth_df['sample1'].unique()).union(set(truth_df['sample2'].unique()))\n",
    "        print(f\"Truth data sample ID formats (sample of 5): {list(truth_samples)[:5]}\")\n",
    "        \n",
    "        for tool_name in ['RefinedIBD', 'HapIBD', 'IBIS']:\n",
    "            tool_df = all_results[all_results['tool'] == tool_name]\n",
    "            if len(tool_df) == 0 or 'sample1' not in tool_df.columns or 'sample2' not in tool_df.columns:\n",
    "                continue\n",
    "                \n",
    "            tool_samples = set(tool_df['sample1'].unique()).union(set(tool_df['sample2'].unique()))\n",
    "            print(f\"{tool_name} sample ID formats (sample of 5): {list(tool_samples)[:5]}\")\n",
    "            \n",
    "            # Check for sample ID format mismatches\n",
    "            if not truth_samples.intersection(tool_samples):\n",
    "                print(f\"⚠️ WARNING: No matching sample IDs between truth and {tool_name}!\")\n",
    "                print(f\"  Consider standardizing sample ID formats\")\n",
    "    \n",
    "    # Check position ranges\n",
    "    truth_min_pos = truth_df['start'].min()\n",
    "    truth_max_pos = truth_df['end'].max()\n",
    "    print(f\"Truth data position range: {truth_min_pos:,} - {truth_max_pos:,}\")\n",
    "    \n",
    "    for tool_name in ['RefinedIBD', 'HapIBD', 'IBIS']:\n",
    "        tool_df = all_results[all_results['tool'] == tool_name]\n",
    "        if len(tool_df) == 0:\n",
    "            continue\n",
    "            \n",
    "        tool_min_pos = tool_df['start'].min()\n",
    "        tool_max_pos = tool_df['end'].max()\n",
    "        print(f\"{tool_name} position range: {tool_min_pos:,} - {tool_max_pos:,}\")\n",
    "        \n",
    "        # Check for major position range mismatches\n",
    "        if (tool_min_pos > truth_max_pos) or (tool_max_pos < truth_min_pos):\n",
    "            print(f\"⚠️ WARNING: Position ranges don't overlap between truth and {tool_name}!\")\n",
    "            print(f\"  Consider checking for coordinate system differences\")\n",
    "    \n",
    "    print(\"\\n=== End of Diagnosis ===\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diagnose_data_format_issues(all_results, truth_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_length_distribution(all_results, truth_df):\n",
    "    \"\"\"Plot the distribution of segment lengths for each tool and truth\"\"\"\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    \n",
    "    # Combine all data\n",
    "    all_data = pd.concat([\n",
    "        all_results[all_results['tool'] == 'RefinedIBD'][['length', 'tool']],\n",
    "        all_results[all_results['tool'] == 'HapIBD'][['length', 'tool']],\n",
    "        all_results[all_results['tool'] == 'IBIS'][['length', 'tool']],\n",
    "        truth_df[['length', 'tool']]\n",
    "    ])\n",
    "    \n",
    "    # Convert length from bp to Mbp\n",
    "    all_data['length'] = all_data['length'] / 1_000_000\n",
    "    \n",
    "    # Plot density\n",
    "    sns.kdeplot(data=all_data, x='length', hue='tool', fill=True, alpha=0.5)\n",
    "    \n",
    "    plt.title('Distribution of IBD Segment Lengths')\n",
    "    plt.xlabel('Segment Length (Mbp)')\n",
    "    plt.ylabel('Density')\n",
    "    plt.grid(True, linestyle='--', alpha=0.7)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(Path(results_directory) / prefix / 'segments' / 'ibd_length_distribution.png')\n",
    "    plt.show()\n",
    "\n",
    "def plot_precision_recall(all_results, truth_df):\n",
    "    \"\"\"Plot precision-recall curves for each tool\"\"\"\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    \n",
    "    has_data = False\n",
    "    # For each tool, calculate precision and recall using overlap percentage as score\n",
    "    for tool_name, color in zip(['RefinedIBD', 'HapIBD', 'IBIS'], ['blue', 'green', 'red']):\n",
    "        tool_df = all_results[all_results['tool'] == tool_name]\n",
    "        \n",
    "        # Skip if no data for this tool\n",
    "        if len(tool_df) == 0:\n",
    "            continue\n",
    "            \n",
    "        # Check if we have true positives\n",
    "        if tool_df['detected_truth'].sum() == 0:\n",
    "            print(f\"Warning: No true positives found for {tool_name}, skipping PR curve\")\n",
    "            continue\n",
    "            \n",
    "        has_data = True\n",
    "        \n",
    "        if 'LOD' in tool_df.columns and not tool_df['LOD'].isna().all():\n",
    "            score = tool_df['LOD']  # Use LOD score if available\n",
    "        else:\n",
    "            score = tool_df['length']  # Otherwise use length as a proxy for confidence\n",
    "            \n",
    "        y_true = tool_df['detected_truth'].astype(int)\n",
    "        \n",
    "        try:\n",
    "            # Calculate precision and recall\n",
    "            precision, recall, _ = precision_recall_curve(y_true, score)\n",
    "            avg_precision = average_precision_score(y_true, score)\n",
    "            \n",
    "            # Plot PR curve\n",
    "            plt.plot(recall, precision, lw=2, color=color,\n",
    "                     label=f'{tool_name} (AP={avg_precision:.2f})')\n",
    "        except Exception as e:\n",
    "            print(f\"Error plotting PR curve for {tool_name}: {e}\")\n",
    "    \n",
    "    if has_data:\n",
    "        plt.title('Precision-Recall Curves for IBD Detection Tools')\n",
    "        plt.xlabel('Recall')\n",
    "        plt.ylabel('Precision')\n",
    "        plt.xlim([0.0, 1.0])\n",
    "        plt.ylim([0.0, 1.05])\n",
    "        plt.grid(True, linestyle='--', alpha=0.7)\n",
    "        plt.legend(loc=\"best\")\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(Path(results_directory) / prefix / 'segments' / 'ibd_precision_recall.png')\n",
    "        plt.show()\n",
    "    else:\n",
    "        print(\"Skipping PR curve plot: No valid data\")\n",
    "\n",
    "def plot_roc_curves(all_results, truth_df):\n",
    "    \"\"\"Plot ROC curves for each tool\"\"\"\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    \n",
    "    has_data = False\n",
    "    # For each tool, calculate ROC using overlap percentage as score\n",
    "    for tool_name, color in zip(['RefinedIBD', 'HapIBD', 'IBIS'], ['blue', 'green', 'red']):\n",
    "        tool_df = all_results[all_results['tool'] == tool_name]\n",
    "        \n",
    "        # Skip if no data for this tool\n",
    "        if len(tool_df) == 0:\n",
    "            continue\n",
    "            \n",
    "        # Check if we have true positives\n",
    "        if tool_df['detected_truth'].sum() == 0:\n",
    "            print(f\"Warning: No true positives found for {tool_name}, skipping ROC curve\")\n",
    "            continue\n",
    "            \n",
    "        has_data = True\n",
    "        \n",
    "        if 'LOD' in tool_df.columns and not tool_df['LOD'].isna().all():\n",
    "            score = tool_df['LOD']  # Use LOD score if available\n",
    "        else:\n",
    "            score = tool_df['length']  # Otherwise use length as a proxy for confidence\n",
    "            \n",
    "        y_true = tool_df['detected_truth'].astype(int)\n",
    "        \n",
    "        try:\n",
    "            # Calculate ROC\n",
    "            fpr, tpr, _ = roc_curve(y_true, score)\n",
    "            roc_auc = auc(fpr, tpr)\n",
    "            \n",
    "            # Plot ROC curve\n",
    "            plt.plot(fpr, tpr, lw=2, color=color,\n",
    "                     label=f'{tool_name} (AUC={roc_auc:.2f})')\n",
    "        except Exception as e:\n",
    "            print(f\"Error plotting ROC curve for {tool_name}: {e}\")\n",
    "    \n",
    "    if has_data:\n",
    "        plt.plot([0, 1], [0, 1], linestyle='--', lw=2, color='gray', alpha=0.8)\n",
    "        plt.title('ROC Curves for IBD Detection Tools')\n",
    "        plt.xlabel('False Positive Rate')\n",
    "        plt.ylabel('True Positive Rate')\n",
    "        plt.xlim([0.0, 1.0])\n",
    "        plt.ylim([0.0, 1.05])\n",
    "        plt.grid(True, linestyle='--', alpha=0.7)\n",
    "        plt.legend(loc=\"lower right\")\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(Path(results_directory) / prefix / 'segments' / 'ibd_roc_curves.png')\n",
    "        plt.show()\n",
    "    else:\n",
    "        print(\"Skipping ROC curve plot: No valid data\")\n",
    "\n",
    "def plot_overlap_histogram(all_results):\n",
    "    \"\"\"Plot histogram of overlap percentages for each tool\"\"\"\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    \n",
    "    for tool_name, color in zip(['RefinedIBD', 'HapIBD', 'IBIS'], ['blue', 'green', 'red']):\n",
    "        tool_df = all_results[all_results['tool'] == tool_name]\n",
    "        plt.hist(tool_df['overlap_pct'], bins=20, alpha=0.5, color=color, label=tool_name)\n",
    "    \n",
    "    plt.title('Distribution of Overlap with Truth Segments')\n",
    "    plt.xlabel('Overlap Percentage')\n",
    "    plt.ylabel('Count')\n",
    "    plt.grid(True, linestyle='--', alpha=0.7)\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(Path(results_directory) / prefix / 'segments' / 'ibd_overlap_histogram.png')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate visualizations\n",
    "print(\"Generating visualizations...\")\n",
    "plot_length_distribution(all_results, truth_df)\n",
    "plot_precision_recall(all_results, truth_df)\n",
    "plot_roc_curves(all_results, truth_df)\n",
    "plot_overlap_histogram(all_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_summary_metrics(all_results, truth_df):\n",
    "    \"\"\"Calculate summary statistics for each tool\"\"\"\n",
    "    metrics = []\n",
    "    \n",
    "    # Count total truth segments\n",
    "    total_truth = len(truth_df)\n",
    "    print(f\"Total truth segments: {total_truth}\")\n",
    "    \n",
    "    for tool_name in ['RefinedIBD', 'HapIBD', 'IBIS']:\n",
    "        tool_df = all_results[all_results['tool'] == tool_name]\n",
    "        \n",
    "        if len(tool_df) == 0:\n",
    "            print(f\"No segments found for {tool_name}, skipping metrics\")\n",
    "            continue\n",
    "        \n",
    "        # Count true positives (segments with >50% overlap)\n",
    "        true_positives = tool_df['detected_truth'].sum()\n",
    "        \n",
    "        # Count false positives (segments with ≤50% overlap)\n",
    "        false_positives = len(tool_df) - true_positives\n",
    "        \n",
    "        # Count truth segments detected by this tool\n",
    "        detected_truth_ids = set([x for x in tool_df['truth_id'] if x is not None])\n",
    "        detected_truths = len(detected_truth_ids)\n",
    "        \n",
    "        # Calculate recall (proportion of truth segments detected)\n",
    "        recall = detected_truths / total_truth if total_truth > 0 else 0\n",
    "        \n",
    "        # Calculate precision (proportion of detected segments that are true)\n",
    "        precision = true_positives / len(tool_df) if len(tool_df) > 0 else 0\n",
    "        \n",
    "        # Calculate F1 score\n",
    "        f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "        \n",
    "        # Print detailed debug info\n",
    "        print(f\"\\n{tool_name} Summary:\")\n",
    "        print(f\"  Total segments: {len(tool_df)}\")\n",
    "        print(f\"  True positives: {true_positives} ({true_positives/len(tool_df)*100:.2f}% of segments)\")\n",
    "        print(f\"  False positives: {false_positives} ({false_positives/len(tool_df)*100:.2f}% of segments)\")\n",
    "        print(f\"  Unique truth segments detected: {detected_truths} ({detected_truths/total_truth*100:.2f}% of truth)\")\n",
    "        print(f\"  Precision: {precision:.4f}\")\n",
    "        print(f\"  Recall: {recall:.4f}\")\n",
    "        print(f\"  F1 Score: {f1:.4f}\")\n",
    "        \n",
    "        metrics.append({\n",
    "            'Tool': tool_name,\n",
    "            'Total Segments': len(tool_df),\n",
    "            'True Positives': true_positives,\n",
    "            'False Positives': false_positives,\n",
    "            'Detected Truth Segments': detected_truths,\n",
    "            'Precision': precision,\n",
    "            'Recall': recall,\n",
    "            'F1 Score': f1\n",
    "        })\n",
    "    \n",
    "    metrics_df = pd.DataFrame(metrics)\n",
    "    \n",
    "    # Check if we have meaningful metrics\n",
    "    if len(metrics_df) == 0 or metrics_df['True Positives'].sum() == 0:\n",
    "        print(\"\\n⚠️ WARNING: No true positives detected across any tool!\")\n",
    "        print(\"This suggests an issue with matching segments to ground truth.\")\n",
    "        print(\"Possible causes:\")\n",
    "        print(\"1. Column name mismatches between tool output and truth data\")\n",
    "        print(\"2. Sample ID format differences\")\n",
    "        print(\"3. Chromosome notation differences (e.g., 'chr1' vs '1')\")\n",
    "        print(\"4. Position coordinate system differences\")\n",
    "    \n",
    "    return metrics_df\n",
    "\n",
    "# Calculate and display summary metrics\n",
    "print(\"Calculating summary metrics...\")\n",
    "metrics_df = calculate_summary_metrics(all_results, truth_df)\n",
    "display(metrics_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_summary_barplot(metrics_df):\n",
    "    \"\"\"Plot summary metrics as a bar chart\"\"\"\n",
    "    plt.figure(figsize=(14, 10))\n",
    "    \n",
    "    # Melt the dataframe to make it suitable for grouped bar chart\n",
    "    plot_metrics = ['Precision', 'Recall', 'F1 Score']\n",
    "    plot_df = pd.melt(metrics_df, id_vars=['Tool'], value_vars=plot_metrics, \n",
    "                      var_name='Metric', value_name='Value')\n",
    "    \n",
    "    # Create grouped bar chart\n",
    "    sns.barplot(x='Tool', y='Value', hue='Metric', data=plot_df)\n",
    "    \n",
    "    plt.title('Performance Metrics by IBD Detection Tool')\n",
    "    plt.ylabel('Score')\n",
    "    plt.ylim(0, 1)\n",
    "    plt.grid(True, linestyle='--', alpha=0.7, axis='y')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(Path(results_directory) / prefix / 'segments' / 'ibd_performance_metrics.png')\n",
    "    plt.show()\n",
    "    \n",
    "plot_summary_barplot(metrics_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_chromosome_performance(all_results, truth_df):\n",
    "    \"\"\"Plot performance metrics by chromosome\"\"\"\n",
    "    # Get unique chromosomes\n",
    "    all_chroms = sorted(truth_df['chrom'].unique())\n",
    "    \n",
    "    # Initialize metrics dictionary\n",
    "    chrom_metrics = {tool: {chrom: {'precision': 0, 'recall': 0, 'f1': 0} \n",
    "                           for chrom in all_chroms} \n",
    "                    for tool in ['RefinedIBD', 'HapIBD', 'IBIS']}\n",
    "    \n",
    "    # Calculate metrics per chromosome\n",
    "    for chrom in all_chroms:\n",
    "        # Count truth segments in this chromosome\n",
    "        truth_in_chrom = truth_df[truth_df['chrom'] == chrom]\n",
    "        total_truth = len(truth_in_chrom)\n",
    "        \n",
    "        for tool in ['RefinedIBD', 'HapIBD', 'IBIS']:\n",
    "            # Get tool results for this chromosome\n",
    "            tool_results = all_results[(all_results['tool'] == tool) & \n",
    "                                       (all_results['chrom'] == chrom)]\n",
    "            \n",
    "            if len(tool_results) == 0:\n",
    "                continue\n",
    "                \n",
    "            # Count true positives\n",
    "            true_positives = tool_results['detected_truth'].sum()\n",
    "            \n",
    "            # Count detected truth segments\n",
    "            detected_truth_ids = set([x for x in tool_results['truth_id'] if x is not None])\n",
    "            detected_truths = len(detected_truth_ids)\n",
    "            \n",
    "            # Calculate metrics\n",
    "            precision = true_positives / len(tool_results) if len(tool_results) > 0 else 0\n",
    "            recall = detected_truths / total_truth if total_truth > 0 else 0\n",
    "            f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "            \n",
    "            chrom_metrics[tool][chrom]['precision'] = precision\n",
    "            chrom_metrics[tool][chrom]['recall'] = recall\n",
    "            chrom_metrics[tool][chrom]['f1'] = f1\n",
    "    \n",
    "    # Create dataframes for plotting\n",
    "    plot_data = []\n",
    "    for tool in chrom_metrics:\n",
    "        for chrom in chrom_metrics[tool]:\n",
    "            for metric in ['precision', 'recall', 'f1']:\n",
    "                plot_data.append({\n",
    "                    'Tool': tool,\n",
    "                    'Chromosome': chrom,\n",
    "                    'Metric': metric.capitalize(),\n",
    "                    'Value': chrom_metrics[tool][chrom][metric]\n",
    "                })\n",
    "    \n",
    "    plot_df = pd.DataFrame(plot_data)\n",
    "    \n",
    "    # Plot\n",
    "    plt.figure(figsize=(16, 12))\n",
    "    g = sns.FacetGrid(plot_df, col='Metric', row='Tool', height=3, aspect=2)\n",
    "    g.map_dataframe(sns.barplot, x='Chromosome', y='Value')\n",
    "    g.set_axis_labels('Chromosome', 'Score')\n",
    "    g.set_titles('{row_name} - {col_name}')\n",
    "    \n",
    "    for ax in g.axes.flat:\n",
    "        ax.set_ylim(0, 1)\n",
    "        ax.grid(True, linestyle='--', alpha=0.7, axis='y')\n",
    "        \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(Path(results_directory) / prefix / 'segments' / 'ibd_chromosome_performance.png')\n",
    "    plt.show()\n",
    "    \n",
    "plot_chromosome_performance(all_results, truth_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "<cell_type>markdown</cell_type>⚠️ **Warning**: The next cell (cell 18) can take a very long time to run (approximately 76 minutes). This is because it performs a detailed analysis for each segment length bin.\n\n**Options for proceeding:**\n\n1. **Run the full analysis** if you have plenty of time and computing resources.\n2. **Use a subset** of the data by modifying the code to use fewer bins or process only certain chromosomes.\n3. **Skip the cell** entirely and continue with the rest of the notebook - the analysis is supplemental and not critical for understanding the core concepts.\n\nIf you're in a workshop or classroom setting with limited time, option 3 is recommended.",
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": "def plot_accuracy_by_length(all_results, truth_df, sample_size=None):\n    \"\"\"\n    Plot detection accuracy as a function of segment length.\n    \n    Args:\n        all_results: DataFrame with IBD detection results\n        truth_df: DataFrame with ground truth segments\n        sample_size: If provided, limit analysis to this many truth segments for faster execution\n    \"\"\"\n    plt.figure(figsize=(12, 8))\n    \n    # Bin truth segments by length\n    bins = [0, 1e4, 5e4, 1e5, 5e5, 1e6, 5e6, float('inf')]\n    bin_labels = ['<10kb', '10-50kb', '50-100kb', '0.1-0.5Mb', '0.5-1Mb', '1-5Mb', '>5Mb']\n    \n    # Add length bins to truth dataframe\n    truth_df['length_bin'] = pd.cut(truth_df['length'], bins=bins, labels=bin_labels)\n    \n    # Optionally sample the truth data for faster execution\n    if sample_size is not None and len(truth_df) > sample_size:\n        # Stratified sampling to maintain distribution across bins\n        truth_df = truth_df.groupby('length_bin', observed=True).apply(\n            lambda x: x.sample(min(len(x), max(1, int(sample_size * len(x) / len(truth_df)))))\n        ).reset_index(drop=True)\n        print(f\"Using a stratified sample of {len(truth_df)} truth segments for faster execution\")\n    \n    # Count total truth segments per bin\n    truth_counts = truth_df.groupby('length_bin', observed=True).size()\n    \n    # For each tool, calculate detection rate by length bin using vectorized operations\n    for tool_name, color in zip(['RefinedIBD', 'HapIBD', 'IBIS'], ['blue', 'green', 'red']):\n        detection_rates = []\n        \n        # Get the tool's dataframe\n        tool_df = all_results[all_results['tool'] == tool_name]\n        if len(tool_df) == 0:\n            continue\n            \n        # Create a set of truth_ids detected by this tool for faster lookup\n        detected_truth_ids = set(tool_df['truth_id'].dropna())\n        \n        for bin_label in bin_labels:\n            # Get truth segments in this bin\n            bin_truth = truth_df[truth_df['length_bin'] == bin_label]\n            total = len(bin_truth)\n            \n            if total == 0:\n                detection_rates.append(0)\n                continue\n            \n            # Count how many were detected by this tool - vectorized approach\n            detected = sum(id in detected_truth_ids for id in bin_truth['segment_id'])\n            detection_rates.append(detected / total if total > 0 else 0)\n        \n        plt.plot(bin_labels, detection_rates, marker='o', label=tool_name, color=color, linewidth=2)\n    \n    plt.title('IBD Detection Rate by Segment Length')\n    plt.xlabel('Segment Length')\n    plt.ylabel('Detection Rate')\n    plt.ylim(0, 1.05)\n    plt.grid(True, linestyle='--', alpha=0.7)\n    plt.legend()\n    plt.tight_layout()\n    plt.savefig(Path(results_directory) / prefix / 'segments' / 'ibd_detection_by_length.png')\n    plt.show()\n    \n    # Also create a bar chart showing segment counts by length\n    plt.figure(figsize=(10, 6))\n    truth_counts.plot(kind='bar', color='purple')\n    plt.title('Number of Truth Segments by Length')\n    plt.xlabel('Segment Length')\n    plt.ylabel('Count')\n    plt.grid(True, linestyle='--', alpha=0.7, axis='y')\n    plt.tight_layout()\n    plt.savefig(Path(results_directory) / prefix / 'segments' / 'ibd_truth_segments_by_length.png')\n    plt.show()\n\n# Choose one of these calls:\n\n# 1. Full analysis (takes ~76 minutes)\n# plot_accuracy_by_length(all_results, truth_df)\n\n# 2. Faster version with sampling (takes ~5-10 minutes)\nplot_accuracy_by_length(all_results, truth_df, sample_size=1000)\n\n# 3. Comment out both lines above to skip this analysis entirely"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}