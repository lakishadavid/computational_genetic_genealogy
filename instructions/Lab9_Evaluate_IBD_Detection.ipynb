{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from collections import Counter\n",
    "import logging\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import subprocess\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from IPython.display import display, HTML\n",
    "import IPython\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "from dotenv import load_dotenv\n",
    "import numpy as np\n",
    "from sklearn.metrics import precision_recall_curve, average_precision_score, roc_curve, auc\n",
    "from intervaltree import IntervalTree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded environment variables from: /home/lakishadavid/computational_genetic_genealogy/.env\n",
      "Working Directory: /home/lakishadavid/computational_genetic_genealogy\n",
      "Data Directory: /home/lakishadavid/computational_genetic_genealogy/data\n",
      "References Directory: /home/lakishadavid/computational_genetic_genealogy/references\n",
      "Results Directory: /home/lakishadavid/computational_genetic_genealogy/results\n",
      "Utils Directory: /home/lakishadavid/computational_genetic_genealogy/utils\n",
      "The current directory is /home/lakishadavid/computational_genetic_genealogy\n"
     ]
    }
   ],
   "source": [
    "def find_comp_gen_dir():\n",
    "    \"\"\"Find the computational_genetic_genealogy directory by searching up from current directory.\"\"\"\n",
    "    current = Path.cwd()\n",
    "    \n",
    "    # Search up through parent directories\n",
    "    while current != current.parent:\n",
    "        # Check if target directory exists in current path\n",
    "        target = current / 'computational_genetic_genealogy'\n",
    "        if target.is_dir():\n",
    "            return target\n",
    "        # Move up one directory\n",
    "        current = current.parent\n",
    "    \n",
    "    raise FileNotFoundError(\"Could not find computational_genetic_genealogy directory\")\n",
    "\n",
    "def load_env_file():\n",
    "    \"\"\"Find and load the .env file from the computational_genetic_genealogy directory.\"\"\"\n",
    "    try:\n",
    "        # Find the computational_genetic_genealogy directory\n",
    "        comp_gen_dir = find_comp_gen_dir()\n",
    "        \n",
    "        # Look for .env file\n",
    "        env_path = comp_gen_dir / '.env'\n",
    "        if not env_path.exists():\n",
    "            print(f\"Warning: No .env file found in {comp_gen_dir}\")\n",
    "            return None\n",
    "        \n",
    "        # Load the .env file\n",
    "        load_dotenv(env_path, override=True)\n",
    "        print(f\"Loaded environment variables from: {env_path}\")\n",
    "        return env_path\n",
    "        \n",
    "    except FileNotFoundError as e:\n",
    "        print(f\"Error: {e}\")\n",
    "        return None\n",
    "\n",
    "# Use the function\n",
    "env_path = load_env_file()\n",
    "\n",
    "working_directory = os.getenv('PROJECT_WORKING_DIR', default=None)\n",
    "data_directory = os.getenv('PROJECT_DATA_DIR', default=None)\n",
    "references_directory = os.getenv('PROJECT_REFERENCES_DIR', default=None)\n",
    "results_directory = os.getenv('PROJECT_RESULTS_DIR', default=None)\n",
    "utils_directory = os.getenv('PROJECT_UTILS_DIR', default=None)\n",
    "\n",
    "print(f\"Working Directory: {working_directory}\")\n",
    "os.environ[\"WORKING_DIRECTORY\"] = working_directory\n",
    "print(f\"Data Directory: {data_directory}\")\n",
    "os.environ[\"DATA_DIRECTORY\"] = data_directory\n",
    "print(f\"References Directory: {references_directory}\")\n",
    "os.environ[\"REFERENCES_DIRECTORY\"] = references_directory\n",
    "print(f\"Results Directory: {results_directory}\")\n",
    "os.environ[\"RESULTS_DIRECTORY\"] = results_directory\n",
    "print(f\"Utils Directory: {utils_directory}\")\n",
    "os.environ[\"UTILS_DIRECTORY\"] = utils_directory\n",
    "\n",
    "os.chdir(working_directory)\n",
    "print(f\"The current directory is {os.getcwd()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Lab 9 log file is located at /home/lakishadavid/computational_genetic_genealogy/results/lab9_log.txt.\n"
     ]
    }
   ],
   "source": [
    "def configure_logging(log_filename, log_file_debug_level=\"INFO\", console_debug_level=\"INFO\"):\n",
    "    \"\"\"\n",
    "    Configure logging for both file and console handlers.\n",
    "\n",
    "    Args:\n",
    "        log_filename (str): Path to the log file where logs will be written.\n",
    "        log_file_debug_level (str): Logging level for the file handler.\n",
    "        console_debug_level (str): Logging level for the console handler.\n",
    "    \"\"\"\n",
    "    # Create a root logger\n",
    "    logger = logging.getLogger()\n",
    "    logger.setLevel(logging.DEBUG)  # Capture all messages at the root level\n",
    "\n",
    "    # Convert level names to numeric levels\n",
    "    file_level = getattr(logging, log_file_debug_level.upper(), logging.INFO)\n",
    "    console_level = getattr(logging, console_debug_level.upper(), logging.INFO)\n",
    "\n",
    "    # File handler: Logs messages at file_level and above to the file\n",
    "    file_handler = logging.FileHandler(log_filename)\n",
    "    file_handler.setLevel(file_level)\n",
    "    file_formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')\n",
    "    file_handler.setFormatter(file_formatter)\n",
    "\n",
    "    # Console handler: Logs messages at console_level and above to the console\n",
    "    console_handler = logging.StreamHandler(sys.stdout)\n",
    "    console_handler.setLevel(console_level)\n",
    "    console_formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')\n",
    "    console_handler.setFormatter(console_formatter)\n",
    "\n",
    "    # Add handlers to the root logger\n",
    "    logger.addHandler(file_handler)\n",
    "    logger.addHandler(console_handler)\n",
    "    \n",
    "def clear_logger():\n",
    "    \"\"\"Remove all handlers from the root logger.\"\"\"\n",
    "    logger = logging.getLogger()\n",
    "    for handler in logger.handlers[:]:\n",
    "        logger.removeHandler(handler)\n",
    "        \n",
    "log_filename = os.path.join(results_directory, \"lab9_log.txt\")\n",
    "print(f\"The Lab 9 log file is located at {log_filename}.\")\n",
    "\n",
    "# Ensure the results_directory exists\n",
    "if not os.path.exists(results_directory):\n",
    "    os.makedirs(results_directory)\n",
    "\n",
    "# Check if the file exists; if not, create it\n",
    "if not os.path.exists(log_filename):\n",
    "    with open(log_filename, 'w') as file:\n",
    "        pass  # The file is now created.\n",
    "    \n",
    "clear_logger() # Clear the logger before reconfiguring it\n",
    "configure_logging(log_filename, log_file_debug_level=\"INFO\", console_debug_level=\"INFO\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "segments_refinedibd = Path(results_directory) / \"merged_opensnps_autosomes_refinedibd.seg\"\n",
    "segments_hapibd = Path(results_directory) / \"merged_opensnps_autosomes_hapibd.seg\"\n",
    "segments_ibis = Path(results_directory) / \"merged_opensnps_autosomes_ibis.seg\"\n",
    "segments_pedsim = Path(results_directory) / \"ped_sim_run2.seg\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_refined_ibd(filepath):\n",
    "    \"\"\"Load Refined IBD output file\"\"\"\n",
    "    # Refined IBD format: sample1 sample2 chrom startpos endpos LOD cM\n",
    "    cols = ['sample1', 'sample2', 'chrom', 'start', 'end', 'LOD', 'cM']\n",
    "    try:\n",
    "        df = pd.read_csv(filepath, sep='\\s+', header=None, names=cols)\n",
    "        # Create a unique segment ID for each segment\n",
    "        df['segment_id'] = range(len(df))\n",
    "        df['tool'] = 'RefinedIBD'\n",
    "        df['length'] = df['end'] - df['start']\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading Refined IBD file: {e}\")\n",
    "        return pd.DataFrame(columns=cols + ['segment_id', 'tool', 'length'])\n",
    "\n",
    "def load_hap_ibd(filepath):\n",
    "    \"\"\"Load Hap IBD output file\"\"\"\n",
    "    # Hap IBD format: sample1 sample2 chrom startpos endpos cM .... \n",
    "    cols = ['sample1', 'sample2', 'chrom', 'start', 'end', 'cM', 'num_sites', 'LOD']\n",
    "    try:\n",
    "        df = pd.read_csv(filepath, sep='\\s+', header=None, names=cols)\n",
    "        df['segment_id'] = range(len(df))\n",
    "        df['tool'] = 'HapIBD'\n",
    "        df['length'] = df['end'] - df['start']\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading Hap IBD file: {e}\")\n",
    "        return pd.DataFrame(columns=cols + ['segment_id', 'tool', 'length'])\n",
    "\n",
    "def load_ibis(filepath):\n",
    "    \"\"\"Load IBIS output file\"\"\"\n",
    "    # IBIS format varies, adjust columns as needed\n",
    "    cols = ['sample1', 'sample2', 'chrom', 'start', 'end', 'cM', 'num_snps']\n",
    "    try:\n",
    "        df = pd.read_csv(filepath, sep='\\s+', header=None, names=cols)\n",
    "        df['segment_id'] = range(len(df))\n",
    "        df['tool'] = 'IBIS'\n",
    "        df['length'] = df['end'] - df['start']\n",
    "        df['LOD'] = np.nan  # IBIS might not have LOD scores\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading IBIS file: {e}\")\n",
    "        return pd.DataFrame(columns=cols + ['segment_id', 'tool', 'length', 'LOD'])\n",
    "\n",
    "def load_pedsim_truth(filepath):\n",
    "    \"\"\"Load ground truth IBD segments from ped-sim\"\"\"\n",
    "    # Adjust format according to your ped-sim output\n",
    "    cols = ['sample1', 'sample2', 'chrom', 'start', 'end', 'segment_type']\n",
    "    try:\n",
    "        df = pd.read_csv(filepath, sep='\\s+', header=None, names=cols)\n",
    "        df['segment_id'] = range(len(df))\n",
    "        df['tool'] = 'Truth'\n",
    "        df['length'] = df['end'] - df['start']\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading PedSim truth file: {e}\")\n",
    "        return pd.DataFrame(columns=cols + ['segment_id', 'tool', 'length'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's create functions to evaluate IBD detection performance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_interval_tree(truth_df):\n",
    "    \"\"\"Create an interval tree from truth segments for efficient overlap checking\"\"\"\n",
    "    trees = {}\n",
    "    for _, row in truth_df.iterrows():\n",
    "        pair_key = tuple(sorted([row['sample1'], row['sample2']]))\n",
    "        chrom = row['chrom']\n",
    "        \n",
    "        if (pair_key, chrom) not in trees:\n",
    "            trees[(pair_key, chrom)] = IntervalTree()\n",
    "            \n",
    "        trees[(pair_key, chrom)].addi(row['start'], row['end'], row['segment_id'])\n",
    "    \n",
    "    return trees\n",
    "\n",
    "def calculate_overlap(segment, tree):\n",
    "    \"\"\"Calculate overlap between a segment and truth segments in the tree\"\"\"\n",
    "    overlaps = tree.overlap(segment['start'], segment['end'])\n",
    "    if not overlaps:\n",
    "        return 0, None\n",
    "    \n",
    "    # Find the best overlapping segment\n",
    "    best_overlap = 0\n",
    "    best_truth_id = None\n",
    "    \n",
    "    for interval in overlaps:\n",
    "        overlap_start = max(segment['start'], interval.begin)\n",
    "        overlap_end = min(segment['end'], interval.end)\n",
    "        overlap_length = overlap_end - overlap_start\n",
    "        \n",
    "        if overlap_length > best_overlap:\n",
    "            best_overlap = overlap_length\n",
    "            best_truth_id = interval.data\n",
    "    \n",
    "    return best_overlap / (segment['end'] - segment['start']), best_truth_id\n",
    "\n",
    "def evaluate_tool(tool_df, truth_trees):\n",
    "    \"\"\"Evaluate IBD detection performance for a specific tool\"\"\"\n",
    "    # Add columns for evaluation metrics\n",
    "    tool_df['detected_truth'] = False\n",
    "    tool_df['overlap_pct'] = 0.0\n",
    "    tool_df['truth_id'] = None\n",
    "    \n",
    "    for idx, row in tool_df.iterrows():\n",
    "        pair_key = tuple(sorted([row['sample1'], row['sample2']]))\n",
    "        chrom = row['chrom']\n",
    "        \n",
    "        if (pair_key, chrom) in truth_trees:\n",
    "            overlap_pct, truth_id = calculate_overlap(row, truth_trees[(pair_key, chrom)])\n",
    "            tool_df.at[idx, 'overlap_pct'] = overlap_pct\n",
    "            tool_df.at[idx, 'truth_id'] = truth_id\n",
    "            tool_df.at[idx, 'detected_truth'] = (overlap_pct > 0.5)  # Consider >50% overlap a true positive\n",
    "    \n",
    "    return tool_df\n",
    "\n",
    "def evaluate_all_tools(refined_df, hap_df, ibis_df, truth_df):\n",
    "    \"\"\"Evaluate all IBD detection tools\"\"\"\n",
    "    # Create interval trees for truth segments\n",
    "    truth_trees = create_interval_tree(truth_df)\n",
    "    \n",
    "    # Evaluate each tool\n",
    "    refined_eval = evaluate_tool(refined_df, truth_trees)\n",
    "    hap_eval = evaluate_tool(hap_df, truth_trees)\n",
    "    ibis_eval = evaluate_tool(ibis_df, truth_trees)\n",
    "    \n",
    "    # Combine results\n",
    "    all_results = pd.concat([refined_eval, hap_eval, ibis_eval])\n",
    "    \n",
    "    return all_results, truth_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's create functions for visualizing the performance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_length_distribution(all_results, truth_df):\n",
    "    \"\"\"Plot the distribution of segment lengths for each tool and truth\"\"\"\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    \n",
    "    # Combine all data\n",
    "    all_data = pd.concat([\n",
    "        all_results[all_results['tool'] == 'RefinedIBD'][['length', 'tool']],\n",
    "        all_results[all_results['tool'] == 'HapIBD'][['length', 'tool']],\n",
    "        all_results[all_results['tool'] == 'IBIS'][['length', 'tool']],\n",
    "        truth_df[['length', 'tool']]\n",
    "    ])\n",
    "    \n",
    "    # Plot density\n",
    "    sns.kdeplot(data=all_data, x='length', hue='tool', fill=True, alpha=0.5)\n",
    "    \n",
    "    plt.title('Distribution of IBD Segment Lengths')\n",
    "    plt.xlabel('Segment Length (bp)')\n",
    "    plt.ylabel('Density')\n",
    "    plt.grid(True, linestyle='--', alpha=0.7)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('ibd_length_distribution.png')\n",
    "    plt.show()\n",
    "\n",
    "def plot_precision_recall(all_results, truth_df):\n",
    "    \"\"\"Plot precision-recall curves for each tool\"\"\"\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    \n",
    "    # For each tool, calculate precision and recall using overlap percentage as score\n",
    "    for tool_name, color in zip(['RefinedIBD', 'HapIBD', 'IBIS'], ['blue', 'green', 'red']):\n",
    "        tool_df = all_results[all_results['tool'] == tool_name]\n",
    "        \n",
    "        if 'LOD' in tool_df.columns and not tool_df['LOD'].isna().all():\n",
    "            score = tool_df['LOD']  # Use LOD score if available\n",
    "        else:\n",
    "            score = tool_df['length']  # Otherwise use length as a proxy for confidence\n",
    "            \n",
    "        y_true = tool_df['detected_truth'].astype(int)\n",
    "        \n",
    "        # Calculate precision and recall\n",
    "        precision, recall, _ = precision_recall_curve(y_true, score)\n",
    "        avg_precision = average_precision_score(y_true, score)\n",
    "        \n",
    "        # Plot PR curve\n",
    "        plt.plot(recall, precision, lw=2, color=color,\n",
    "                 label=f'{tool_name} (AP={avg_precision:.2f})')\n",
    "    \n",
    "    plt.title('Precision-Recall Curves for IBD Detection Tools')\n",
    "    plt.xlabel('Recall')\n",
    "    plt.ylabel('Precision')\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.grid(True, linestyle='--', alpha=0.7)\n",
    "    plt.legend(loc=\"best\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('ibd_precision_recall.png')\n",
    "    plt.show()\n",
    "\n",
    "def plot_roc_curves(all_results, truth_df):\n",
    "    \"\"\"Plot ROC curves for each tool\"\"\"\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    \n",
    "    # For each tool, calculate ROC using overlap percentage as score\n",
    "    for tool_name, color in zip(['RefinedIBD', 'HapIBD', 'IBIS'], ['blue', 'green', 'red']):\n",
    "        tool_df = all_results[all_results['tool'] == tool_name]\n",
    "        \n",
    "        if 'LOD' in tool_df.columns and not tool_df['LOD'].isna().all():\n",
    "            score = tool_df['LOD']  # Use LOD score if available\n",
    "        else:\n",
    "            score = tool_df['length']  # Otherwise use length as a proxy for confidence\n",
    "            \n",
    "        y_true = tool_df['detected_truth'].astype(int)\n",
    "        \n",
    "        # Calculate ROC\n",
    "        fpr, tpr, _ = roc_curve(y_true, score)\n",
    "        roc_auc = auc(fpr, tpr)\n",
    "        \n",
    "        # Plot ROC curve\n",
    "        plt.plot(fpr, tpr, lw=2, color=color,\n",
    "                 label=f'{tool_name} (AUC={roc_auc:.2f})')\n",
    "    \n",
    "    plt.plot([0, 1], [0, 1], linestyle='--', lw=2, color='gray', alpha=0.8)\n",
    "    plt.title('ROC Curves for IBD Detection Tools')\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.grid(True, linestyle='--', alpha=0.7)\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('ibd_roc_curves.png')\n",
    "    plt.show()\n",
    "\n",
    "def plot_overlap_histogram(all_results):\n",
    "    \"\"\"Plot histogram of overlap percentages for each tool\"\"\"\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    \n",
    "    for tool_name, color in zip(['RefinedIBD', 'HapIBD', 'IBIS'], ['blue', 'green', 'red']):\n",
    "        tool_df = all_results[all_results['tool'] == tool_name]\n",
    "        plt.hist(tool_df['overlap_pct'], bins=20, alpha=0.5, color=color, label=tool_name)\n",
    "    \n",
    "    plt.title('Distribution of Overlap with Truth Segments')\n",
    "    plt.xlabel('Overlap Percentage')\n",
    "    plt.ylabel('Count')\n",
    "    plt.grid(True, linestyle='--', alpha=0.7)\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('ibd_overlap_histogram.png')\n",
    "    plt.show()\n",
    "\n",
    "def calculate_summary_metrics(all_results, truth_df):\n",
    "    \"\"\"Calculate summary statistics for each tool\"\"\"\n",
    "    metrics = []\n",
    "    \n",
    "    # Count total truth segments\n",
    "    total_truth = len(truth_df)\n",
    "    \n",
    "    for tool_name in ['RefinedIBD', 'HapIBD', 'IBIS']:\n",
    "        tool_df = all_results[all_results['tool'] == tool_name]\n",
    "        \n",
    "        # Count true positives (segments with >50% overlap)\n",
    "        true_positives = tool_df['detected_truth'].sum()\n",
    "        \n",
    "        # Count false positives (segments with ≤50% overlap)\n",
    "        false_positives = len(tool_df) - true_positives\n",
    "        \n",
    "        # Count truth segments detected by this tool\n",
    "        detected_truth_ids = set([x for x in tool_df['truth_id'] if x is not None])\n",
    "        detected_truths = len(detected_truth_ids)\n",
    "        \n",
    "        # Calculate recall (proportion of truth segments detected)\n",
    "        recall = detected_truths / total_truth if total_truth > 0 else 0\n",
    "        \n",
    "        # Calculate precision (proportion of detected segments that are true)\n",
    "        precision = true_positives / len(tool_df) if len(tool_df) > 0 else 0\n",
    "        \n",
    "        # Calculate F1 score\n",
    "        f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "        \n",
    "        metrics.append({\n",
    "            'Tool': tool_name,\n",
    "            'Total Segments': len(tool_df),\n",
    "            'True Positives': true_positives,\n",
    "            'False Positives': false_positives,\n",
    "            'Detected Truth Segments': detected_truths,\n",
    "            'Precision': precision,\n",
    "            'Recall': recall,\n",
    "            'F1 Score': f1\n",
    "        })\n",
    "    \n",
    "    return pd.DataFrame(metrics)\n",
    "\n",
    "def plot_summary_barplot(metrics_df):\n",
    "    \"\"\"Plot summary metrics as a bar chart\"\"\"\n",
    "    plt.figure(figsize=(14, 10))\n",
    "    \n",
    "    # Melt the dataframe to make it suitable for grouped bar chart\n",
    "    plot_metrics = ['Precision', 'Recall', 'F1 Score']\n",
    "    plot_df = pd.melt(metrics_df, id_vars=['Tool'], value_vars=plot_metrics, \n",
    "                      var_name='Metric', value_name='Value')\n",
    "    \n",
    "    # Create grouped bar chart\n",
    "    sns.barplot(x='Tool', y='Value', hue='Metric', data=plot_df)\n",
    "    \n",
    "    plt.title('Performance Metrics by IBD Detection Tool')\n",
    "    plt.ylabel('Score')\n",
    "    plt.ylim(0, 1)\n",
    "    plt.grid(True, linestyle='--', alpha=0.7, axis='y')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('ibd_performance_metrics.png')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's put everything together with a main function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    # File paths - replace with your actual file paths\n",
    "    refined_path = 'path/to/refined_ibd_output.txt'\n",
    "    hap_path = 'path/to/hap_ibd_output.txt'\n",
    "    ibis_path = 'path/to/ibis_output.txt'\n",
    "    truth_path = 'path/to/pedsim_truth.txt'\n",
    "    \n",
    "    # Load data\n",
    "    print(\"Loading data...\")\n",
    "    refined_df = load_refined_ibd(refined_path)\n",
    "    hap_df = load_hap_ibd(hap_path)\n",
    "    ibis_df = load_ibis(ibis_path)\n",
    "    truth_df = load_pedsim_truth(truth_path)\n",
    "    \n",
    "    # Print data summaries\n",
    "    print(f\"Loaded {len(refined_df)} Refined IBD segments\")\n",
    "    print(f\"Loaded {len(hap_df)} Hap IBD segments\")\n",
    "    print(f\"Loaded {len(ibis_df)} IBIS segments\")\n",
    "    print(f\"Loaded {len(truth_df)} truth segments from ped-sim\")\n",
    "    \n",
    "    # Evaluate tools\n",
    "    print(\"Evaluating IBD detection tools...\")\n",
    "    all_results, truth_df = evaluate_all_tools(refined_df, hap_df, ibis_df, truth_df)\n",
    "    \n",
    "    # Generate visualizations\n",
    "    print(\"Generating visualizations...\")\n",
    "    plot_length_distribution(all_results, truth_df)\n",
    "    plot_precision_recall(all_results, truth_df)\n",
    "    plot_roc_curves(all_results, truth_df)\n",
    "    plot_overlap_histogram(all_results)\n",
    "    \n",
    "    # Calculate and display summary metrics\n",
    "    print(\"Calculating summary metrics...\")\n",
    "    metrics_df = calculate_summary_metrics(all_results, truth_df)\n",
    "    print(metrics_df)\n",
    "    plot_summary_barplot(metrics_df)\n",
    "    \n",
    "    print(\"Evaluation complete!\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_chromosome_performance(all_results, truth_df):\n",
    "    \"\"\"Plot performance metrics by chromosome\"\"\"\n",
    "    # Get unique chromosomes\n",
    "    all_chroms = sorted(truth_df['chrom'].unique())\n",
    "    \n",
    "    # Initialize metrics dictionary\n",
    "    chrom_metrics = {tool: {chrom: {'precision': 0, 'recall': 0, 'f1': 0} \n",
    "                           for chrom in all_chroms} \n",
    "                    for tool in ['RefinedIBD', 'HapIBD', 'IBIS']}\n",
    "    \n",
    "    # Calculate metrics per chromosome\n",
    "    for chrom in all_chroms:\n",
    "        # Count truth segments in this chromosome\n",
    "        truth_in_chrom = truth_df[truth_df['chrom'] == chrom]\n",
    "        total_truth = len(truth_in_chrom)\n",
    "        \n",
    "        for tool in ['RefinedIBD', 'HapIBD', 'IBIS']:\n",
    "            # Get tool results for this chromosome\n",
    "            tool_results = all_results[(all_results['tool'] == tool) & \n",
    "                                       (all_results['chrom'] == chrom)]\n",
    "            \n",
    "            if len(tool_results) == 0:\n",
    "                continue\n",
    "                \n",
    "            # Count true positives\n",
    "            true_positives = tool_results['detected_truth'].sum()\n",
    "            \n",
    "            # Count detected truth segments\n",
    "            detected_truth_ids = set([x for x in tool_results['truth_id'] if x is not None])\n",
    "            detected_truths = len(detected_truth_ids)\n",
    "            \n",
    "            # Calculate metrics\n",
    "            precision = true_positives / len(tool_results) if len(tool_results) > 0 else 0\n",
    "            recall = detected_truths / total_truth if total_truth > 0 else 0\n",
    "            f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "            \n",
    "            chrom_metrics[tool][chrom]['precision'] = precision\n",
    "            chrom_metrics[tool][chrom]['recall'] = recall\n",
    "            chrom_metrics[tool][chrom]['f1'] = f1\n",
    "    \n",
    "    # Create dataframes for plotting\n",
    "    plot_data = []\n",
    "    for tool in chrom_metrics:\n",
    "        for chrom in chrom_metrics[tool]:\n",
    "            for metric in ['precision', 'recall', 'f1']:\n",
    "                plot_data.append({\n",
    "                    'Tool': tool,\n",
    "                    'Chromosome': chrom,\n",
    "                    'Metric': metric.capitalize(),\n",
    "                    'Value': chrom_metrics[tool][chrom][metric]\n",
    "                })\n",
    "    \n",
    "    plot_df = pd.DataFrame(plot_data)\n",
    "    \n",
    "    # Plot\n",
    "    plt.figure(figsize=(16, 12))\n",
    "    g = sns.FacetGrid(plot_df, col='Metric', row='Tool', height=3, aspect=2)\n",
    "    g.map_dataframe(sns.barplot, x='Chromosome', y='Value')\n",
    "    g.set_axis_labels('Chromosome', 'Score')\n",
    "    g.set_titles('{row_name} - {col_name}')\n",
    "    \n",
    "    for ax in g.axes.flat:\n",
    "        ax.set_ylim(0, 1)\n",
    "        ax.grid(True, linestyle='--', alpha=0.7, axis='y')\n",
    "        \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('ibd_chromosome_performance.png')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_accuracy_by_length(all_results, truth_df):\n",
    "    \"\"\"Plot detection accuracy as a function of segment length\"\"\"\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    \n",
    "    # Bin truth segments by length\n",
    "    bins = [0, 1e4, 5e4, 1e5, 5e5, 1e6, 5e6, float('inf')]\n",
    "    bin_labels = ['<10kb', '10-50kb', '50-100kb', '0.1-0.5Mb', '0.5-1Mb', '1-5Mb', '>5Mb']\n",
    "    \n",
    "    # Add length bins to truth dataframe\n",
    "    truth_df['length_bin'] = pd.cut(truth_df['length'], bins=bins, labels=bin_labels)\n",
    "    \n",
    "    # Count total truth segments per bin\n",
    "    truth_counts = truth_df.groupby('length_bin').size()\n",
    "    \n",
    "    # For each tool, calculate detection rate by length bin\n",
    "    for tool_name, color in zip(['RefinedIBD', 'HapIBD', 'IBIS'], ['blue', 'green', 'red']):\n",
    "        detection_rates = []\n",
    "        \n",
    "        for bin_label in bin_labels:\n",
    "            # Get truth segments in this bin\n",
    "            bin_truth = truth_df[truth_df['length_bin'] == bin_label]\n",
    "            total = len(bin_truth)\n",
    "            \n",
    "            if total == 0:\n",
    "                detection_rates.append(0)\n",
    "                continue\n",
    "            \n",
    "            # Count how many were detected by this tool\n",
    "            tool_df = all_results[all_results['tool'] == tool_name]\n",
    "            detected = 0\n",
    "            \n",
    "            for _, truth_row in bin_truth.iterrows():\n",
    "                truth_id = truth_row['segment_id']\n",
    "                if (tool_df['truth_id'] == truth_id).any():\n",
    "                    detected += 1\n",
    "            \n",
    "            detection_rates.append(detected / total if total > 0 else 0)\n",
    "        \n",
    "        plt.plot(bin_labels, detection_rates, marker='o', label=tool_name, color=color, linewidth=2)\n",
    "    \n",
    "    plt.title('IBD Detection Rate by Segment Length')\n",
    "    plt.xlabel('Segment Length')\n",
    "    plt.ylabel('Detection Rate')\n",
    "    plt.ylim(0, 1.05)\n",
    "    plt.grid(True, linestyle='--', alpha=0.7)\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('ibd_detection_by_length.png')\n",
    "    plt.show()\n",
    "    \n",
    "    # Also create a bar chart showing segment counts by length\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    truth_counts.plot(kind='bar', color='purple')\n",
    "    plt.title('Number of Truth Segments by Length')\n",
    "    plt.xlabel('Segment Length')\n",
    "    plt.ylabel('Count')\n",
    "    plt.grid(True, linestyle='--', alpha=0.7, axis='y')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('ibd_truth_segments_by_length.png')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    # ... (previous code)\n",
    "    \n",
    "    # Additional visualizations\n",
    "    plot_chromosome_performance(all_results, truth_df)\n",
    "    plot_accuracy_by_length(all_results, truth_df)\n",
    "    \n",
    "    print(\"Evaluation complete!\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
