{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from collections import Counter\n",
    "import logging\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import subprocess\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from IPython.display import display, HTML\n",
    "import IPython\n",
    "import pandas as pd\n",
    "import boto3\n",
    "\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_comp_gen_dir():\n",
    "    \"\"\"Find the computational_genetic_genealogy directory by searching up from current directory.\"\"\"\n",
    "    current = Path.cwd()\n",
    "    \n",
    "    # Search up through parent directories\n",
    "    while current != current.parent:\n",
    "        # Check if target directory exists in current path\n",
    "        target = current / 'computational_genetic_genealogy'\n",
    "        if target.is_dir():\n",
    "            return target\n",
    "        # Move up one directory\n",
    "        current = current.parent\n",
    "    \n",
    "    raise FileNotFoundError(\"Could not find computational_genetic_genealogy directory\")\n",
    "\n",
    "def load_env_file():\n",
    "    \"\"\"Find and load the .env file from the computational_genetic_genealogy directory.\"\"\"\n",
    "    try:\n",
    "        # Find the computational_genetic_genealogy directory\n",
    "        comp_gen_dir = find_comp_gen_dir()\n",
    "        \n",
    "        # Look for .env file\n",
    "        env_path = comp_gen_dir / '.env'\n",
    "        if not env_path.exists():\n",
    "            print(f\"Warning: No .env file found in {comp_gen_dir}\")\n",
    "            return None\n",
    "        \n",
    "        # Load the .env file\n",
    "        load_dotenv(env_path, override=True)\n",
    "        print(f\"Loaded environment variables from: {env_path}\")\n",
    "        return env_path\n",
    "        \n",
    "    except FileNotFoundError as e:\n",
    "        print(f\"Error: {e}\")\n",
    "        return None\n",
    "\n",
    "# Use the function\n",
    "env_path = load_env_file()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "working_directory = os.getenv('PROJECT_WORKING_DIR', default=None)\n",
    "data_directory = os.getenv('PROJECT_DATA_DIR', default=None)\n",
    "references_directory = os.getenv('PROJECT_REFERENCES_DIR', default=None)\n",
    "results_directory = os.getenv('PROJECT_RESULTS_DIR', default=None)\n",
    "utils_directory = os.getenv('PROJECT_UTILS_DIR', default=None)\n",
    "\n",
    "print(f\"Working Directory: {working_directory}\")\n",
    "print(f\"Data Directory: {data_directory}\")\n",
    "print(f\"References Directory: {references_directory}\")\n",
    "print(f\"Results Directory: {results_directory}\")\n",
    "print(f\"Utils Directory: {utils_directory}\")\n",
    "\n",
    "os.chdir(working_directory)\n",
    "print(f\"The current directory is {os.getcwd()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def configure_logging(log_filename, log_file_debug_level=\"INFO\", console_debug_level=\"INFO\"):\n",
    "    \"\"\"\n",
    "    Configure logging for both file and console handlers.\n",
    "\n",
    "    Args:\n",
    "        log_filename (str): Path to the log file where logs will be written.\n",
    "        log_file_debug_level (str): Logging level for the file handler.\n",
    "        console_debug_level (str): Logging level for the console handler.\n",
    "    \"\"\"\n",
    "    # Create a root logger\n",
    "    logger = logging.getLogger()\n",
    "    logger.setLevel(logging.DEBUG)  # Capture all messages at the root level\n",
    "\n",
    "    # Convert level names to numeric levels\n",
    "    file_level = getattr(logging, log_file_debug_level.upper(), logging.INFO)\n",
    "    console_level = getattr(logging, console_debug_level.upper(), logging.INFO)\n",
    "\n",
    "    # File handler: Logs messages at file_level and above to the file\n",
    "    file_handler = logging.FileHandler(log_filename)\n",
    "    file_handler.setLevel(file_level)\n",
    "    file_formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')\n",
    "    file_handler.setFormatter(file_formatter)\n",
    "\n",
    "    # Console handler: Logs messages at console_level and above to the console\n",
    "    console_handler = logging.StreamHandler(sys.stdout)\n",
    "    console_handler.setLevel(console_level)\n",
    "    console_formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')\n",
    "    console_handler.setFormatter(console_formatter)\n",
    "\n",
    "    # Add handlers to the root logger\n",
    "    logger.addHandler(file_handler)\n",
    "    logger.addHandler(console_handler)\n",
    "    \n",
    "def clear_logger():\n",
    "    \"\"\"Remove all handlers from the root logger.\"\"\"\n",
    "    logger = logging.getLogger()\n",
    "    for handler in logger.handlers[:]:\n",
    "        logger.removeHandler(handler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_filename = os.path.join(results_directory, \"lab_get_opensnp_data.log\")\n",
    "print(f\"The log file is located at {log_filename}.\")\n",
    "\n",
    "# Ensure the results_directory exists\n",
    "if not os.path.exists(results_directory):\n",
    "    os.makedirs(results_directory)\n",
    "\n",
    "# Check if the file exists; if not, create it\n",
    "if not os.path.exists(log_filename):\n",
    "    with open(log_filename, 'w') as file:\n",
    "        pass  # The file is now created.\n",
    "    \n",
    "clear_logger() # Clear the logger before reconfiguring it\n",
    "configure_logging(log_filename, log_file_debug_level=\"INFO\", console_debug_level=\"INFO\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get The Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸ“¥ How to Download OpenSNP Data\n",
    "\n",
    "There are **two ways** to download OpenSNP data. You only need to select **one** method. **The preferred method is using `boto3`.**\n",
    "\n",
    "---\n",
    "\n",
    "## **âœ… Option 1: Download Using `boto3` (Preferred)**\n",
    "With `boto3` installed, you can use it to efficiently download files from the OpenSNP public S3 bucket. This method is recommended for bulk downloading and better reliability. However, how to setup boto3 will not be covered until later in the semester. For now, use option 2.\n",
    "\n",
    "---\n",
    "\n",
    "## **âœ… Option 2: Download Using `requests` (No AWS Setup Required)**\n",
    "You can use the `requests` library to download the files directly from a public S3 URLs. This method is easier to use but may be slower for large downloads.\n",
    "\n",
    "---\n",
    "\n",
    "## **ðŸ“Œ Manually Downloading Files**\n",
    "If you prefer to manually download individual files:\n",
    "1. Open your web browser and go to:\n",
    "   ```\n",
    "   https://opensnpdata.s3.us-east-2.amazonaws.com/[FILENAME]\n",
    "   ```\n",
    "   Replace `[FILENAME]` with the exact filename from `opensnp_file_list.txt`.\n",
    "\n",
    "2. Example:\n",
    "\n",
    "   https://opensnpdata.s3.us-east-2.amazonaws.com/user1001_file496_yearofbirth_unknown_sex_unknown.ancestry.txt\n",
    "\n",
    "3. **Right-click â†’ Save As...** to download the file.\n",
    "\n",
    "4. Move the file to:\n",
    "   ```\n",
    "   data_directory/class_data/raw_dna_profiles/\n",
    "   ```\n",
    "\n",
    "---\n",
    "\n",
    "ðŸ“Œ **Choose one method that works best for you.** If unsure, use **boto3** for better performance.\n",
    "\n",
    "ðŸš€ Happy downloading! ðŸš€"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Option 1: boto3 version**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define constants\n",
    "BUCKET_NAME = \"opensnpdata\"\n",
    "SAVE_DIR = f\"{data_directory}/class_data/raw_dna_profiles\"\n",
    "\n",
    "# Ensure the save directory exists\n",
    "os.makedirs(SAVE_DIR, exist_ok=True)\n",
    "\n",
    "# Initialize an anonymous S3 client\n",
    "s3 = boto3.client(\"s3\", config=boto3.session.Config(signature_version=\"s3v4\"))\n",
    "\n",
    "def count_files_in_bucket():\n",
    "    \"\"\"Count the total number of files in the OpenSNP bucket.\"\"\"\n",
    "    response = s3.list_objects_v2(Bucket=BUCKET_NAME)\n",
    "    if \"Contents\" in response:\n",
    "        return len(response[\"Contents\"])\n",
    "    return 0\n",
    "\n",
    "num_files = count_files_in_bucket()\n",
    "print(f\"Total files in bucket: {num_files}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_files(limit=None):\n",
    "    \"\"\"\n",
    "    Download a specified number of files (or all files) from the OpenSNP bucket.\n",
    "\n",
    "    Parameters:\n",
    "        limit (int or None): Number of files to download. If None, downloads all files.\n",
    "    \"\"\"\n",
    "    response = s3.list_objects_v2(Bucket=BUCKET_NAME)\n",
    "    \n",
    "    if \"Contents\" not in response:\n",
    "        print(\"No files found in the OpenSNP bucket.\")\n",
    "        return\n",
    "    \n",
    "    files = response[\"Contents\"]\n",
    "    \n",
    "    # Apply limit if specified\n",
    "    if limit is not None:\n",
    "        files = files[:limit]\n",
    "\n",
    "    print(f\"Downloading {len(files)} files...\")\n",
    "\n",
    "    for obj in files:\n",
    "        file_key = obj[\"Key\"]\n",
    "        local_path = os.path.join(SAVE_DIR, os.path.basename(file_key))\n",
    "\n",
    "        print(f\"Downloading: {file_key} -> {local_path}\")\n",
    "        s3.download_file(BUCKET_NAME, file_key, local_path)\n",
    "\n",
    "    print(\"Download completed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download a certain number of files (e.g., first 5)\n",
    "download_files(limit=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download all files\n",
    "download_files(limit=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Option 2: non boto3 version**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from tqdm.notebook import tqdm  # Changed to notebook version of tqdm\n",
    "from tqdm.auto import tqdm as tqdm_auto  # For auto-detection of environment\n",
    "import signal\n",
    "import sys\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Optional\n",
    "import logging\n",
    "\n",
    "@dataclass\n",
    "class DownloadConfig:\n",
    "    bucket_url: str\n",
    "    save_dir: str\n",
    "    file_list_path: str\n",
    "    max_workers: int = 5\n",
    "    chunk_size: int = 8192\n",
    "\n",
    "class ParallelDownloader:\n",
    "    def __init__(self, config: DownloadConfig):\n",
    "        self.config = config\n",
    "        self.interrupted = False\n",
    "        self.failed_downloads = []\n",
    "        self.setup_logging()\n",
    "        self.setup_signal_handlers()\n",
    "        \n",
    "    def setup_logging(self):\n",
    "        logging.basicConfig(\n",
    "            level=logging.INFO,\n",
    "            format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "            handlers=[\n",
    "                logging.FileHandler('download_log.txt'),\n",
    "                logging.StreamHandler(sys.stdout)\n",
    "            ]\n",
    "        )\n",
    "        self.logger = logging.getLogger(__name__)\n",
    "\n",
    "    def setup_signal_handlers(self):\n",
    "        signal.signal(signal.SIGINT, self.handle_interrupt)\n",
    "        signal.signal(signal.SIGTERM, self.handle_interrupt)\n",
    "\n",
    "    def handle_interrupt(self, signum, frame):\n",
    "        self.logger.warning(\"Received interrupt signal. Finishing current downloads...\")\n",
    "        self.interrupted = True\n",
    "\n",
    "    def get_file_list(self, start: Optional[int] = None, end: Optional[int] = None) -> List[str]:\n",
    "        \"\"\"Read and optionally slice the file list.\"\"\"\n",
    "        try:\n",
    "            with open(self.config.file_list_path, \"r\") as f:\n",
    "                file_list = [line.strip() for line in f.readlines()]\n",
    "            return file_list[start:end] if start is not None else file_list\n",
    "        except FileNotFoundError:\n",
    "            self.logger.error(f\"File list not found: {self.config.file_list_path}\")\n",
    "            raise\n",
    "\n",
    "    def download_file(self, filename: str, overall_pbar) -> bool:\n",
    "        \"\"\"Download a single file with progress tracking.\"\"\"\n",
    "        if self.interrupted:\n",
    "            return False\n",
    "\n",
    "        file_url = f\"{self.config.bucket_url}/{filename}\"\n",
    "        local_path = os.path.join(self.config.save_dir, os.path.basename(filename))\n",
    "\n",
    "        # Skip if file exists and has content\n",
    "        if os.path.exists(local_path) and os.path.getsize(local_path) > 0:\n",
    "            self.logger.info(f\"Skipping existing file: {filename}\")\n",
    "            overall_pbar.update(1)\n",
    "            return True\n",
    "\n",
    "        try:\n",
    "            response = requests.get(file_url, stream=True)\n",
    "            response.raise_for_status()\n",
    "\n",
    "            total_size = int(response.headers.get('content-length', 0))\n",
    "            \n",
    "            with open(local_path, \"wb\") as file:\n",
    "                if total_size == 0:\n",
    "                    file.write(response.content)\n",
    "                else:\n",
    "                    downloaded = 0\n",
    "                    for chunk in response.iter_content(chunk_size=self.config.chunk_size):\n",
    "                        if self.interrupted:\n",
    "                            return False\n",
    "                        if chunk:\n",
    "                            file.write(chunk)\n",
    "                            downloaded += len(chunk)\n",
    "                            \n",
    "            overall_pbar.update(1)\n",
    "            return True\n",
    "\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Failed to download {filename}: {str(e)}\")\n",
    "            self.failed_downloads.append((filename, str(e)))\n",
    "            if os.path.exists(local_path):\n",
    "                os.remove(local_path)\n",
    "            overall_pbar.update(1)\n",
    "            return False\n",
    "\n",
    "    def download_files(self, start: Optional[int] = None, \n",
    "                      end: Optional[int] = None, \n",
    "                      max_retries: int = 3) -> None:\n",
    "        \"\"\"\n",
    "        Download files in parallel with retry mechanism.\n",
    "        \n",
    "        Args:\n",
    "            start: Optional starting index for file range\n",
    "            end: Optional ending index for file range\n",
    "            max_retries: Maximum number of retry attempts for failed downloads\n",
    "        \"\"\"\n",
    "        os.makedirs(self.config.save_dir, exist_ok=True)\n",
    "        \n",
    "        file_list = self.get_file_list(start, end)\n",
    "        total_files = len(file_list)\n",
    "        \n",
    "        self.logger.info(f\"Starting download of {total_files} files...\")\n",
    "        \n",
    "        for attempt in range(max_retries + 1):\n",
    "            if not file_list:\n",
    "                break\n",
    "                \n",
    "            with ThreadPoolExecutor(max_workers=self.config.max_workers) as executor:\n",
    "                # Create the progress bar\n",
    "                with tqdm_auto(total=len(file_list), \n",
    "                             desc=\"Downloading files\", \n",
    "                             unit=\"file\") as pbar:\n",
    "                    \n",
    "                    # Submit all downloads\n",
    "                    future_to_file = {\n",
    "                        executor.submit(self.download_file, filename, pbar): filename\n",
    "                        for filename in file_list\n",
    "                    }\n",
    "                    \n",
    "                    successful_downloads = []\n",
    "                    \n",
    "                    # Process completed downloads\n",
    "                    for future in as_completed(future_to_file):\n",
    "                        filename = future_to_file[future]\n",
    "                        try:\n",
    "                            if future.result():\n",
    "                                successful_downloads.append(filename)\n",
    "                        except Exception as e:\n",
    "                            self.logger.error(f\"Error downloading {filename}: {str(e)}\")\n",
    "                        \n",
    "                        if self.interrupted:\n",
    "                            self.logger.warning(\"Download interrupted by user.\")\n",
    "                            return\n",
    "\n",
    "                # Remove successful downloads from the list\n",
    "                file_list = [f for f in file_list if f not in successful_downloads]\n",
    "                \n",
    "                if file_list and attempt < max_retries:\n",
    "                    self.logger.info(f\"Retrying {len(file_list)} failed downloads... \"\n",
    "                                   f\"(Attempt {attempt + 2}/{max_retries + 1})\")\n",
    "        \n",
    "        if self.failed_downloads:\n",
    "            self.logger.error(\"Failed downloads:\")\n",
    "            for filename, error in self.failed_downloads:\n",
    "                self.logger.error(f\"  {filename}: {error}\")\n",
    "            \n",
    "        self.logger.info(f\"Download completed. \"\n",
    "                        f\"Successfully downloaded: {total_files - len(self.failed_downloads)}/{total_files}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create config in one cell\n",
    "config = DownloadConfig(\n",
    "    bucket_url=\"https://opensnpdata.s3.us-east-2.amazonaws.com\",\n",
    "    save_dir=f\"{data_directory}/class_data/raw_dna_profiles\",\n",
    "    file_list_path=f\"{data_directory}/class_data/opensnp_file_list.txt\",\n",
    "    max_workers=5\n",
    ")\n",
    "\n",
    "# Create downloader instance\n",
    "downloader = ParallelDownloader(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can run this cell multiple times if needed\n",
    "downloader.download_files(start=5, end=15)  # Start with just 10 files as a test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download all files\n",
    "downloader.download_files()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
